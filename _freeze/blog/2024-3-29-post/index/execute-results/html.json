{
  "hash": "826963567f098ecfd6b68f30e01702ba",
  "result": {
    "markdown": "---\ntitle: \"Building Classification Models with data from Spotify Web API\"\ndescription: \"In this blog post, I build and compare four classification models that predict whether a given song was in my Spotify collection or that of my friend Maxwell. I also analyze feature importances in the best performing model.\"\nauthor: \n  - name: Linus Ghanadan\n    url: https://linusghanadan.github.io/\n    affiliation: MEDS\n    affiliation-url: https://bren.ucsb.edu/masters-programs/master-environmental-data-science\ndate: 03-29-2024\ncategories:\n  - [Machine Learning]\n  - [Random Forest]\n  - [Gradient Descent]\n  - [R]\ncitation:\n  url: https://linusghanadan.github.io/blog/2024-3-29-post/\nimage: plot.jpeg\ndraft: false\nformat:\n    html:\n        code-fold: true\n---\n\n\n## [Link to GitHub repository](https://github.com/linusghanadan/ml-spotify-lab)\n\n## Background\n\nThe idea for this blog post comes from a group assignment in my machine learning class. For this assignment, my classmate Maxwell and I started by each using the Spotify Web API to access our recent liked songs data, and we each retrieved data on our 200 most recently liked songs (cite spotify web API). We then trained three decision tree models using 75% of the data (training set) and compared performance based on performance on the remaining 25% (testing set). Now, I've decided to go back and also build a model that uses Stochastic Gradient Boosted (SGB) decision trees and also update my model comparisons to include this SGB model.\n\n## Setup & data import\n\nAccess the Spotify Web API requires having an existing Spotify account and creating a Spotify for Developers account on the [Spotify for Developers website](https://developer.spotify.com/documentation/web-api). For the purposes of this blog, which focuses on the model-building process, I'll skip over the API access steps. Instead, I'll just start by importing the CSV files, which were written using information from the API.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(baguette)\nlibrary(vip)\n\n# read in my data (CSV that was previously written)\nlinus_tracks <- read.csv(here::here(\"data\", \"2024-3-29-post-data\", \"linus_tracks.csv\"))\n\n# read in partner data\nmaxwell_tracks <- read.csv(here::here(\"data\", \"2024-3-29-post-data\", \"maxwell_songs.csv\")) %>% \n  mutate(name = \"maxwell\")\n\n# bind my liked songs df with partner df\ncombined_tracks <- rbind(linus_tracks, maxwell_tracks) %>% \n  rename(time_sig = time_signature) %>% \n  select(-track.name, -type, -id, -uri, -track_href, -analysis_url) # remove irrelevent columns\n```\n:::\n\n\n## Data exploration\n\nOur data set contains 13 features that might be useful for predicting whether a song is in my collection or Maxwell's. I'll start by exploring what these 13 features are and how some of them vary between Maxwell's liked songs and my own. Note that the first 13 columns contain the features we are looking at, and the 14th column contains our outcome variable 'name', which is either 'linus' or 'maxwell' depending on whose collection it is from.\n\nIn the following code, we look at a summarized breakdown of each column in the combined tracks data frame and then take a closer look at how Maxwell and I differ in terms of the tempo and danceability of our liked songs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# look at summary of columns\nsummary(combined_tracks)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  danceability        energy            key           loudness      \n Min.   :0.1440   Min.   :0.0844   Min.   : 0.00   Min.   :-20.149  \n 1st Qu.:0.5570   1st Qu.:0.5350   1st Qu.: 2.00   1st Qu.: -9.371  \n Median :0.6580   Median :0.6865   Median : 5.00   Median : -7.524  \n Mean   :0.6428   Mean   :0.6608   Mean   : 5.27   Mean   : -8.053  \n 3rd Qu.:0.7642   3rd Qu.:0.8215   3rd Qu.: 8.00   3rd Qu.: -5.899  \n Max.   :0.9730   Max.   :0.9840   Max.   :11.00   Max.   : -2.421  \n      mode         speechiness       acousticness       instrumentalness   \n Min.   :0.0000   Min.   :0.02450   Min.   :0.0000075   Min.   :0.0000000  \n 1st Qu.:0.0000   1st Qu.:0.03610   1st Qu.:0.0121500   1st Qu.:0.0000229  \n Median :1.0000   Median :0.04685   Median :0.0721000   Median :0.0198500  \n Mean   :0.5975   Mean   :0.07576   Mean   :0.2068194   Mean   :0.2509544  \n 3rd Qu.:1.0000   3rd Qu.:0.07193   3rd Qu.:0.3255000   3rd Qu.:0.5235000  \n Max.   :1.0000   Max.   :0.51900   Max.   :0.9780000   Max.   :0.9670000  \n    liveness          valence           tempo         duration_ms    \n Min.   :0.02990   Min.   :0.0322   Min.   : 61.85   Min.   : 57877  \n 1st Qu.:0.09557   1st Qu.:0.2640   1st Qu.:112.08   1st Qu.:169776  \n Median :0.11900   Median :0.4530   Median :126.03   Median :210192  \n Mean   :0.16955   Mean   :0.4711   Mean   :125.03   Mean   :219639  \n 3rd Qu.:0.19250   3rd Qu.:0.6843   3rd Qu.:135.12   3rd Qu.:257514  \n Max.   :0.93300   Max.   :0.9810   Max.   :208.65   Max.   :627097  \n    time_sig         name          \n Min.   :1.000   Length:400        \n 1st Qu.:4.000   Class :character  \n Median :4.000   Mode  :character  \n Mean   :3.947                     \n 3rd Qu.:4.000                     \n Max.   :5.000                     \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare mean of tempo and danceability for Linus and Maxwell\ncombined_tracks %>%\n  group_by(name) %>%\n  summarise(mean_tempo = mean(tempo),\n            mean_danceability = mean(danceability),\n            mean_instrumentalness = mean(instrumentalness)) %>% \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 4\n  name    mean_tempo mean_danceability mean_instrumentalness\n  <chr>        <dbl>             <dbl>                 <dbl>\n1 linus         125.             0.609                 0.157\n2 maxwell       125.             0.676                 0.344\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare distribution of tempo for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$tempo, combined_tracks$name),\n             main = \"Spotify liked songs comparison of tempo\", \n             ylab = \"tempo\",\n             xlab = c(\"linus\", \"maxwell\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhile the liked songs of Maxwell and I have a very similar mean tempo, the tempo of my liked songs exhibits a significantly wider distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare distribution of danceability for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$danceability, combined_tracks$name),\n             main = \"Spotify liked songs comparison of danceability\", \n             ylab = \"danceability\",\n             xlab = c(\"linus\", \"maxwell\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nMaxwell's collection has a slightly higher mean danceability, and the distribution for his songs is more left skewed compared to mine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare distribution of instrumentalness for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$instrumentalness, combined_tracks$name),\n             main = \"Spotify liked songs comparison of instrumentalness\", \n             ylab = \"instrumentalness\",\n             xlab = c(\"linus\", \"maxwell\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe mean instrumentalness of Maxwell's songs is more than twice that of the mean for my collection, and this is reflected in the histogram by the larger proportion of my songs with an instrumentalness of zero.\n\n## Data pre-processing\n\nTo start, we'll set the seed. This sets the randomization for creating our cross validation folds such that our results will be reproduced if ran on my local device again. We set the seed in its own code chunk because sometimes it can interfere with other process if included in a larger code chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(123)\n```\n:::\n\n\nWe split our data in training and testing sets. We'll use the training set to train the model during cross validation and the testing set to compare the performance of the different models. Next, we pre-process the data by specifying and prepping a recipe that converts all nominal features to dummy variables and normalizes all numeric features. We also create 10 folds of the training data to use for cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# initial split of data into training and testing sets (default 75/25)\ntracks_split <- initial_split(combined_tracks)\ntracks_test <- testing(tracks_split)\ntracks_train <- training(tracks_split)\n\n# specify recipe for model preprocessing\ntracks_recipe <- recipe(name ~ ., data = tracks_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  prep() # prep recipe\n\n# create 10 folds of the training data set for CV\ncv_folds <- tracks_train %>% vfold_cv(v = 10)\n```\n:::\n\n\n## Decision tree model\n\nFor our first model, we'll build just a single decision tree. A decision tree generates predictions by asking simple yes-or-no questions about the features. Which question to ask is determined by the partitioning objective. For our partitioning objective, we will be minimizing cross-entropy, which is the most common objective used for classification tasks.\n\n#### Build preliminary model & tune hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model for tuning hyperparameters\nsingle_tree_spec <- decision_tree(\n  cost_complexity = tune(), # tune cost complexity for pruning tree\n  tree_depth = tune(), # tune maximum tree depth\n  min_n = tune()) %>% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\n# create tuning grid for hyperparameters\ntuning_grid <- grid_latin_hypercube(cost_complexity(),\n                                    tree_depth(),\n                                    min_n(),\n                                    size = 10)\n\n# create workflow for tuning hyperparameters\nsingle_tree_wf <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(single_tree_spec)\n\n# tune hyperparameters using CV\nsingle_tree_tune <- tune_grid(single_tree_spec, \n                              as.factor(name) ~ ., \n                              resamples = cv_folds,\n                              grid = tuning_grid,\n                              metrics = metric_set(accuracy))\n```\n:::\n\n\n#### Build final model & predict testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify final model with optimized hyperparameters\nsingle_tree_final <- finalize_model(single_tree_spec, select_best(single_tree_tune))\n\n# fit final model to training data\nsingle_tree_fit <- fit(single_tree_final, as.factor(name)~., tracks_train)\n\n# predict testing data\nsingle_tree_predict <- predict(single_tree_fit, tracks_test) %>%\n  bind_cols(tracks_test) %>%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nsingle_tree_predict <- predict(single_tree_fit, tracks_test, type = \"prob\") %>%\n  bind_cols(single_tree_predict) %>%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nsingle_tree_conf_matrix <- single_tree_predict %>% \n  conf_mat(truth = name, estimate = .pred_class) %>% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Single DT\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nsingle_tree_accuracy <- accuracy(single_tree_predict, truth = name, estimate = .pred_class)\nsingle_tree_roc_auc <- roc_auc(single_tree_predict, truth = name, .pred_linus)\nsingle_tree_sensitivity <- sensitivity(single_tree_predict, truth = name, estimate = .pred_class)\nsingle_tree_specificity <- specificity(single_tree_predict, truth = name, estimate = .pred_class)\n```\n:::\n\n\n## Bagged trees model\n\nBagged, or \"bootstrap aggregating\", prediction models train multiple shallow decision tree models and then combines them to generate an aggregated prediction. Compared to building a single deep decision tree, building multiple shallow decision trees greatly reduces the potential for overfitting. However, in a bagged decision tree model, there is concern about the trees being correlated with one another, meaning they may not provide a substantial improvement in predictive power.\n\n#### Build final model & predict testing data (no tuning required)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model\nbagged_trees_spec <- bag_tree() %>%\n  set_engine(\"rpart\", times = 50) %>% # specify number of trees (50-500 trees is usually sufficient)\n  set_mode(\"classification\")\n\n# create workflow\nbagged_trees_wf <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(bagged_trees_spec)\n\n# fit model to training data\nbagged_trees_fit <- bagged_trees_wf %>%\n  fit(data = tracks_train)\n\n# predict testing data\nbagged_trees_predict <- predict(bagged_trees_fit, tracks_test) %>% \n  bind_cols(tracks_test) %>%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nbagged_trees_predict <- predict(bagged_trees_fit, tracks_test, type = \"prob\") %>%\n  bind_cols(bagged_trees_predict) %>%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nbagged_trees_conf_matrix <- bagged_trees_predict %>% \n  conf_mat(truth = name, estimate = .pred_class) %>% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Bagged DTs\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nbagged_trees_accuracy <- accuracy(bagged_trees_predict, truth = name, estimate = .pred_class)\nbagged_trees_roc_auc <- roc_auc(bagged_trees_predict, truth = name, .pred_linus)\nbagged_trees_sensitivity <- sensitivity(bagged_trees_predict, truth = name, estimate = .pred_class)\nbagged_trees_specificity <- specificity(bagged_trees_predict, truth = name, estimate = .pred_class)\n```\n:::\n\n\n## Random forest model\n\nRandom forest models are a modification of bagged decision trees that builds a large collection of de-correlated trees to further improve predictive performance. Unlike with bagged decision trees, we now define an additional hyperparameter for the number of unique features that will be considered at each split in the decision tree. This hyperparameter, called mtry, makes it so we don't have to worry about the trees being correlated with one another because we are only looking at a randomized subset of the features at each split in each tree. Having these un-correlated trees allows us to build many trees that are also deep, without overfitting to the training data. Because there are many trees in this model and these trees are also built to be deep based on a randomized set of features, they are referred to as a random forest.\n\n#### Build preliminary model & tune hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model for tuning hyperparameters\nrf_spec <- rand_forest(trees = 500, # set number of trees to 500\n                       mtry = tune(), # tune mtry (number of unique feature variables that will be considered at each split)\n                       min_n = tune()) %>% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# create tuning grid for hyperparameters\n  tuning_grid <- grid_latin_hypercube(mtry(range = c(2, 4)), \n                                      min_n(c(1, 10)),\n                                      size = 10)\n\n# create workflow for tuning hyperparameters\nrf_wf <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(rf_spec)\n\n# tune hyperparameters using CV\nrf_tune <- tune_grid(rf_wf,\n                     resamples = cv_folds,\n                     grid = tuning_grid,\n                     metrics = metric_set(accuracy))\n```\n:::\n\n\n#### Build final model & predict testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify final model with optimized hyperparameters\nrf_final <- finalize_model(rf_spec, select_best(rf_tune))\n\n# create workflow for final version of model\nrf_final_wf <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(rf_final)\n\n# fit final workflow to training data\nrf_fit <- rf_final_wf %>%\n  fit(data = tracks_train)\n\n# predict testing data\nrf_predict <- predict(rf_fit, tracks_test) %>%\n  bind_cols(tracks_test) %>%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nrf_predict <- predict(rf_fit, tracks_test, type = \"prob\") %>%\n  bind_cols(rf_predict) %>%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nrf_conf_matrix <- rf_predict %>% \n  conf_mat(truth = name, estimate = .pred_class) %>% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Random Forest\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nrf_accuracy <- accuracy(rf_predict, truth = name, estimate = .pred_class)\nrf_roc_auc <- roc_auc(rf_predict, truth = name, .pred_linus)\nrf_sensitivity <- sensitivity(rf_predict, truth = name, estimate = .pred_class)\nrf_specificity <- specificity(rf_predict, truth = name, estimate = .pred_class)\n```\n:::\n\n\n## Stochastic Gradient Boosting (SGB) model\n\nBoosting is a general algorithm that is often applied to decision tree models as a way to improve predictive performance through introducing another form of randomization. Boosted models are built sequentially, as each version of the model is fit to the residuals from the previous version.\n\nSGB models use a large number of shallow decision trees as a base learner. These early versions of the model, which are called \"weak models\" are improved sequentially based on the residuals of the previous version. At each sequential step, these weak models are improved using the sequential fitting algorithm of stochastic gradient descent, which uses random sampling of features to optimize the defined loss function (for this classification problem, we will look to optimize accuracy) for each iteration based on the defined learning rate. We start by tuning the learning rate, which specifies the extent to which we want to change our weak models at each iteration. If we choose to low of a learning rate, it may require too many iterations for our model to improve at all, but if we choose a learning rate that is too high, we may accidently skip over a better performing version of the model.\n\n#### Build preliminary model & tune learning rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model for tuning learning rate\nsgb_lr_spec <- boost_tree(mode = \"classification\",\n                      engine = \"xgboost\",\n                      learn_rate = tune())\n\n# create tuning grid for learning rate\ntuning_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n# create workflow for tuning learning rate\nsgb_lr_wf <- workflow() %>%\n  add_model(sgb_lr_spec) %>%\n  add_recipe(tracks_recipe)\n\n# tune learning rate using CV\nsgb_lr_tune <- tune_grid(sgb_lr_wf,\n                         resamples = cv_folds,\n                         grid = tuning_grid,\n                         metrics = metric_set(accuracy))\n\n# store optimized learning rate\nbest_lr <- select_best(sgb_lr_tune)\n```\n:::\n\n\n#### Build preliminary model & tune tree parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model for tuning tree parameters\nsgb_tree_spec <- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate from previous step\n                            trees = 3000, # set number of trees to 3000\n                            tree_depth = tune(), # tune maximum tree depth\n                            min_n = tune(), # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n                            loss_reduction = tune(), # tune loss reduction (minimum loss required for further splits)\n                            mode = \"classification\",\n                            engine = \"xgboost\")\n\n# create tuning grid for tree parameters\ntuning_grid <- grid_latin_hypercube(tree_depth(),\n                                    min_n(),\n                                    loss_reduction(),\n                                    size = 10)\n\n# create workflow for tuning tree parameters\nsgb_tree_wf <- workflow() %>%\n  add_model(sgb_tree_spec) %>%\n  add_recipe(tracks_recipe)\n\n# tune tree parameters using CV\nsgb_tree_tune <- tune_grid(sgb_tree_wf,\n                           resamples = cv_folds,\n                           grid = tuning_grid,\n                           metrics = metric_set(accuracy))\n\n# store optimized tree parameters\nbest_tree <- select_best(sgb_tree_tune)\n```\n:::\n\n\n#### Build preliminary model & tune stochasticity parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model for tuning stochasticity parameters\nsgb_stochastic_spec <- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate\n                                  trees = 3000, # set number of trees to 3000\n                                  tree_depth = best_tree$tree_depth, # use optimized maximum tree depth\n                                  min_n = best_tree$min_n, # use optimized minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n                                  loss_reduction = best_tree$loss_reduction, # use optimized loss reduction (minimum loss required for further splits)\n                                  mtry = tune(), # tune mtry (number of unique feature variables in each subsample)\n                                  sample_size = tune(), # tune sample size (amount of randomly selected data exposed to the fitting routine when conducting stochastic gradient descent at each split)\n                                  mode = \"classification\",\n                                  engine = \"xgboost\")\n\n# specify mtry range based on the number of predictors\nmtry_final <- finalize(mtry(), tracks_train)\n\n# create tuning grid for stochasticity parameters\ntuning_grid <- grid_latin_hypercube(mtry_final,\n                                    sample_size = sample_prop(),\n                                    size = 10)\n\n# create workflow for tuning stochasticity parameters\nsgb_stochastic_wf <- workflow() %>%\n  add_model(sgb_stochastic_spec) %>%\n  add_recipe(tracks_recipe)\n\n# tune stochasticity parameters using CV\nsgb_stochastic_tune <- tune_grid(sgb_stochastic_wf,\n                                 resamples = cv_folds,\n                                 grid = tuning_grid,\n                                 metrics = metric_set(accuracy))\n\n# store optimized stochasticity parameters\nbest_stochastic <- select_best(sgb_stochastic_tune)\n```\n:::\n\n\n#### Build final model & predict testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify final model with optimized parameters\nsgb_final <- finalize_model(sgb_stochastic_spec, best_stochastic)\n\n# fit final model to training data\nsgb_fit <- fit(sgb_final, as.factor(name)~., tracks_train)\n\n# predict testing data\nsgb_predict <- predict(sgb_fit, tracks_test) %>%\n  bind_cols(tracks_test) %>%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nsgb_predict <- predict(sgb_fit, tracks_test, type = \"prob\") %>%\n  bind_cols(sgb_predict) %>%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nsgb_conf_matrix <- sgb_predict %>% \n  conf_mat(truth = name, estimate = .pred_class) %>% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"SGB\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nsgb_accuracy <- accuracy(sgb_predict, truth = name, estimate = .pred_class)\nsgb_roc_auc <- roc_auc(sgb_predict, truth = name, .pred_linus)\nsgb_sensitivity <- sensitivity(sgb_predict, truth = name, estimate = .pred_class)\nsgb_specificity <- specificity(sgb_predict, truth = name, estimate = .pred_class)\n```\n:::\n\n\n## Compare models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# display confusion matrices of all four models\nsingle_tree_conf_matrix + bagged_trees_conf_matrix + rf_conf_matrix + sgb_conf_matrix +\n  plot_layout(nrow = 2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create tibble of accuracy and ROC AUC for all four models\nmetrics_tibble <- tibble(\n  Method = factor(rep(c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\"), times = 2),\n                  levels = c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\")),\n  Metric = rep(c(\"Accuracy\", \"Area under Receiver Operating Characteristic (ROC) curve\"), each = 4),\n  Value = c(single_tree_accuracy$.estimate[1], bagged_trees_accuracy$.estimate[1],\n            rf_accuracy$.estimate[1], sgb_accuracy$.estimate[1],\n            single_tree_roc_auc$.estimate[1], bagged_trees_roc_auc$.estimate[1],\n            rf_roc_auc$.estimate[1], sgb_roc_auc$.estimate[1]))\n\n# create bar plot comparing accuracy and ROC AUC across all four models\nggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  geom_text(aes(label = sprintf(\"%.2f\", Value),\n                y = Value + 0.02),\n            position = position_dodge(width = 0.9),\n            vjust = 0,\n            size = 4) +\n  theme_minimal() +\n  labs(y = \"Metric Value\", x = \"Model\", title = \"Model Comparison\") +\n  scale_fill_brewer(palette = \"BuPu\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        legend.position = \"top\",\n        legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create tibble of accuracy and ROC AUC for all four models\nmetrics_tibble <- tibble(\n  Method = factor(rep(c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\"), times = 2),\n                  levels = c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\")),\n  Metric = rep(c(\"Sensitivity\\n(Accuracy when truth was Linus)\", \"Specificity\\n(Accuracy when truth was Maxwell)\"), each = 4),\n  Value = c(single_tree_sensitivity$.estimate[1], bagged_trees_sensitivity$.estimate[1],\n            rf_sensitivity$.estimate[1], sgb_sensitivity$.estimate[1],\n            single_tree_specificity$.estimate[1], bagged_trees_specificity$.estimate[1],\n            rf_specificity$.estimate[1], sgb_specificity$.estimate[1]))\n\n\n# create bar plot comparing sensitivity and specificity across all four models\nggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  geom_text(aes(label = sprintf(\"%.2f\", Value),\n                y = Value + 0.02),\n            position = position_dodge(width = 0.9),\n            vjust = 0,\n            size = 4) +\n  theme_minimal() +\n  labs(y = \"Metric Value\", x = \"Model\", title = \"Model Comparison\") +\n  scale_fill_brewer(palette = \"Greens\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        legend.position = \"top\",\n        legend.title = element_blank(),\n        legend.key.height = unit(10, \"mm\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThe Stochastic Gradient Boosting (SGB) model performed the best at predicting the testing data, slightly outperforming the random forest model. While both the SGB and random forest models had the same accuracy for correctly classifying songs that were in my collection, the SGB model was slightly better at accurately classifying songs that were in Maxwell's collection. When using a single decision tree, there was a significant drop-off in accuracy, largely due to difficulty classifying songs that were in Maxwell's collection.\n\n## Compare importance of predictor variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare importance of different predictor variables in best performing model\nvip(sgb_fit, method = \"model\", num_features = 13) +\n  ggtitle(\"Importance of features in SGB model\") +\n  labs(caption = \"Note: Importance of time_sig is <0.01\") +\n  ylim(0.00, 0.20) +\n  geom_text(aes(label = sprintf(\"%.2f\", Importance), # label values\n                x = Variable,\n                y = Importance + 0.001),\n            hjust = 0,\n            color = \"black\",\n            size = 3.5) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.text.y = element_text(color = \"black\", size = 12))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nIn the SGB model, which was the best performing model, tempo, valence, danceability, energy, and instrumentalness were the most important feature for predicting whether a song was in my collection or Maxwell's.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}