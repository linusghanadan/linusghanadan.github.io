[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Science",
    "section": "",
    "text": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nPython\n\n\n\n\nThis analysis was originally coded using R for my Statistics final project, but I decided to reproduce the same analysis to practice building statistical models in Python with the statsmodels library.\n\n\n\n\n\n\nAug 20, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nVisualization Portfolio (R)\n\n\n\n\n\n\n\nData Visualization\n\n\nR\n\n\n\n\nA collection of data visualizations that I have created using the R programming language.\n\n\n\n\n\n\nJul 24, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nVisualization Portfolio (Python)\n\n\n\n\n\n\n\nData Visualization\n\n\nPython\n\n\n\n\nA collection of data visualizations that I have created using the Python programming language.\n\n\n\n\n\n\nJul 23, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nRegression Models to predict Phosphorus Concentration based on Time and Location\n\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nPython\n\n\n\n\nIn this blog post, I build two models to explore potential advantages of adding an additional hidden layer to a traditional Recurrent Neural Network (RNN) architecture.\n\n\n\n\n\n\nJul 16, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nCreating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis\n\n\n\n\n\n\n\nAnalytical Workflows\n\n\nInteractive Applications\n\n\nData Visualization\n\n\nMicrosoft Excel\n\n\nR\n\n\n\n\nAs our Master’s program capstone project, our team of four helped Darn Tough, a Vermont-based outdoor clothing company, streamline their process for carbon accounting and sustainability analysis.\n\n\n\n\n\n\nJun 20, 2024\n\n\nLinus Ghanadan, Flora Hamilton, Carly Caswell, Annie Adams\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Simulation of Forest Growth\n\n\n\n\n\n\n\nDynamic Simulation\n\n\nSensitivity Analysis\n\n\nR\n\n\n\n\nUsing a model of forest growth, I run a 300-year continuous dynamic simulation for the forest and then conduct a global sensitivity analysis looking at variation in maximum forest size.\n\n\n\n\n\n\nJun 10, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nGlobal Sensitivity Analysis for an Atmospheric Conductance Model\n\n\n\n\n\n\n\nSensitivity Analysis\n\n\nR\n\n\n\n\nIn this analysis, I look at an atmospheric conductance model based on vegetation height and windspeed, and I apply a variance-based sensitivity analysis by calculating Sobol indices.\n\n\n\n\n\n\nJun 7, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nImpact Analysis of a 1998 Cash-Transfer Program in Rural Mexico\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nEconometric Analysis\n\n\nR\n\n\n\n\nFor this analysis, I estimate the Average Treatment Effect (ATE) of the 1998 Prospera cash-transfer program on the value of animals owned by a household.\n\n\n\n\n\n\nApr 10, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nUsing Propensity Score Methods to analyze the impact of Catch Shares\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nEconometric Analysis\n\n\nR\n\n\n\n\nDisclaimer: The data used in this blog was synthetically generated (i.e., data is fake so results have no real implications).\n\n\n\n\n\n\nApr 9, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nRegression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems\n\n\n\n\n\n\n\nMachine Learning\n\n\nEnsemble Learning\n\n\nPython\n\n\n\n\nFor this blog post, I build and compare four regression models that predict DIC in water samples. I also analyze feature importances in the best performing model.\n\n\n\n\n\n\nApr 3, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nCluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay\n\n\n\n\n\n\n\nMachine Learning\n\n\nCluster Analysis\n\n\nR\n\n\n\n\nFor clustering, I use data from Roberts et al. 2008 on biological contaminants in Port Jackson Bay.\n\n\n\n\n\n\nApr 1, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nClassification Models using data from Spotify Web API\n\n\n\n\n\n\n\nMachine Learning\n\n\nEnsemble Learning\n\n\nR\n\n\n\n\nIn this blog post, I build and compare four classification models that predict whether a given song was in my Spotify collection or that of my friend Maxwell. I also analyze feature importances in the best performing model.\n\n\n\n\n\n\nMar 29, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nInfographic on Anthropogenic Methane Emissions\n\n\n\n\n\n\n\nData Visualization\n\n\nR\n\n\n\n\nThis blog post explains the process of creating an infographic on anthropogenic methane emissions in 2021, with a particular focus on the data visualization considerations and choices made.\n\n\n\n\n\n\nMar 12, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Analysis of 2021 Houston Power Crisis\n\n\n\n\n\n\n\nGeospatial Analysis\n\n\nR\n\n\n\n\nUsing data from the NASA’s VIIRS instrument, I look at where residential blackouts occurred. Then, I visualize this data at the census tract level and analyze the relationship to median income.\n\n\n\n\n\n\nJan 20, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Analysis of PM 2.5 and Race Demographics in Maryland\n\n\n\n\n\n\n\nGeospatial Analysis\n\n\nR\n\n\n\n\nUsing data from the EPA’s Environmental Justice Screening and Mapping Tool (EJScreen), I create basic visualizations looking at the relationship between PM 2.5 concentrations and demographics.\n\n\n\n\n\n\nJan 10, 2024\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Analysis of Nutrient Concentration in Chesapeake Bay\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nR\n\n\n\n\nThis analysis seeks to better understand the seasonality and trends of nitrogen and phosphorus concentration in Chesapeake Bay tidal regions since the 2010 introduction of federal water quality requirements under the Clean Water Act.\n\n\n\n\n\n\nDec 12, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Analysis of Biodiversity Changes in Phoenix, AZ\n\n\n\n\n\n\n\nGeospatial Analysis\n\n\nPython\n\n\n\n\nIn this analysis, I retrieve raster data of Biodiversity Intactness Index (BII) from the Microsoft Planetary Computer (MPC) catalog. This allows me to calculate areas of interest based on changes in biodiversity and show my results on a map.\n\n\n\n\n\n\nDec 11, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nLand Cover Analysis around Mount Whitney\n\n\n\n\n\n\n\nGeospatial Analysis\n\n\nPython\n\n\n\n\nThis analysis uses Python to extract land cover statistics from a USGS raster dataset on land cover classification in the area surrounding Mount Whitney in California.\n\n\n\n\n\n\nDec 11, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Analysis of Allometric Equations used for estimating Carbon Sequestration from Agroforestry Systems\n\n\n\n\n\n\n\nGeospatial Analysis\n\n\nR\n\n\n\n\nAs an undergraduate capstone project, I worked with six other students and The Nature Conservancy to analyze the spatial distribution of scientific studies that used allometric equations for estimating carbon sequestration from agroforestry systems.\n\n\n\n\n\n\nJun 1, 2023\n\n\nLinus Ghanadan, Virginia Borda, Sophie Jabés, Eric Marquez, Michael Deffinbaugh, Troy Wynne, Chelsea Patterson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html",
    "href": "blog/2024-3-12-post/index.html",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "",
    "text": "Infographic on anthropogenic methane emissions in 2021"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#completed-infographic-where-we-are-heading",
    "href": "blog/2024-3-12-post/index.html#completed-infographic-where-we-are-heading",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "",
    "text": "Infographic on anthropogenic methane emissions in 2021"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#link-to-github-repository",
    "href": "blog/2024-3-12-post/index.html#link-to-github-repository",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Link to GitHub repository",
    "text": "Link to GitHub repository"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#purpose",
    "href": "blog/2024-3-12-post/index.html#purpose",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Purpose",
    "text": "Purpose\nIn my infographic, the overarching question that I will be answering is where anthropogenic methane emissions came from in 2021. This includes the countries where emissions are occurring most frequently and also the human activities (e.g., energy production, agriculture, etc.) that contribute the most to these emissions."
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#data",
    "href": "blog/2024-3-12-post/index.html#data",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Data",
    "text": "Data\nThe data set that I will use comes from the International Energy Agency (IEA), a Paris-based intergovernmental organization with 31 member countries and 13 association countries. The group was created following the 1973 oil crisis by the Organisation for Economic Co-operation and Development (OECD) to oversee and collect data on global energy markets. In the last decade, the group has increasingly played an important role in guiding and advocating for an accelerated global energy transition away from fossil fuels (International Energy Agency (IEA) 2024).\nSince 2020, the IEA has published yearly data estimating global methane emissions at a country-level. For methane emissions resulting from oil and gas processes (upstream and downstream), these figures are calculated using a combination of measurement data (mostly from satellite readings) and activity data on the specific actions being taken that release vented, fugitive, or incomplete-flare emissions. Coal mine methane emissions are estimated primarily by looking at the ash content of coal produced in different countries, mine depth, and regulatory oversight. Furthermore, estimating country-level emissions from agriculture and waste mainly relies only satellite technology. Lastly, other methane sources are estimated using manufacturing data and the emissions factors associated with the industrial processes carried out in that country (International Energy Agency (IEA) 2022b).\nI will be using the 2022 data set, which provides emissions estimates for the year 2021 (International Energy Agency (IEA) 2022a). Anyone can access this data set for free after making an account on the IEA website.\nIn addition to the methane data set, I also want data on 2021 population values of different countries for computing emissions per capita, so I downloaded a free data set from the World Bank website, which did not require me to have any sort of account (The World Bank, n.d.)."
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#setup-data-import",
    "href": "blog/2024-3-12-post/index.html#setup-data-import",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Setup & data import",
    "text": "Setup & data import\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                setup                                     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# load packages\nlibrary(here)\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(treemapify)\nlibrary(showtext)\n\n# import fonts\nfont_add_google(name = \"Merriweather Sans\", family = \"merri sans\")\nfont_add_google(name = \"Barlow Condensed\", regular.wt = 200, family = \"barlow\")\n\n# enable {showtext} for rendering\nshowtext_auto()\n\n# set scipen option to a high value to avoid scientific notation\noptions(scipen = 999)\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# read in methane data\nmethane_df &lt;- readr::read_csv(here(\"data\", \"2024-3-12-post-data\", \"IEA-MethaneEmissionsComparison-World.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% # convert column names to lower_case_snake format\n  select('country', 'emissions', 'type') # select relevant columns\n\n# read in population data\npop_df &lt;- readr::read_csv(here(\"data\", \"2024-3-12-post-data\", \"worldbank_pop.csv\")) %&gt;% \n  janitor::clean_names() %&gt;%\n  rename('population' = 'x2021', # rename column with 2022 populations to 'population'\n         'country' = 'country_name') %&gt;% # rename columns with countries (for joining)\n  select('country', 'population') # select these two columns"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#general-data-wrangling",
    "href": "blog/2024-3-12-post/index.html#general-data-wrangling",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "General data wrangling",
    "text": "General data wrangling\nTo start, I have some general data wrangling steps that allowed me to explore the data and calculate some of the statistics that I ended up including in my infographic. In the code chunk below, I’m storing the world-level rows in the IEA data set as their own data frame, and reconfiguring the data frame to find the percent of total global emissions coming from each of the four sectors, which I ended up including in the legend of my first plot.\n\n\nCode\n# store observations regarding entire world as its own df\nworld_df &lt;- methane_df %&gt;%\n  filter(is.na(country)) %&gt;% \n  group_by(country, type) %&gt;% # group by input variables ('type' must be last to combine observations in next line)\n  summarize(total_emissions = sum(emissions, na.rm = TRUE)) %&gt;% # create summary df that combines observations with same 'type'\n  ungroup() %&gt;% \n  pivot_wider(names_from = type, values_from = total_emissions) %&gt;% # create new columns named based on 'type' and containing values from 'total_emissions'\n  janitor::clean_names() %&gt;%\n  mutate(total_emissions = agriculture + energy + waste + other) %&gt;% # re-create 'total_emissions' column\n  select(-country)\n\n# calculate percents of 'total_emissions' coming from each type of emissions (to be put in legend of treemap)\nworld_df$agriculture / world_df$total_emissions\n\n\n[1] 0.2902036\n\n\nCode\nworld_df$energy / world_df$total_emissions\n\n\n[1] 0.545233\n\n\nCode\nworld_df$waste / world_df$total_emissions\n\n\n[1] 0.1446558\n\n\nCode\nworld_df$other / world_df$total_emissions\n\n\n[1] 0.01990765\n\n\nI also perform some general data wrangling on my methane data frame, which will be important moving forward. After removing the world-level rows that I subsetted in the previous code chunk, I’m doing a ‘group_by’ command followed by a ‘summarize’ command to combine observations that are of the same type. Before doing this, there were multiple observations for energy emissions, breaking down into further levels of granularity based on other columns that are not selected here. I’m also combining all countries that are part of the European Union by changing their country names to the same string and then again using the ‘group_by’ and ‘summarize’ commands to combine rows. Lastley, I decided to make a wide form of this same data frame, which will be helpful when we get to plot 2 of my infographic.\n\n\nCode\nmethane_df &lt;- methane_df %&gt;%\n  filter(!(is.na(country))) %&gt;% # remove observations regarding entire world\n  group_by(country, type) %&gt;% # group by input variables ('type' must be last to combine observations in next line)\n  summarize(total_emissions = sum(emissions, na.rm = TRUE)) %&gt;% # create summary df that combines observations with same 'type'\n  ungroup() %&gt;% \n  mutate(country = case_when(country == \"Other EU17 countries\" ~ \"EU*\", # reassign country names for countries in EU so we can combine these observations\n                             country == \"Other EU7 countries\" ~ \"EU*\",\n                             country == \"France\" ~ \"EU*\",\n                             country == \"Italy\" ~ \"EU*\",\n                             country == \"Germany\" ~ \"EU*\",\n                             country == \"Sweden\" ~ \"EU*\",\n                             country == \"Norway\" ~ \"EU*\",\n                             country == \"Poland\" ~ \"EU*\",\n                             country == \"Denmark\" ~ \"EU*\",\n                             country == \"Estonia\" ~ \"EU*\",\n                             country == \"Netherlands\" ~ \"EU*\",\n                             country == \"Slovenia\" ~ \"EU*\",\n                             country == \"Romania\" ~ \"EU*\",\n                             country == \"United States\" ~ \"U.S.\", # shorten United States to U.S.\n                             TRUE ~ country)) %&gt;%\n  group_by(type, country) %&gt;% # group by input variables ('country' must be last to combine observations in next line)\n  summarize(total_emissions = sum(total_emissions, na.rm = TRUE)) %&gt;% # combine observations\n  ungroup()\n\n# create wide version of methane_df so that there is one observation for each 'country' (to be used for next graph)\nwide_df &lt;- methane_df %&gt;%\n  filter(!(country == \"Other\")) %&gt;% # remove observations where 'country' is other\n  filter(!(country == \"Other countries in Europe\")) %&gt;% \n  filter(!(country == \"Other countries in Southeast Asia\")) %&gt;% \n  pivot_wider(names_from = type, values_from = total_emissions) %&gt;% # create new columns named based on 'type' and containing values from 'total_emissions'\n  janitor::clean_names() %&gt;% \n  mutate(energy = ifelse(is.na(energy), 0, energy)) %&gt;% # set NA in 'energy' column to 0 so that next line works\n  mutate(total_emissions = energy + agriculture + waste + other) %&gt;%  # re-create 'total_emissions' column\n  arrange(desc(total_emissions))"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#plot-1-vizualization",
    "href": "blog/2024-3-12-post/index.html#plot-1-vizualization",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Plot 1 vizualization",
    "text": "Plot 1 vizualization\nFor my first plot, I’ll start by making a treemap of how the four different categories (energy, agriculture, waste, and other) of methane emissions and the country that they are in contribute to total global emissions. To do this, I start by taking my methane data frame and making a version of it specifically for my treemap plot with altered names of countries (I didn’t like the way that they looked when they were included) and the types of emissions (adding the percents in that I calculated from my general data wrangling). I then re-level the factors so they appear in descending order, define a custom color palette that I built using the website Coolers, and finally I’m ready to make my actual plot.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          plot 1 visualization                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# rename countries to empty strings so that don't show up in plot\ntreemap_df &lt;- methane_df %&gt;%\n  mutate(country = case_when(country == \"Mexico\" ~ \"\",\n                             country == \"Algeria\" ~ \"\",\n                             country == \"Libya\" ~ \"\",\n                             country == \"Venezuela\" ~ \"\",\n                             country == \"Turkmenistan\" ~ \"\",\n                             country == \"Nigeria\" ~ \"\",\n                             country == \"Pakistan\" ~ \"\",\n                             country == \"Kazakhstan\" ~ \"\",\n                             country == \"Kuwait\" ~ \"\",\n                             country == \"Qatar\" ~ \"\",\n                             country == \"Indonesia\" ~ \"\",\n                             country == \"Other\" ~ \"\",\n                             TRUE ~ country)) %&gt;%\n  mutate(type = case_when(type == \"Agriculture\" ~ \"Agriculture\\n(29%)\", # rename types of emissions to include percents (for legend in plot)\n                          type == \"Energy\" ~ \"Energy\\n(55%)\",\n                          type == \"Waste\" ~ \"Waste\\n(14%)\",\n                          type == \"Other\" ~ \"Other\\n(2%)\",\n                          TRUE ~ type))\n\n# re-order sector factors (for legend in plot)\ntreemap_df &lt;- treemap_df %&gt;%\n   mutate(type = factor(type, levels = c(\"Energy\\n(55%)\", \"Agriculture\\n(29%)\", \"Waste\\n(14%)\", \"Other\\n(2%)\")))\n\n# define custom color palette\ncustom_colors &lt;- c(\n  \"Agriculture\\n(29%)\" = \"#D2B48C\",\n  \"Energy\\n(55%)\" = \"#2F2720\",\n  \"Waste\\n(14%)\" = \"#2ca02c\",\n  \"Other\\n(2%)\" = \"#2B4690\")\n\n# create treemap\nggplot(treemap_df, aes(area = total_emissions, fill = type, label = country, subgroup = type)) + # using sector ('type') for coloring and as subgroups (appear in legend), labeling based on country\n  geom_treemap(color = \"white\", size = 0.5) + # adjust color and size of lines separating rectangles\n  labs(x = \"Data Source: International Energy Agency (IEA)\\n\\n*The EU is a group of 27 countries in Europe.\") + # use x axis title for caption\n  geom_treemap_text(color = \"white\", place = \"center\", grow = TRUE, reflow = TRUE, family = \"barlow\", min.size = 12) + # for text inside the treemap, allow to grow with grow = TRUE, flow onto next line with reflow = TRUE, and set font family to barlow\n  scale_fill_manual(values = custom_colors) +  # apply custom color palette\n  labs(title = \"Sources of Anthropogenic Methane Emissions in 2021\") +\n  theme(axis.title.x = element_text(size = 8, hjust = 1, color = \"grey30\", family = \"merri sans\", margin = margin(20, 0, 0, 0)), # adjust font, fontface, size, and color of x axis title (use hjust = 1 to move to far right since this is caption)\n        legend.position = \"top\", # set legend to top\n        legend.title = element_blank(),\n        legend.text = element_text(size = 15, family = \"barrow\", face = \"bold\"), # set legend font to merri sans\n        legend.title.align = 0.5, # center legend\n        legend.spacing.x = unit(5, \"mm\"), # set space between legend keys\n        legend.background = element_rect(fill = \"#FEF6EC\", color = NA), # change legend background color\n        legend.key.size = unit(4, \"mm\"), # set legend key size\n        plot.title = element_text(family = \"merri sans\", size = 16, hjust = 0.5), # set title font to merri sans\n        plot.background = element_rect(fill = \"#FEF6EC\", color = NA), # change the plot background color\n        panel.background = element_rect(fill = \"#FEF6EC\", color = NA)) # change the panel background color to match"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#plot-2-visualization",
    "href": "blog/2024-3-12-post/index.html#plot-2-visualization",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Plot 2 visualization",
    "text": "Plot 2 visualization\nFor plot 2, I start by creating a new version of the wide-version of my methane data frame for the scatterplot that I will make. I start by changing the names of certain countries to match to the World Bank data set that I’m joining with. After I perform my join, I change the names back.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          joining data frames                             ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# change 'country' names to match population data set and store as new data frame\nscatter_df &lt;- wide_df %&gt;%\n  mutate(country = case_when(\n    country == \"Congo\" ~ \"Congo, Rep.\",\n    country == \"Democratic Republic of Congo\" ~ \"Congo, Dem. Rep.\",\n    country == \"Egypt\" ~ \"Egypt, Arab Rep.\",\n    country == \"Gambia\" ~ \"Gambia, The\",\n    country == \"Brunei\" ~ \"Brunei Darussalam\",\n    country == \"Korea\" ~ \"Korea, Rep.\",\n    country == \"Vietnam\" ~ \"Viet Nam\",\n    country == \"U.S.\" ~ \"United States\",\n    country == \"EU*\" ~ \"European Union\",\n    country == \"Venezuela\" ~ \"Venezuela, RB\",\n    country == \"Iran\" ~ \"Iran, Islamic Rep.\",\n    country == \"Syria\" ~ \"Syrian Arab Republic\",\n    country == \"Yemen\" ~ \"Yemen, Rep.\",\n    country == \"Russia\" ~ \"Russian Federation\",\n    TRUE ~ country))\n\n# join this data frame with pop_df\nscatter_df &lt;- left_join(x = scatter_df, y = pop_df, by = \"country\")\n\n# change 'country' names back to how they were\nscatter_df &lt;- scatter_df %&gt;%\n  mutate(country = case_when(\n    country == \"Congo, Rep.\" ~ \"Congo\",\n    country == \"Congo, Dem. Rep.\" ~ \"Democratic Republic of Congo\",\n    country == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n    country == \"Gambia, The\" ~ \"Gambia\",\n    country == \"Brunei Darussalam\" ~ \"Brunei\",\n    country == \"Korea, Rep.\" ~ \"Korea\",\n    country == \"Viet Nam\" ~ \"Vietnam\",\n    country == \"United States\" ~ \"U.S.\",\n    country == \"European Union\" ~ \"EU*\",\n    country == \"Venezuela, RB\" ~ \"Venezuela\",\n    country == \"Iran, Islamic Rep.\" ~ \"Iran\",\n    country == \"Syrian Arab Republic\" ~ \"Syria\",\n    country == \"Yemen, Rep.\" ~ \"Yemen\",\n    country == \"Russian Federation\" ~ \"Russia\",\n    TRUE ~ country\n  ))\n\n\nNow that the join is complete and my country names are back to normal, I create a new column in my data frame for emissions per capita, which divides total emissions by population. I also multiply this new column by 1,000,000 to convert units from million tons to tons, which produces values that are easier to understand when talking about emissions per person.\nNext, I create a new data frame only containing six countries (China, U.S., Russia, Brazil, Canada, and Australia) and the European Union, as I want to focus on the emissions from these places in my infographic. I take the sum of their emissions and populations and compare that to global emissions and populations, and the resulting percent values will also be included on my infographic as text.\n\n\nCode\n# add 'emissions_pc' column and convert column units\nscatter_df &lt;- scatter_df %&gt;% \n  mutate(emissions_pc = (total_emissions / population) * 1000000) %&gt;% # create 'emissions_pc' column (in tons)\n  mutate(population = population / 1000000) %&gt;% # convert 'population' values from people to millions of people\n  arrange(desc(emissions_pc))\n\n# create new data frame with only the 7 countries of focus (including EU)\nmain_countries &lt;- scatter_df %&gt;%\n  filter(country %in% c(\"China\", \"U.S.\", \"Russia\", \"Brazil\", \"EU*\", \"Canada\", \"Australia\")) %&gt;% \n  arrange(desc(total_emissions))\n\n# add 'population' column to world_df\nworld_df &lt;- world_df %&gt;%\n  mutate(population = 7950946801) %&gt;% # found from row 260 in pop_df\n  mutate(population = population / 1000000) # convert 'population' values from people to millions of people\n\n# calculate the percents of global population and global emissions in countries of focus\nsum(main_countries$population, na.rm = TRUE) / sum(world_df$population, na.rm = TRUE)\n\n\n[1] 0.3287581\n\n\nCode\nsum(main_countries$total_emissions, na.rm = TRUE) / sum(world_df$total_emissions, na.rm = TRUE)\n\n\n[1] 0.4664618\n\n\nAt this point, I just need to get a few more data frames in order to make my scatterplot. I create four new data frames: one for all the countries that I’m not focusing on in my infographic, one for notable countries that I want to include (but not highlight) in my infographic for the sake of comparison, one for just Australia (so I can adjust its label on the plot), and one for the remaining countries of focus. Importantly, I also make a new column that I will use to set the alpha value, which moduates transparency in ggplot, of each point in my scatter plot. I want my countries of focus to not be transparent at all, and the rest of my points to be reasonably transparent. After this, its time to make the scatterplot.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          plot 2 visualization                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# create new data frame excluding the countries of focus\nother_countries &lt;- scatter_df %&gt;% \n  filter(!(country %in% c(\"China\", \"U.S.\", \"Russia\", \"Brazil\", \"EU*\", \"Canada\", \"Australia\")))\n\n# create new data frame with countries to label but not highlight (for context)\nother_notable_countries &lt;- other_countries %&gt;% \n  filter(country %in% c(\"India\", \"Indonesia\", \"Iran\", \"Mexico\", \"Congo\", \"Venezuela\", \"Bangladesh\"))\n\n# create new df for just Australia (so can adjust where label is on plot)\naustralia_df &lt;- scatter_df %&gt;% subset(country == \"Australia\")\n\n# create new df for highlighted countries minus Australia (so can plot separately)\nother_main_countries &lt;- main_countries %&gt;% filter(!(country == \"Australia\"))\n\n# add new column for alpha values to scatter_df\nscatter_df$alpha_value &lt;- ifelse(scatter_df$country %in% c(\"China\", \"U.S.\", \"Russia\", \"Brazil\", \"Australia\", \"Canada\", \"EU*\"), 1, 0.2) # only countries of focus have alpha of 1, all else 0.2\n\n# create scatterplot\nggplot(scatter_df) +\n  geom_point(aes(x = population, y = emissions_pc, alpha = alpha_value), color = \"#020122\") + # use alpha values from the column\n  scale_alpha_identity() + # tell ggplot to use the alpha values as given (without scaling)\n  geom_text(data = australia_df, # add text for Australia\n            aes(label = country, x = population - 5, y = emissions_pc + 15), # position text so does not overlap\n            size = 5, hjust = 0, family = \"barlow\", fontface = \"bold\", check_overlap = TRUE) + # adjust other text features\n  geom_text(data = other_main_countries, # add text for other main countries\n            aes(label = country, x = population + 15, y = emissions_pc), # position text\n            size = 5, hjust = 0, family = \"barlow\", fontface = \"bold\", check_overlap = TRUE) + # adjust other text features\n  geom_text(data = other_notable_countries, # add text for other countries to label\n            aes(label = country, x = population + 15, y = emissions_pc), # position text\n            alpha = 0.2, size = 5, hjust = 0, family = \"barlow\", fontface = \"bold\", check_overlap = TRUE) + # adjust other text features\n  labs(x = \"Population (millions)\",\n       y = \"Per Capita Methane Emissions (tons CO2eq)\", \n       title = \"Population and Per-Capita Anthropogenic Methane Emissions in 2021\",\n       caption = \"Note: 8 countries had per capita emissions &gt;350 tons CO2eq and are not displayed.\\nData Sources: International Energy Agency (IEA), World Bank\\n\\n*The EU is a group of 27 countries in Europe.\\n**Based on calculation from BBC News in 2021 article.⁵\") +\n  scale_x_continuous(limits = c(0, 1550), expand = c(0, 0)) + # set limits on min/max x axis values, use expand to tell to have where two axis meet as origin point\n  scale_y_continuous(limits = c(0, 350), expand = c(0, 0)) + # set limits on min/max y axis values, use expand to tell to have where two axis meet as origin point\n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(), panel.grid.minor = element_blank(),\n        axis.title.x = element_text(family = \"barlow\", face = \"bold\", size = 15, color = \"grey30\", # adjust text features for x axis title\n                                    margin = margin(20, 0, 0, 0)), # set margin\n        axis.title.y = element_text(family = \"barlow\", face = \"bold\", size = 15, color = \"grey30\", # adjust text features for y axis title\n                                    margin = margin(0, 20, 0, 0)), # set margin\n        panel.grid.major.y = element_line(color = \"grey90\", size = 0.5), # add horizontal gridlines (major)\n        panel.grid.minor.y = element_line(color = \"grey90\", size = 0.25), # add horizontal gridlines (minor)\n        axis.text.x = element_text(family = \"barlow\", face = \"bold\", size = 13), # adjust x axis text\n        axis.text.y = element_text(family = \"barlow\", face = \"bold\", size = 13), # adjust y axis text\n        axis.line = element_line(color = \"black\", size = 0.5), # adjust color and size of axis lines\n        plot.title = element_text(family = \"merri sans\", size = 12.5, hjust = 0.5), # adjust plot title text\n        plot.caption = element_text(hjust = 1, size = 8, color = \"grey30\", family = \"merri sans\", # adjust caption text\n                                    margin = margin(20, 0, 0, 0)), # set margin\n        plot.background = element_rect(fill = \"#FEF6EC\", color = NA), # set plot background\n        panel.background = element_rect(fill = \"#FEF6EC\", color = NA)) + # set panel background\n  geom_hline(yintercept = 197, linetype = \"dashed\", linewidth = 0.8, color = \"cornflowerblue\") + # add dashed horizontal line at y = 197\n  annotate(\"text\", x = 700, y = 240, # position text annotation\n           label = \"Carbon footprint of a typical\\nprivate jet flying for 48 hours**\",\n           size = 5, hjust = 0, color = \"cornflowerblue\", family = \"barlow\", fontface = \"bold\") # adjust text features"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#plot-3-visualization",
    "href": "blog/2024-3-12-post/index.html#plot-3-visualization",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Plot 3 visualization",
    "text": "Plot 3 visualization\nFor my third plot, I don’t have too much extra wrangling to do. I have to make a new version of my data frame for my main countries. I can’t use the one from before because it is in wide format, and for my dodged column plot, I need it in long format. After making my new version just by filtering my original methane data frame, I re-order my country and type factors so that they are in descending order, redefine my custom color palette, and then make my dodged column plot.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          plot 3 visualization                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# filter methane_df to create long format of main_countries\ncols_df &lt;- methane_df %&gt;% \n  filter(country %in% c(\"China\", \"U.S.\", \"Russia\", \"Brazil\", \"EU*\", \"Canada\", \"Australia\"))\n\n# re-order countries as factors (for plotting)\ncols_df$country &lt;- factor(cols_df$country,\n                          levels = c(\"China\", \"U.S.\", \"Russia\", \"Brazil\", \"EU*\", \"Canada\", \"Australia\"))\n\n\n# re-order countries as factors (for plotting)\ncols_df$type &lt;- factor(cols_df$type,\n                          levels = c(\"Energy\", \"Agriculture\", \"Waste\", \"Other\"))\n\n# define a custom color palette\ncustom_colors &lt;- c(\n  \"Agriculture\" = \"#D2B48C\",\n  \"Energy\" = \"#2F2720\",\n  \"Waste\" = \"#2ca02c\",\n  \"Other\" = \"#2B4690\")\n\n# create dodged column plot\nggplot(cols_df, aes(x = country, y = total_emissions, fill = type)) + # fill columns based on sector\n  geom_col(position = \"stack\") + # specify dodged position to add space between countries\n  labs(x = \"\", # no x axis title\n       y = \"Methane Emissions (million tons CO2eq)\",\n       title = \"Sources of 2021 Anthropogenic Methane Emissions in selected countries\",\n       caption = \"Data Source: International Energy Agency (IEA)\\n\\n*The EU is a group of 27 countries in Europe.\") +\n  scale_fill_manual(values = custom_colors) + # apply custom color palette\n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), # remove major and minor vertical grid lines\n        panel.grid.major.y = element_line(color = \"grey90\", size = 0.5), # add horizontal grid lines (major)\n        panel.grid.minor.y = element_line(color = \"grey90\", size = 0.25), # add horizontal grid lines(minor)\n        plot.caption = element_text(size = 8, hjust = 1, colour = \"grey30\", family = \"merri sans\", # adjust caption text\n                                    margin = margin(20, 0, 0, 0)), # set margin\n        axis.title.y = element_text(family = \"barlow\", face = \"bold\", size = 15, color = \"grey30\", # adjust y axis title text\n                                    margin = margin(0, 20, 0, 0)), # set margin\n        axis.text.x = element_text(family = \"barlow\", face = \"bold\", size = 16), # adjust x axis text\n        axis.text.y = element_text(family = \"barlow\", size = 13, face = \"bold\"), # adjust x axis text\n        legend.position = c(0.88, 0.85), # specify legend position\n        legend.text = element_text(color = \"grey30\", face = \"bold\", size = 16, family = \"barlow\"), # adjust legend text\n        legend.title = element_blank(),\n        plot.title = element_text(family = \"merri sans\", size = 12.5, hjust = 0.5), # adjust plot title text and alignment\n        legend.background = element_rect(fill = \"#FEF6EC\", color = NA), # change legend background color\n        plot.background = element_rect(fill = \"#FEF6EC\", color = NA), # change plot background color\n        panel.background = element_rect(fill = \"#FEF6EC\", color = NA)) # change panel background color"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#map-visualization",
    "href": "blog/2024-3-12-post/index.html#map-visualization",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Map visualization",
    "text": "Map visualization\nI also want to add a world map that highlights the countries that I’m highlighting in the second and third visualizations (China, the U.S., Russia, the EU, Canada, and Australia), which requires two additional packages ({rnaturalearth} for basemap and {sf} for importing geometric objects).\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                       setup & data wrangling                             ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# load additional packages\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# get world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# exclude Antarctica from the dataset\nworld &lt;- world[world$name != \"Antarctica\", ]\n\n# specify the countries to highlight\ncountries_to_highlight &lt;- c(\"China\", \"United States of America\", \"Russia\", \"Brazil\",\n                            \"Australia\", \"Canada\",\n                            \"Austria\", \"Belgium\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Czech Republic\", \n                            \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \n                            \"Ireland\", \"Italy\", \"Latvia\", \"Lithuania\", \"Luxembourg\", \"Malta\", \"Netherlands\", \n                            \"Poland\", \"Portugal\", \"Romania\", \"Slovakia\", \"Slovenia\", \"Spain\", \"Sweden\")\n\n# specify the countries that will receive a custom label (will use Denmark to label EU)\ncountries_to_label &lt;- c(\"China\", \"United States of America\", \"Russia\", \"Brazil\",\n                        \"Australia\", \"Canada\", \"Denmark\")\n\n# filter world data for highlighted and labeled countries\nhighlighted_countries &lt;- world[world$name %in% countries_to_highlight, ]\nlabeled_countries &lt;- world[world$name %in% countries_to_label, ]\n\n\n# calculate centroids for countries to label and store in data frame\ncentroids &lt;- st_centroid(labeled_countries$geometry)\ncentroids_df &lt;- data.frame(name = labeled_countries$name,\n                           lon = st_coordinates(centroids)[,1],\n                           lat = st_coordinates(centroids)[,2])\n\n# change 'United States of America' to 'U.S.' and 'Denmark' to 'EU*'\ncentroids_df$name &lt;- recode(centroids_df$name,\n                            'United States of America' = 'U.S.',\n                            'Denmark' = 'EU*')\n\n# adjust centroid longitude to move the labels left or right\ncentroids_df &lt;- centroids_df %&gt;%\n  mutate(lon = case_when(\n    name == \"Russia\" ~ lon + 50,\n    name == \"U.S.\" ~ lon + 60,\n    name == \"EU*\" ~ lon - 20,\n    name == \"Canada\" ~ lon - 1,\n    name == \"China\" ~ lon + 55,\n    name == \"Brazil\" ~ lon + 45,\n    TRUE ~ lon\n  ))\n\n# adjust centroid latitude to move the labels up or down\ncentroids_df &lt;- centroids_df %&gt;%\n  mutate(lat = case_when(\n    name == \"Russia\" ~ lat + 20,\n    name == \"Canada\" ~ lat + 24.5,\n    name == \"U.S.\" ~ lat - 10,\n    name == \"China\" ~ lat - 10,\n    name == \"Australia\" ~ lat - 20,\n    name == \"Brazil\" ~ lat - 20,\n    TRUE ~ lat\n  ))\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              create map                                  ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nggplot(data = world) +\n  geom_sf(fill = \"gray\", color = \"grey90\", linewidth = 0.05) +\n  geom_sf(data = highlighted_countries, fill = \"#020122\", size = 0.5) +\n  geom_text(data = centroids_df, aes(x = lon, y = lat, label = name), hjust = \"right\", color = \"#020122\", family = \"barlow\", fontface = \"bold\", size = 7) +\n  labs(caption = \"*The EU is a group of 27 countries in Europe\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(), axis.ticks.x = element_blank(),\n        axis.title = element_blank(),\n        plot.background = element_rect(fill = \"#93B88E\", color = NA),\n        panel.background = element_rect(fill = \"#93B88E\", color = NA),\n        plot.caption = element_text(hjust = 1, size = 11, color = \"#020122\", family = \"merri sans\", margin = margin(20, 0, 0, 0)),)"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#making-infographic",
    "href": "blog/2024-3-12-post/index.html#making-infographic",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "Making infographic",
    "text": "Making infographic\nTo create my infographic, I rendered my work in R to an HTML document and dragged the embedded plots onto a Canva document, which I then designed further and added text to as well. At the end of all my diligent work, I made the following infographic!\n\n\n\nInfographic on anthropogenic methane emissions in 2021"
  },
  {
    "objectID": "blog/2024-3-12-post/index.html#my-visualization-approach",
    "href": "blog/2024-3-12-post/index.html#my-visualization-approach",
    "title": "Infographic on Anthropogenic Methane Emissions",
    "section": "My visualization approach",
    "text": "My visualization approach\nWhen creating my plots in R and then my infographic in Canva, I thought through several aspects that are integral to effective data visualization practice. In this final section, I’ll go through each of these aspects and comment on my approach.\n\nGraphic form\nFor my first plot, I decided to do a treemap because it is a useful way to compare different parts of a ‘whole’, which in this case was total global emissions. Generally, treemaps are a better way to display this than a pie chart, as humans are much worse at understanding relative sizes that are part of a rounded object. In my second plot, I decided on scatterplot because I thought that seeing the distribution of per capita emissions among all countries, with the countries of focus highlighted, would be the easiest way to put understand how these countries compared to others. For my third plot, I made a dodged column plot because it seemed like a simple and effective way to communicate to my audience the nuances of how each of the countries of focus have emissions coming from different types of sources.\n\n\nText\nThis is a broad category, and I made a lot of decisions regarding the text used in my plots. Every word of text that I included was thoughtfully considered. My most major decisions were in my scatterplot, where I included labeled points (not only of focus countries but also other comparison countries), an annotation (for contextualizing emissions data), and a note in my caption to tell the audience directly that there were actually 8 countries with even higher per capita emissions than what was shown within the bounds of the plot.\n\n\nThemes\nI made a great deal of theme adjustments in each of my plots. For the first plot, the main theme choice was to orient my legend the way that I did, at the very top of the plot so the reader would read it immediately. In the second and third plot, I decided to get rid of vertical grid lines but keep some horizontal grid lines to help my audience to more easily see where countries lined up along the y-axis. I also moved the axis titled farther away from the axis in both of these plots, which I think is easier on the eyes.\n\n\nColors\nI made a custom color palette that I thought was unique, aesthetically pleasing, color-blind friendly (checked using the “let’s get color blind” Chrome Extension), and fit the variables that I was representing. I reused the colors from my first plot in my last plot, where they represented the same thing in each. I also reused the dark blue color quite a bit in my infographic, most notabley for shading the highlighted areas in my map and then to highlight these same countries as point in my scatterplot. All of the text that I wrote in Canva was also this color.\n\n\nTypography\nI chose to use Merriweather Sans for the title and captions of my plots and Barlow Condensed to use for all other text within my plots (labeling rectangles in plot 1, labeling points in plot 2, plot 2 annotation, legend in plot 1 and 2, all axis titles and text). Using a condensed typeface within my plots was nice because it allowed me to fit text more easily. I also think that it gave my visuals a more serious tone, especially since it was so thin that I had to bold it everywhere that I used it. I definetly spent more time selecting this typeface, and then I determined that Merriweather Sans was a good matching typeface for titles and captions of my plots. In Canva, I used the typeface Alike for most text, as it seemed both unique but still easy to read. I used Agrandir for my infographic title and other larger text in my infographic. Its a pretty basic typeface, but I couldn’t find anything that I liked better.\n\n\nGeneral design\nI definitely thought a lot about where I wanted my readers eyes to go. This is why I re-ordered factors so that the legends where in descending order in the first and third plot. I also re-ordered the factors my columns to be in descending order in my third plot. To avoid information overload in my first plot, I changed the name of many countries to empty strings. I also selected points to label carefully in my second plot for this reason. In addition, I also feel like I did a good job combining the plot and text elements in Canva so that the ideas flowed naturally using a visual hierarchy.\n\n\nContextualizing data\nAs previously mentioned, I used an annotation in my second plot to contextualize emissions to my audience. Recently, there have been a lot of news stories about the use of private jets among celebrities, so I felt like this was a good way to contextualize emissions, especially since the plot had to do with emissions per individual and flying a private jet is a very individualistic action. In my second plot, I also added the labels for other countries that weren’t the primary focus, like Venezuela, Congo, Mexico, and Bangladesh, as a way to provide more context for the emissions of countries around the world.\n\n\nCentering my primary message\nI think that the map that I included alongside the large text in the middle of my infographic was very effective at centering my message that there are some countries that have a disproportionately large amount of methane emissions relative to their population. Another point that I wanted to emphasize was how methane is mostly from fossil fuels, which is one reason why I put my treemap first. I also feel like the point I discussed in the second paragraph of text, right after my intro, emphasized this point by mentioning the 2021 study by McGill University scientists.\n\n\nConsidering accessibility\nI spent a great deal of time ensuring that the colors I chose were color-blind friendly using the “let’s get color blind” Chrome Extension. I tested using the simulate, daltonized, and simulate daltonized versions of Deuteranomaly, Protanomaly, and Tritanomaly. I also wrote alt text (embedded in images below) for all three of my plots (and my map) to ensure that blind users could still understand the figures in my infographic.\n\n\n\nTreemap (rectangle divided up into smaller rectangles) where each smaller rectangle represents anthropogenic methane emissions in 2021 from a specific country and sector (rectangles are colored by sector). Energy makes up 55% of global emissions, about half of which comes from the China, Russia, the U.S., Iran, and India. Agriculture makes up 29% of global emissions, waste makes up 14%, and 2% are from other sources.\n\n\n\n\n\nMap of the world where six countries (Canada, the U.S., Brazil, Russia, China, and Australia) and the 27 countries of the European Union are highlighted.\n\n\n\n\n\nScatterplot of many countries showing 2021 population on the x-axis and 2021 per-capita methane emissions (in tons of CO2eq) on the y-axis. Canada, the U.S., Brazil, Russia, China, and Australia are labeled and emphasized. Australia and Russia are in the top left of the plot, with per-capita emissions around 300 tons CO2eq. Australia’s population is about 25 million people, while Russia’s is about 200 million. Canada is around 200 tons CO2 eq, right below a dashed line indicating the carbon footprint of a typical private jet flying for 48 hours. Canada’s population is around 35 million people. The U.S. is at around 150 tons CO2eq and has a population of about 300 million people. Brazil is at around 100 tons CO2eq and has a population of about 250 million people. China is at around 55 tons CO2eq and has a population of about 1,400 million (1.4 billion) people. Latley, the EU is at around 45 tons CO2eq and has a population of about 450 million people.\n\n\n\n\n\nBar graph with 7 vertical columns, where the height represents the total methane emissions in 2021 (measured in million tons of CO2eq). From left to right (aligns with highest to lowest total emissions), the 7 vertical columns are China, the U.S. Russia, Brazil, the EU, Canada, and Australia. China is at about 80,000 million tons CO2eq, the U.S. and Russia are both close to 45,000 tons CO2eq, Brazil and the EU are both around 20,000 tons CO2eq, and Canada and Australia are both less than 10,000 tons CO2eq. The 7 columns are each seperated into four colored components, one for each type of emissions source (energy, agriculture, waste, and other). Russia’s energy sector (85%) and Brazil’s agricultural sector (65%) stand out as particularly high. Emissions from energy in the EU (29%) and Brazil (16%) make up a relatively low share of their total emissions, compared to about 60 to 70% in China, the U.S., Canada, and Australia.\n\n\n\n\nApplying a lense of Diversity, Equity, & Inclusion (DEI)\nOne decision that I thought about through a DEI lense was the decision of which countries to highlight in my plots, as I had some leeway in these decisions. I wanted to ensure that I wasn’t being biased towards any specific country when excludeing or singling-out specific countries in my analysis. This was somewhat difficult when highlighting countries because I was kind of singling them out, but I tried my best to highlight many examples of countries around the world to avoid really placing the blame exclusively on a certain type of country."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#question",
    "href": "blog/2024-3-6-post/index.html#question",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Question",
    "text": "Question\nWhat was the average treatment effect (ATE) of the 1998 Progresa cash-transfer program on the value of animals owned by a household?"
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#background",
    "href": "blog/2024-3-6-post/index.html#background",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Background",
    "text": "Background\nAccording to the World Health Organization, cash-transfer programs are a form of social assistance assisting beneficiaries who are vulnerable to impoverishment without support. These programs assist beneficiaries through providing re-occurring cash payments, which can either be provided unconditionally or conditional to certain requirements (e.g., periodic health visits). Through boosting household income, the goal of cash-transfer programs is usually to improve the food security of families and provide them the resources to get medical care and prioritize childhood education."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#data",
    "href": "blog/2024-3-6-post/index.html#data",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Data",
    "text": "Data\nOur data comes from a 2012 research paper published in the American Economic Journal looking at the Progresa cash-transfer program, which was implemented in rural Mexican villages in 1998. Eligible households that were randomly selected to be part of the program were provided bi-monthly cash-transfers of up to 550 pesos per month. These cash-transfers were conditional on children attending school, family members obtaining preventive medical care, and attending health-related education talks. In total, over 17,000 households were part of the Progresa program.\nThe outcome and treatment variables are:\n\nvani = value of animals owned by household (in 1997 USD)\ntreatment = dummy variable indicating whether an individual was part of the cash-transfer program (equal to 1 if the individual was part of the program)\n\nThere are 55 control variables, including:\n\ndirtfloor97 = dummy variable indicating whether a household had a dirt floor in 1997\nelectricity97 = dummy variable indicating whether a household had electricity in 1997\nhomeown97 = dummy variable indicating whether a household owned a house in 1997\nfemale_hh = dummy variable indicating whether a household has a female head of household\nage_hh = head of household age\neduc_hh = head of household years of education"
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#setup",
    "href": "blog/2024-3-6-post/index.html#setup",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n# Import packages\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(Hmisc)\nlibrary(RItools)\nlibrary(MatchIt)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\n\n\nCode\n## Load the datasets\nprogresa_pre_1997 &lt;- read_csv(here::here(\"data\", \"2024-3-6-post-data\", \"progresa_pre_1997.csv\"))\nprogresa_post_1999 &lt;- read_csv(here::here(\"data\", \"2024-3-6-post-data\", \"progresa_post_1999.csv\"))\n\n## Append post to pre dataset \nprogresa &lt;- rbind(progresa_pre_1997, progresa_post_1999)\n\n# Remove all families who were treated/controls in the program, but did not get measured in the second year\nprogresa &lt;- progresa %&gt;%\n  group_by(hhid) %&gt;% filter(n() == 2) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#comparison-of-pre-treatment-characteristics-in-the-treatment-and-control-groups",
    "href": "blog/2024-3-6-post/index.html#comparison-of-pre-treatment-characteristics-in-the-treatment-and-control-groups",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Comparison of pre-treatment characteristics in the treatment and control groups",
    "text": "Comparison of pre-treatment characteristics in the treatment and control groups\n\nDirt floor in 1997 (dummy variable)\n\n\nCode\n# Subset data for treatment and control groups\ntreatment_group &lt;- progresa[progresa$treatment == 1, ]\ncontrol_group &lt;- progresa[progresa$treatment == 0, ]\n\n# Compare proportion of units that had dirt floor in 1997 (binary variable) between treatment and control group\nprop.test(x = c(sum(treatment_group$dirtfloor97, na.rm = TRUE), sum(control_group$dirtfloor97, na.rm = TRUE)),\n          n &lt;- c(length(treatment_group$dirtfloor97), length(control_group$dirtfloor97)))\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(treatment_group$dirtfloor97, na.rm = TRUE), sum(control_group$dirtfloor97, na.rm = TRUE)) out of n &lt;- c(length(treatment_group$dirtfloor97), length(control_group$dirtfloor97))\nX-squared = 99.041, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.04486458 0.06704500\nsample estimates:\n   prop 1    prop 2 \n0.6394000 0.5834452 \n\n\n\n\nElectricity in 1997 (dummy variable)\n\n\nCode\n# Compare proportion of units that had electricity in 1997 (binary variable) between treatment and control group\nprop.test(x = c(sum(treatment_group$electricity97, na.rm = TRUE), sum(control_group$electricity97, na.rm = TRUE)),\n          n &lt;- c(length(treatment_group$electricity97), length(control_group$electricity97)))\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(treatment_group$electricity97, na.rm = TRUE), sum(control_group$electricity97, na.rm = TRUE)) out of n &lt;- c(length(treatment_group$electricity97), length(control_group$electricity97))\nX-squared = 147.97, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.08027241 -0.05800589\nsample estimates:\n   prop 1    prop 2 \n0.5583031 0.6274422 \n\n\n\n\nOwned home in 1997 (dummy variable)\n\n\nCode\n# Compare proportion of units that owned home in 1997 (binary variable) between treatment and control group\nprop.test(x = c(sum(treatment_group$homeown97, na.rm = TRUE), sum(control_group$homeown97, na.rm = TRUE)),\n          n &lt;- c(length(treatment_group$homeown97), length(control_group$homeown97)))\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(treatment_group$homeown97, na.rm = TRUE), sum(control_group$homeown97, na.rm = TRUE)) out of n &lt;- c(length(treatment_group$homeown97), length(control_group$homeown97))\nX-squared = 51.805, df = 1, p-value = 6.128e-13\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.02251224 0.03967849\nsample estimates:\n   prop 1    prop 2 \n0.8460096 0.8149142 \n\n\n\n\nFemale head of house (dummy variable)\n\n\nCode\n# Compare proportion of units that had female head of house (binary variable) between treatment and control group\nprop.test(x = c(sum(treatment_group$female_hh, na.rm = TRUE), sum(control_group$female_hh, na.rm = TRUE)),\n          n &lt;- c(length(treatment_group$female_hh), length(control_group$female_hh)))\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(sum(treatment_group$female_hh, na.rm = TRUE), sum(control_group$female_hh, na.rm = TRUE)) out of n &lt;- c(length(treatment_group$female_hh), length(control_group$female_hh))\nX-squared = 9.0087, df = 1, p-value = 0.002687\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.016058579 -0.003296729\nsample estimates:\n    prop 1     prop 2 \n0.07980780 0.08948546 \n\n\n\n\nHead of house age\n\n\nCode\n# Mean difference t-test for head of house age\nt.test(age_hh ~ treatment, data = progresa)\n\n\n\n    Welch Two Sample t-test\n\ndata:  age_hh by treatment\nt = 12.657, df = 24438, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 2.008817 2.744967\nsample estimates:\nmean in group 0 mean in group 1 \n       46.70221        44.32532 \n\n\n\n\nHead of house years of education\n\n\nCode\n# Mean difference t-test for head of house years of education\nt.test(educ_hh ~ treatment, data = progresa)\n\n\n\n    Welch Two Sample t-test\n\ndata:  educ_hh by treatment\nt = -1.2186, df = 25414, p-value = 0.223\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.10171053  0.02372436\nsample estimates:\nmean in group 0 mean in group 1 \n       2.658340        2.697333 \n\n\n\n\nFindings\nFor several of the covariates, there are statistically significant differences between the pre-treatment characteristics of the treated and untreated groups. This indicates that there were likely systemic differences between the pre-treatment characteristics of individuals that were part of the cash-transfer program and those that were not, and if this is the case, simply controlling for all covariates is an insufficient method to estimate the ATE, since systemic differences means that there are differences between the groups that extend beyond what we can control for.\nBecause there seem to be systemic differences between the pre-treatment characteristics of the treated and untreated group, we will use more advanced techniques to estimate the ATE. Since we are working with panel data, our options for advanced techniques include the FD, FE, or DiD estimators."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#estimating-ate-with-the-first-difference-fd-estimator",
    "href": "blog/2024-3-6-post/index.html#estimating-ate-with-the-first-difference-fd-estimator",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Estimating ATE with the First-Difference (FD) estimator",
    "text": "Estimating ATE with the First-Difference (FD) estimator\nBecause a FD model controls for the differences in the explanatory variables between two time periods, the estimator is effective at removing bias from omitted variables that result from differences between time periods. If we think that the potential ommited variables (i.e., a variable that is influencing our outcome variable but is not included as a covariate) that are most important are likely to vary over different time periods, then using the FD estimator is the best approach for estimating ATE. For example, if the head of house having to deal with a family emergency was by far the most influential omitted variable, the FD estimator would likely be the best approach for estimating ATE, since having to deal with a family emergency is something that would likely vary over different time periods.\n\n\nCode\n# i. Sort the panel data in the order in which you want to take differences, i.e. by household and time.\nprogresa_sorted &lt;- progresa %&gt;% \n  arrange(hhid, year) %&gt;%\n  group_by(hhid) %&gt;%\n  \n  # ii. Calculate the first difference using the lag function from the dplyr package.\n  mutate(vani_fd = vani - dplyr::lag(vani)) \n\n# iii. Estimate manual first-difference regression (Estimate the regression using the newly created variables.)\nfd_manual &lt;- lm(vani_fd ~ treatment, data = progresa_sorted)\n\n# Extracting the coefficients table\nsummary_reg &lt;- summary(fd_manual)\nsummary_reg$coefficients %&gt;% \n  kbl(caption = \"FD estimator\") %&gt;%  # Generate table\n  kable_classic(full_width = FALSE)\n\n\n\nFD estimator\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-1156.752\n64.4938\n-17.935859\n0.0000000\n\n\ntreatment\n287.905\n85.6020\n3.363297\n0.0007723\n\n\n\n\n\n\n\nOur FD regression tells us that program participants experienced a change in the value of their animal holdings that was, on average, 287.90 dollars greater than the change experienced by non-participants from 1997 to 1999. Our standard error is 85.60 dollars, and our low p-value means we reject the null hypothesis that the difference is zero at an alpha level of 0.01."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#estimating-ate-with-the-fixed-effects-fe-estimator",
    "href": "blog/2024-3-6-post/index.html#estimating-ate-with-the-fixed-effects-fe-estimator",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Estimating ATE with the Fixed-Effects (FE) estimator",
    "text": "Estimating ATE with the Fixed-Effects (FE) estimator\nA FE model directly controls for omitted variables that do no change over time, so the estimator is effective at removing bias that comes from time-invariant characteristics. If we think that the potential ommited variables (i.e., a variable that is influencing our outcome variable but is not included as a covariate) that are most important are likely stay constant over different time periods, then using the FE estimator is the best approach for estimating ATE. For example, if the head of house being an only child was by far the most influential omitted variable, the FE estimator would likely be the best approach for estimating ATE, since being an only child as an adult is unlikely to be something that would change over time periods.\n\n\nCode\n# ESTIMATE THE BASIC 'WITHIN' FIXED EFFECTS REGRESSION\n# NOTE \"plm\" ONLY PRODUCES CLUSTER-ROBUST STANDARD ERRORS\nwithin_reg &lt;- plm(vani ~ treatment, index = c(\"state\", \"year\"), model = c(\"within\"), effect = c(\"twoways\"), data = progresa)\n\n# Extracting the coefficients table\nsummary_reg &lt;- summary(within_reg)\nsummary_reg$coefficients %&gt;% \n  kbl(caption = \"FE estimator\") %&gt;%  # Generate table\n  kable_classic(full_width = FALSE)\n\n\n\nFE estimator\n\n\n\nEstimate\nStd. Error\nt-value\nPr(&gt;|t|)\n\n\n\n\ntreatment\n-234.0142\n56.65699\n-4.130368\n3.63e-05\n\n\n\n\n\n\n\nOur FE regression tells us that program participants experienced a change in the value of their animal holdings that was, on average, 234.01 dollars less than the change experienced by non-participants within each State from 1997 to 1999. Our cluster-standard error is 56.66 dollars, and our low p-value means we reject the null hypothesis that the difference is zero at an alpha level of &lt;0.01. The standard error being cluster-robust means that it accounts for the fact that observations in the same State as one another will have results that are not entirely independent of one another."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#estimating-ate-with-the-difference-in-difference-did-estimator",
    "href": "blog/2024-3-6-post/index.html#estimating-ate-with-the-difference-in-difference-did-estimator",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Estimating ATE with the Difference-in-Difference (DiD) estimator",
    "text": "Estimating ATE with the Difference-in-Difference (DiD) estimator\nIf we think that our omitted variables are likely to be a mix of variables that stay constant and change across time periods (and treatment occurs only at a single point in time), we are best off using the DiD estimator, which calculates ATE as the difference in the mean outcome variable in the treated group before and after the time of treatment minus the difference in the mean outcome variable in the untreated group before and after the time of treatment.\n\n\nCode\n# Create the dummy variables\nprogresa$treatment_dummy &lt;- ifelse(progresa$treatment == 1, 1, 0)\nprogresa$post_treatment_time_dummy &lt;- ifelse(progresa$year == 1999, 1, 0)\nprogresa$interaction_dummy &lt;- progresa$treatment_dummy * progresa$post_treatment_time_dummy\n\n# OLS regression\nols_reg &lt;- lm(vani ~ treatment_dummy + post_treatment_time_dummy + interaction_dummy, data = progresa)\n\n# Present Regressions in Table\nsummary_reg &lt;- summary(ols_reg)\nsummary_reg$coefficients %&gt;% \n  kbl(caption = \"DiD estimator\") %&gt;%  # Generate table\n  kable_classic(full_width = FALSE)\n\n\n\nDiD estimator\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n2848.2175\n60.61455\n46.989010\n0.0000000\n\n\ntreatment_dummy\n-237.6927\n80.45311\n-2.954425\n0.0031352\n\n\npost_treatment_time_dummy\n-1156.7517\n85.72191\n-13.494235\n0.0000000\n\n\ninteraction_dummy\n287.9050\n113.77788\n2.530413\n0.0113985\n\n\n\n\n\n\n\nFrom our regression, we estimate the average effect of the cash transfer program on value of animal holdings to be about 287.91 USD (where program participants had a higher average value of animal holdings at the end of the program), with a standard error of about 113.78 USD. To interpret this result as the ATE, we have to assume that the control group (units that did not participate in program) provides a valid counterfactual for what would have happened to units in our treatment group (program participants) had they not participated in the program. Furthermore, our p-value of 0.011 tells us that, at an alpha level of 0.05, we reject the null hypothesis that there was no average effect of the cash transfer program on value of animal holdings.\nThe coefficient on our treatment dummy variable tells us that we estimate the mean difference in the outcome variable (value of animal holdings) between the treatment group (program participants) and the control group (non-participants) before the program started to have been 237.69 USD (where program participants had a lower average value of animal holdings than non-participants prior to the start of the program), with a standard error of 80.45 USD. Our p-value of &lt;0.01 tells us that, at an alpha level of 0.01, we reject the null hypothesis that there was a mean difference of zero.\nThe coefficient on our post treatment time dummy variable tells us that we estimate the mean change in the outcome variable (value of animal holdings) between the beginning and end of the program for the control group (non-participants in program) to be 1,156.75 USD (where non-participants had a lower average value of animal holdings when the program ended than when it started), with a standard error of 85.72 USD. Our p-value of 0.003 tells us that, at an alpha level of 0.01, we reject the null hypothesis that there was a mean change of zero."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#conclusion",
    "href": "blog/2024-3-6-post/index.html#conclusion",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, the cash-transfer program appears to have been quite successful at boosting the value of animals owned by a household. In this cash-transfer program, all treatment occurred at the same time and we expect our omitted variables to be a mix of variable that change and stay constant across time periods, so the DiD estimator is likely the best method for estimating ATE in our situation. Using the DiD estimator, we estimated the average effect of the cash transfer program on value of animal holdings to be about 287.91 USD (where program participants had a higher average value of animal holdings at the end of the program), with a standard error of about 113.78 USD. For this to be interpreted as the ATE, we have to assume that the control group (units that did not participate in program) provides a valid counterfactual for what would have happened to units in our treatment group (program participants) had they not participated in the program."
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "The purpose of this analysis is to better understand and visualize biodiversity in Phoenix and highlight areas where biodiversity is declining. Specifically, the final map will show a map of Phoenix displayed on 100 meter grid-cells colored based on the area’s 2020 Biodiversity Intactness Index (BII), which is a score from 0 to 1. In addition, areas where BII declined from greater than 0.75 in 2017 to less than 0.75 in 2020 will be highlighted in a seperate color.\n\n\n\n\nAdd basemap with Contextily\nFetch items from Microsoft Planetary Computer (MPC) catalog using search criteria\nClip biodiversity raster based on polygon from shapefile of Arizona subdivisions\nCalculate percent of area with BII&gt;0.75 in 2017 and 2020\nVisualize 2020 biodiversity and changes from 2017\n\n\n\n\n\nThe primary dataset used in this analysis estimates terrestrial Biodiversity Intactness as a 100-meter gridded maps for years 2017 to 2020. The data contained in the dataset comes from Impact Observatory and Vizzuality, and they generated the data using a database of spatially referenced observations of biodiversity across 32,000 sites and over 750 studies. The data was accessed from the Microsoft Planetary Computer (MPC) catalog.\nA dataset from the U.S. Census Bureau was used to clip biodiversity raster.\n\n\n\n\n\nImpact Observatory & Vizzuality. 2022. “Biodiversity Intactness.” Accessed via Microsoft Planetary Computer (MPC) Catalog. https://planetarycomputer.microsoft.com/dataset/io-biodiversity#overview.\nU.S. Census Bureau. 2022. “2022 TIGER/Line Shapefiles: County Subdivision”. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions\n\n\n\n\n\n\n# Load libraries\nimport numpy as np\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.lines import Line2D\nfrom matplotlib.colors import ListedColormap\nfrom shapely.geometry import Polygon\n\nfrom pystac_client import Client\nimport planetary_computer\n\nimport rasterio\nfrom rasterio.plot import show\nimport contextily as ctx\n\n\n\n\n\n\n\n# Access catalog\ncatalog = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\n# Set temporal range of interest\ntime_range = \"2017-01-01/2020-01-01\"\n\n# Set Phoenix bounding box\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n# Search catalog\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox,\n    datetime = time_range)\n\n# Get items from search as list\nitems = search.item_collection()\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Store individual items\nitem_2020 = items[0]\nitem_2017 = items[3]\n\n# Check CRS\nitem_2017.properties['proj:epsg']\n\n4326\n\n\n\n# Store raster data from items\nbii_2020 = rioxr.open_rasterio(item_2020.assets['data'].href)\nbii_2017 = rioxr.open_rasterio(item_2017.assets['data'].href)\n\n# Print original dimensions and coords\nprint('Original 2017 raster', bii_2017.dims, bii_2017.coords, '\\n')\n\n# Remove length band dimension\n# Print updated dimensions and coords\nbii_2020 = bii_2020.squeeze()\nbii_2017 = bii_2017.squeeze()\n\n# Remove coordinates associated to band\nbii_2020 = bii_2020.drop('band')\nbii_2017 = bii_2017.drop('band')\n\n# Print updated dimensions and coords\nprint('Updated 2017 raster', bii_2017.dims, bii_2017.coords, '\\n')\n\nOriginal 2017 raster ('band', 'y', 'x') Coordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57 27.57\n    spatial_ref  int64 0 \n\nUpdated 2017 raster ('y', 'x') Coordinates:\n  * x            (x) float64 -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57 27.57\n    spatial_ref  int64 0 \n\n\n\n\n\n\n\n# Read in Arizona shapefile\narizona = gpd.read_file(\"data/tl_2022_04_cousub/tl_2022_04_cousub.shp\")\n\n# Select Phoenix subdivision\nphoenix = arizona[arizona['NAME'] == 'Phoenix']\n\n# Convert CRS to match MPC data\nphoenix = phoenix.to_crs('4326')\n\n# Check for matching CRS (should return True)\nitem_2017.properties['proj:epsg'] == phoenix.crs\n\nTrue\n\n\n\n\n\n\n\n# Set axis for Phoenix\nax = phoenix.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\n\n# Add basemap from OpenStreetMap using Contextily\nctx.add_basemap(ax, crs=phoenix.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Add title\nplt.title(\"Map of Phoenix subdivision\")\n\n# Display map\nplt.show()\n\n\n\n\n\n\n\n\n# Clip the rasters to Phoenix geometry\nbii_2017_clipped = bii_2017.rio.clip(phoenix.geometry, crs='4326')\nbii_2020_clipped = bii_2020.rio.clip(phoenix.geometry, crs='4326')\n\n# Calculate percent area with BII&gt;0.75\npercent_area_2017 = (np.sum(bii_2017_clipped &gt; 0.75) / np.sum(bii_2017_clipped &gt; 0)) * 100\npercent_area_2020 = (np.sum(bii_2020_clipped &gt; 0.75) / np.sum(bii_2020_clipped &gt; 0)) * 100\n\n# Print percents\nprint(f\"Percentage of area with BII&gt;0.75 in 2017: {percent_area_2017:.2f}%\")\nprint(f\"Percentage of area with BII&gt;0.75 in 2020: {percent_area_2020:.2f}%\")\n\nPercentage of area with BII&gt;0.75 in 2017: 7.13%\nPercentage of area with BII&gt;0.75 in 2020: 6.49%\n\n\n\n\n\n\n# Make raster representing the area with BII&gt;0.75 in 2017 that was lost by 2020\nlost_area = (bii_2017_clipped &gt; 0.75) & (bii_2020_clipped &lt; 0.75)\n\n# Check that lost area raster has same spatial reference dimension as original rasters (should return True)\nif lost_area.spatial_ref == bii_2017.spatial_ref == bii_2017.spatial_ref:\n    print('True')\n\nTrue\n\n\n\n# Convert bools to binary values\nlost_area_numeric = lost_area.astype(int)\n\n# Replace zeros with NaN\ncropped_lost_area = lost_area_numeric.where(lost_area_numeric == 1)\n\n# Define custom colormap for the lost area\ncustom_cmap = ListedColormap(['red'])\n\n# Plot figure and axis with custom colorbar\nfig, ax = plt.subplots()\nim = bii_2020_clipped.plot(ax=ax, cmap='Blues', add_colorbar=False)\ncropped_lost_area.plot(ax=ax, cmap=custom_cmap, add_colorbar=False)\ncbar = plt.colorbar(im, orientation='horizontal', pad=0.15)\ncbar.set_label('Biodiversity Intactness Index (BII)')\ncbar.outline.set_visible(False)\n\n# Create legend patch\nred_patch = Patch(color='red', label='Area where 2017 BII &gt; 0.75 but 2020 BII &lt; 0.75')\n\n# Add legend below the figure\nlegend_ax = fig.add_axes([0.80, 0.2, 0.02, 0.12])\nlegend_ax.legend(handles=[red_patch], ncol=1, frameon=False)\nlegend_ax.axis('off')\n\n# Add title\ntitle_text = 'Biodiversity in Phoenix, Arizona (2020)'\nax.set_title(title_text)\n\n# Remove axis labels and ticks\nax.set_xlabel('')\nax.set_ylabel('')\nax.set_xticks([])\nax.set_yticks([])\n\n# Remove frame around the entire figure\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()\n\n\n\n\nFrom this map, we can see how the edges of certain areas in Phoenix, which had relatively high BII, decrease from greater than 0.75 to less than 0.75. In particular, this can be seen in the North East area of Phoenix, in addition to one spot in South Central Phoenix."
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#introduction",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#introduction",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "The purpose of this analysis is to better understand and visualize biodiversity in Phoenix and highlight areas where biodiversity is declining. Specifically, the final map will show a map of Phoenix displayed on 100 meter grid-cells colored based on the area’s 2020 Biodiversity Intactness Index (BII), which is a score from 0 to 1. In addition, areas where BII declined from greater than 0.75 in 2017 to less than 0.75 in 2020 will be highlighted in a seperate color.\n\n\n\n\nAdd basemap with Contextily\nFetch items from Microsoft Planetary Computer (MPC) catalog using search criteria\nClip biodiversity raster based on polygon from shapefile of Arizona subdivisions\nCalculate percent of area with BII&gt;0.75 in 2017 and 2020\nVisualize 2020 biodiversity and changes from 2017\n\n\n\n\n\nThe primary dataset used in this analysis estimates terrestrial Biodiversity Intactness as a 100-meter gridded maps for years 2017 to 2020. The data contained in the dataset comes from Impact Observatory and Vizzuality, and they generated the data using a database of spatially referenced observations of biodiversity across 32,000 sites and over 750 studies. The data was accessed from the Microsoft Planetary Computer (MPC) catalog.\nA dataset from the U.S. Census Bureau was used to clip biodiversity raster.\n\n\n\n\n\nImpact Observatory & Vizzuality. 2022. “Biodiversity Intactness.” Accessed via Microsoft Planetary Computer (MPC) Catalog. https://planetarycomputer.microsoft.com/dataset/io-biodiversity#overview.\nU.S. Census Bureau. 2022. “2022 TIGER/Line Shapefiles: County Subdivision”. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions"
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#import-libraries",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#import-libraries",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "# Load libraries\nimport numpy as np\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.lines import Line2D\nfrom matplotlib.colors import ListedColormap\nfrom shapely.geometry import Polygon\n\nfrom pystac_client import Client\nimport planetary_computer\n\nimport rasterio\nfrom rasterio.plot import show\nimport contextily as ctx"
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#read-in-data",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#read-in-data",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "# Access catalog\ncatalog = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\n# Set temporal range of interest\ntime_range = \"2017-01-01/2020-01-01\"\n\n# Set Phoenix bounding box\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n# Search catalog\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox,\n    datetime = time_range)\n\n# Get items from search as list\nitems = search.item_collection()\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Store individual items\nitem_2020 = items[0]\nitem_2017 = items[3]\n\n# Check CRS\nitem_2017.properties['proj:epsg']\n\n4326\n\n\n\n# Store raster data from items\nbii_2020 = rioxr.open_rasterio(item_2020.assets['data'].href)\nbii_2017 = rioxr.open_rasterio(item_2017.assets['data'].href)\n\n# Print original dimensions and coords\nprint('Original 2017 raster', bii_2017.dims, bii_2017.coords, '\\n')\n\n# Remove length band dimension\n# Print updated dimensions and coords\nbii_2020 = bii_2020.squeeze()\nbii_2017 = bii_2017.squeeze()\n\n# Remove coordinates associated to band\nbii_2020 = bii_2020.drop('band')\nbii_2017 = bii_2017.drop('band')\n\n# Print updated dimensions and coords\nprint('Updated 2017 raster', bii_2017.dims, bii_2017.coords, '\\n')\n\nOriginal 2017 raster ('band', 'y', 'x') Coordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57 27.57\n    spatial_ref  int64 0 \n\nUpdated 2017 raster ('y', 'x') Coordinates:\n  * x            (x) float64 -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57 27.57\n    spatial_ref  int64 0 \n\n\n\n\n\n\n\n# Read in Arizona shapefile\narizona = gpd.read_file(\"data/tl_2022_04_cousub/tl_2022_04_cousub.shp\")\n\n# Select Phoenix subdivision\nphoenix = arizona[arizona['NAME'] == 'Phoenix']\n\n# Convert CRS to match MPC data\nphoenix = phoenix.to_crs('4326')\n\n# Check for matching CRS (should return True)\nitem_2017.properties['proj:epsg'] == phoenix.crs\n\nTrue"
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#create-basic-map-of-phoenix-subdivision",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#create-basic-map-of-phoenix-subdivision",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "# Set axis for Phoenix\nax = phoenix.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\n\n# Add basemap from OpenStreetMap using Contextily\nctx.add_basemap(ax, crs=phoenix.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n\n# Add title\nplt.title(\"Map of Phoenix subdivision\")\n\n# Display map\nplt.show()"
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#clip-rasters-and-calculate-percent-area-with-bii-0.75-for-2017-and-2020",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#clip-rasters-and-calculate-percent-area-with-bii-0.75-for-2017-and-2020",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "# Clip the rasters to Phoenix geometry\nbii_2017_clipped = bii_2017.rio.clip(phoenix.geometry, crs='4326')\nbii_2020_clipped = bii_2020.rio.clip(phoenix.geometry, crs='4326')\n\n# Calculate percent area with BII&gt;0.75\npercent_area_2017 = (np.sum(bii_2017_clipped &gt; 0.75) / np.sum(bii_2017_clipped &gt; 0)) * 100\npercent_area_2020 = (np.sum(bii_2020_clipped &gt; 0.75) / np.sum(bii_2020_clipped &gt; 0)) * 100\n\n# Print percents\nprint(f\"Percentage of area with BII&gt;0.75 in 2017: {percent_area_2017:.2f}%\")\nprint(f\"Percentage of area with BII&gt;0.75 in 2020: {percent_area_2020:.2f}%\")\n\nPercentage of area with BII&gt;0.75 in 2017: 7.13%\nPercentage of area with BII&gt;0.75 in 2020: 6.49%"
  },
  {
    "objectID": "blog/2023-12-13-post/phoenix_biodiversity.html#visualize-2020-bii-and-changes-from-2017",
    "href": "blog/2023-12-13-post/phoenix_biodiversity.html#visualize-2020-bii-and-changes-from-2017",
    "title": "Spatial Analysis of Biodiversity Changes in Phoenix, AZ",
    "section": "",
    "text": "# Make raster representing the area with BII&gt;0.75 in 2017 that was lost by 2020\nlost_area = (bii_2017_clipped &gt; 0.75) & (bii_2020_clipped &lt; 0.75)\n\n# Check that lost area raster has same spatial reference dimension as original rasters (should return True)\nif lost_area.spatial_ref == bii_2017.spatial_ref == bii_2017.spatial_ref:\n    print('True')\n\nTrue\n\n\n\n# Convert bools to binary values\nlost_area_numeric = lost_area.astype(int)\n\n# Replace zeros with NaN\ncropped_lost_area = lost_area_numeric.where(lost_area_numeric == 1)\n\n# Define custom colormap for the lost area\ncustom_cmap = ListedColormap(['red'])\n\n# Plot figure and axis with custom colorbar\nfig, ax = plt.subplots()\nim = bii_2020_clipped.plot(ax=ax, cmap='Blues', add_colorbar=False)\ncropped_lost_area.plot(ax=ax, cmap=custom_cmap, add_colorbar=False)\ncbar = plt.colorbar(im, orientation='horizontal', pad=0.15)\ncbar.set_label('Biodiversity Intactness Index (BII)')\ncbar.outline.set_visible(False)\n\n# Create legend patch\nred_patch = Patch(color='red', label='Area where 2017 BII &gt; 0.75 but 2020 BII &lt; 0.75')\n\n# Add legend below the figure\nlegend_ax = fig.add_axes([0.80, 0.2, 0.02, 0.12])\nlegend_ax.legend(handles=[red_patch], ncol=1, frameon=False)\nlegend_ax.axis('off')\n\n# Add title\ntitle_text = 'Biodiversity in Phoenix, Arizona (2020)'\nax.set_title(title_text)\n\n# Remove axis labels and ticks\nax.set_xlabel('')\nax.set_ylabel('')\nax.set_xticks([])\nax.set_yticks([])\n\n# Remove frame around the entire figure\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()\n\n\n\n\nFrom this map, we can see how the edges of certain areas in Phoenix, which had relatively high BII, decrease from greater than 0.75 to less than 0.75. In particular, this can be seen in the North East area of Phoenix, in addition to one spot in South Central Phoenix."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#link-to-github-repository",
    "href": "blog/2024-3-11-post/index.html#link-to-github-repository",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Link to GitHub repository",
    "text": "Link to GitHub repository"
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#question",
    "href": "blog/2024-3-11-post/index.html#question",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Question",
    "text": "Question\nFrom 1990 to 2012, what was the average treatment effect on the treated (ATT) and average treatment effect (ATE) of implementing an Individual Transferable Quota (ITQ) on the share of years that a fishery was collapsed? For our purposes, “collapsed” is defined as harvest being more than 10% below maximum recorded harvest."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#background",
    "href": "blog/2024-3-11-post/index.html#background",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Background",
    "text": "Background\nAccording to a 2020 article published in the journal Marine Policy, Individual Transferable Quotas (ITQs) involve the allocation of shares or portions of a total allowable catch (TAC) to individual fishers, vessels, communities or others with an interest in the fishery, such as processors. A number of fisheries around the world have introduced ITQs as a way to regulate fisheries in a sustainable manner, preserving important natural resources while also allowing for increased economic productivity."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#data",
    "href": "blog/2024-3-11-post/index.html#data",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Data",
    "text": "Data\nWe will use synthetically generated data to simplify things for our purposes (i.e., our data is fake so our results will have no real implications). Our data contains the variables on 11,135 hypothetical fisheries (only cross sectional, no time observations). These fisheries were either regulated by an Individual Transferable Quota (ITQ) for all years between 1990 and 2012 or in none of those years.\nThe outcome and treatment variables are:\n\nCOLL_SHARE = share of years a fishery is collapsed between 1990 and 2012 (collapse defined as harvest being more than 10% below maximum recorded harvest).\nITQ = dummy variable indicating ‘treatment’ with an ITQ (equal to 1 if the fishery has been regulated by an ITQ and 0 otherwise).\n\nThe control variables are:\n\nMET1, MET2, ….MET6 = Dummy variables indicating to which Marine Ecosystem Type (MET) the fishery belongs to (coral reefs, kelp forests, seagrass meadows, open ocean, deep sea, mangrove forests). This type does not change over the relevant time period and does not depend on human influence.\nIND_SR = Index of species richness in 1980 with values between 0 and 100 indicating the biodiversity with respect to species in the fishery. Bounds of 0 and 100 are the lowest and highest observed values of species diversity across all fisheries in 1980, respectively.\nCOMM_VAL = Commercial value of fisheries in 1980 in million US-$"
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#setup",
    "href": "blog/2024-3-11-post/index.html#setup",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n# Import packages\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(Hmisc)\nlibrary(RItools)\nlibrary(MatchIt)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\n\n\nCode\n## Load Data\nfisheries_df &lt;- read.csv(here::here(\"data\", \"2024-3-11-post-data\", \"final_fisheries_data.csv\"))\n\n## Prepare Data\n# Change all column names to lowercase\ncolnames(fisheries_df) &lt;- tolower(colnames(fisheries_df))"
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#comparison-of-pre-treatment-ecosystem-characteristics-in-the-treatment-and-control-groups",
    "href": "blog/2024-3-11-post/index.html#comparison-of-pre-treatment-ecosystem-characteristics-in-the-treatment-and-control-groups",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Comparison of pre-treatment ecosystem characteristics in the treatment and control groups",
    "text": "Comparison of pre-treatment ecosystem characteristics in the treatment and control groups\n\nMarine Ecosystem Type (MET)\n\n\nCode\n# Create MET column\nfisheries_df &lt;- fisheries_df %&gt;%\n  mutate(met = case_when(\n    met1 == 1 ~ 1,\n    met2 == 1 ~ 2,\n    met3 == 1 ~ 3,\n    met4 == 1 ~ 4,\n    met5 == 1 ~ 5,\n    met6 == 1 ~ 6))\n\n# Convert MET to factors\nfisheries_df$met &lt;- as.factor(fisheries_df$met)\n\n# Aggregate data and negate count for control\nfisheries_summary &lt;- fisheries_df %&gt;%\n  count(met, itq) %&gt;%\n  spread(key = itq, value = n) %&gt;%\n  gather(key = itq, value = 'count', -met) %&gt;% \n  mutate(count = if_else(itq == '0', -count, count))\n\n# Create histograms\nggplot(fisheries_summary, aes(x = met, y = count, fill = as.factor(itq))) +\n  geom_col() +\n  scale_y_continuous(labels = function(x) abs(x), limits = c(-2000, 2000)) +\n  coord_flip() +\n  labs(title = \"Covariates comparison\", x = \"MET\", y = \"Count\") +\n  scale_fill_brewer(palette = \"Set2\", name = \"ITQ\",\n                    labels = c(\"0\" = \"control\", \"1\" = \"treatment\")) +\n  theme_minimal()\n\n\n\n\n\nCompared to fisheries in the control group (not regulated by an ITQ), fisheries in the treatment group (regulated by an ITQ) were less likely to be coral reefs or mangrove forests and more likely to be kelp forests, seagrass meadows, open ocean, or deep sea (these differences are particularly pronounced for open ocean and deep sea).\n\n\nSpecies Richness Index\n\n\nCode\n# Calculate mean difference for IND_SR\nmean_diff_ind_sr &lt;- mean(fisheries_df$ind_sr[fisheries_df$itq == 1]) - mean(fisheries_df$ind_sr[fisheries_df$itq == 0])\nmean_diff_ind_sr\n\n\n[1] -8.825477\n\n\n\n\nCode\n# Mean difference t-test for IND_SR\nt.test(ind_sr ~ itq, data = fisheries_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ind_sr by itq\nt = 41.826, df = 10839, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 8.411868 9.239086\nsample estimates:\nmean in group 0 mean in group 1 \n       57.38515        48.55968 \n\n\nFor species richness (SR) index in 1980, we reject the null hypothesis, at an alpha level of &lt;0.001, that the difference in the mean SR index between the control (not regulated by an ITQ) and treatment (regulated by an ITQ) groups is zero. We estimate this mean difference to be about 8.83 index units (where the mean SR index is lower in the treatment group), and our 95% confidence interval for the mean difference is from 8.41 to 9.24 index units.\n\n\nCommercial value\n\n\nCode\n# Calculate mean difference for COMM_VAL\nmean_diff_comm_val &lt;- mean(fisheries_df$comm_val[fisheries_df$itq == 1]) - mean(fisheries_df$comm_val[fisheries_df$itq == 0])\nmean_diff_comm_val\n\n\n[1] -32.34931\n\n\n\n\nCode\n# Mean difference t-test for COMM_VAL\nt.test(comm_val ~ itq, data = fisheries_df)\n\n\n\n    Welch Two Sample t-test\n\ndata:  comm_val by itq\nt = 39.507, df = 9334.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 30.74425 33.95437\nsample estimates:\nmean in group 0 mean in group 1 \n      117.22839        84.87908 \n\n\nRegarding the commercial value of fisheries in 1980, we also reject the null hypothesis, at an alpha level of &lt;0.001, that the difference in the mean SR index between the control (not regulated by an ITQ) and treatment (regulated by an ITQ) groups is zero. We estimate this mean difference to be about 32.35 million USD (where the mean commercial value is lower in the treatment group), and our 95% confidence interval for the mean difference is from 30.74 to 33.95 million USD.\n\n\nFindings\nFor two of the covariates (SR index and commercial value), there are statistically significant differences between the treated and untreated groups. This indicates that there may be systemic differences between the pre-treatment ecosystem characteristics of fisheries that were regulated by an ITQ and those that were not, and if this is the case, simply controlling for all covariates is an insufficient method to estimate the ATE, since systemic differences means that there are differences between the groups that extend beyond what we can control for.\nBecause there seem to be systemic differences between the pre-treatment characteristics of the treated and untreated group, we will use the nearest neighbor matching estimator to estimate the ATT and the Weighted Least Squares (WLS) estimator to estimate the ATE."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#estimating-propensity-scores",
    "href": "blog/2024-3-11-post/index.html#estimating-propensity-scores",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Estimating propensity scores",
    "text": "Estimating propensity scores\n\n\nCode\n# Estimation of propensity scores with the glm function and logit model\nps_reg &lt;- glm(itq ~ met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,\n          data = fisheries_df, family = binomial())\n\n# Attach the predicted propensity score to the data frame\nfisheries_df$psvalue &lt;- predict(ps_reg, type    = \"response\")\n\n# Drawing back to back histograms for propensity scores for treated and non-treated before matching\nhistbackback(split(fisheries_df$psvalue, fisheries_df$itq),\n             main = \"Propensity score before matching\", xlab = c(\"control\", \"treatment\"), xlim = c(-600, 800))\n\n\n\n\n\nThere is some overlap in the distributions of our histograms, mainly for fisheries that have propensity scores between 0.4 and 0.6. I’m concerned about the lack of overlap for fisheries with high propensity scores, as this may lead to issues when we do nearest-neighbor matching. During nearest-neighbor matching, treated units are assigned to the non-treated unit with the closest propensity score as a match, and we want the resulting set of fisheries to have a great deal of overlap in their propensity scores to ensure a balance in covariates, since they are also likely to have an influence on our outcome variable. However, since there are very few fisheries in the control group with high propensity scores, I’m concerned that even after nearest-neighbor matching occurs, there will still be a lack of overlap in the distributions of our histograms."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#estimating-att-with-nearest-neighbor-matching-estimator",
    "href": "blog/2024-3-11-post/index.html#estimating-att-with-nearest-neighbor-matching-estimator",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Estimating ATT with nearest neighbor matching estimator",
    "text": "Estimating ATT with nearest neighbor matching estimator\n\n\nCode\n## Nearest Neighbor Matching\n\n# Match using nearest-neighbor approach (treated units are assigned the non-treated unit with the closest propensity score as match)\nnn_matching &lt;- matchit(itq ~ met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,\n                       data = fisheries_df, method = \"nearest\", ratio = 1)\nmatch_data = match.data(nn_matching)\n\n## Estimate ATT\n\n# Calculate sum of the differences of outcomes between matches\nsumdiff_data &lt;- match_data %&gt;%\n  group_by(subclass) %&gt;%\n  mutate(diff = coll_share[itq==1] - coll_share[itq==0])\nsumdiff &lt;- sum(sumdiff_data$diff)/2\n\n# Divide sum of the difference by the number treated to generate ATT estimate\nATT_nn = sumdiff / sum(match_data$itq)\nATT_nn\n\n\n[1] -0.07132623\n\n\nUsing the matched data, we estimate the average effect of an ITQ on a fishery that had an ITQ to be a 7.13 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse. This estimated effect size only applies to fisheries that were regulated by an ITQ because we only defined counterfactuals for these fisheries. Thus, our average outcome only demonstrates the effect of an ITQ on fishery collapse as it pertains to fisheries that were regulated by an ITQ."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#calculating-ate-with-the-wls-estimator",
    "href": "blog/2024-3-11-post/index.html#calculating-ate-with-the-wls-estimator",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Calculating ATE with the WLS estimator",
    "text": "Calculating ATE with the WLS estimator\n\n\nCode\n## WLS Matching\n\n# calculation of the weights (see slide 25 of lecture 5)\nPS &lt;- fisheries_df$psvalue\nD &lt;- fisheries_df$itq\n\n# add weights to data frame\nfisheries_df$wgt = (D/PS + (1-D)/(1-PS))\n\n# run WLS regression\nreg_wls &lt;- lm(coll_share ~ itq + met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,\n               data = fisheries_df, weights = wgt)\n\n## Estimate ATE\n\n# Extracting the coefficients table\nsummary_reg &lt;- summary(reg_wls)\nsummary_reg$coefficients %&gt;% \n  kbl(caption = \"WLS estimator\") %&gt;%  # Generate table\n  kable_classic(full_width = FALSE)\n\n\n\nWLS estimator\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n0.3748451\n0.0030280\n123.79136\n0\n\n\nitq\n-0.0766783\n0.0009902\n-77.43563\n0\n\n\nmet1\n-0.1119518\n0.0019244\n-58.17396\n0\n\n\nmet2\n-0.1285996\n0.0022096\n-58.20071\n0\n\n\nmet3\n-0.0968933\n0.0018096\n-53.54421\n0\n\n\nmet4\n-0.0634837\n0.0022391\n-28.35257\n0\n\n\nmet5\n-0.0323470\n0.0019230\n-16.82119\n0\n\n\nind_sr\n-0.0014749\n0.0000412\n-35.80433\n0\n\n\ncomm_val\n0.0003786\n0.0000112\n33.94403\n0\n\n\n\n\n\n\n\nUsing the WLS estimator, we estimate the average effect of an ITQ on a fishery to be a 7.67 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse, with a standard error of 0.099 percentage-points. The very low p-value means we reject the null hypothesis that the average effect of an ITQ is zero at an alpha level of &lt;0.001. Our estimated effect size, as calculated here, applies to all fisheries because we did not remove any fisheries from our sample, like we did when estimating effect size based on matching."
  },
  {
    "objectID": "blog/2024-3-11-post/index.html#conclusion",
    "href": "blog/2024-3-11-post/index.html#conclusion",
    "title": "Using Propensity Score Methods to analyze the impact of Catch Shares",
    "section": "Conclusion",
    "text": "Conclusion\nOur comparison of pre-treatment ecosystem characteristics showed that fisheries regulated with an ITQ between 1990 and 2012 had significant differences in Species Richness Index and commercial value compared to ones not regulated by an ITQ. For this reason, the treatment ignorability seemed like it would be a bad assumption to make, so we estimated propensity scores in order to calculate ATT using Nearest Neighbor Matching and ATE using the WLS estimator.\nUsing the matched data, we estimated the ATT to be a 7.13 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse. Then, using the WLS estimator, we estimated the ATE to be a 7.67 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse, with a standard error of 0.099 percentage-points.\nOverall, our most useful finding here is the very low standard error when estimating ATE with the WLS estimator, indicating that we are 95% confident that the true ATE is between a 7.57 and 7.77 percentage-point decrease in share of years with a fishery collapse. In addition, the very low p-value corresponding to the low standard error means we reject the null hypothesis that the average effect of an ITQ is zero at an alpha level of &lt;0.001."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "",
    "text": "In June 2024, I graduated with a Master’s in Environmental Data Science from the Bren School of Environmental Science & Management at UC Santa Barbara. I also hold a B.S. in Environmental Economics from the University of Maryland, which I received in May 2023. If you want to learn more about me, check out my profiles on LinkedIn and GitHub as well.\n\nData Science Portfolio\n\nData Visualization Portfolio Blogs\nPython | Blog\nR | Blog\n\n\nFinal Project for Statistics Class\nTime Series Analysis of Nutrient Concentration in the Chesapeake Bay\nPython | Repository | Blog\nProposed statistical question on how nutrient concentrations have changed since Clean Water Act protection measures (implemented in 2010) and found appropriate data for answering the question (used over 43,000 samples from the Bay’s tidal regions). Constructed two Seasonal-Trend using LOESS (STL) decomposition models to conduct time series analysis of nitrogen and phosphorus concentrations (selected length of seasons based on autocorrelation). For each pollutant, visualized model parameters comparatively. In addition, ran regressions to determine the proportion of variation attributable to seasonality and the 95% confidence interval for change in trend component over the 10-year period.\n\n\nProject for Modeling Class\nDynamic Simulation of Forest Growth\nR | Repository | Blog\nUsed common scientific model that estimates forest size (measured in kilograms of carbon) and generated stochastic parameter sets for model inputs (exponential growth rate before canopy closure, linear growth rate after canopy closure, carrying capacity, and canopy closure threshold). Used an ordinary differential equations solver to run 300-year continuous dynamic simulations of forests. Conducted global sensitivity analysis (ran 2,000 simulations and computed Sobol indices of input parameters) to look at impact of varying parameter values on maximum forest size.\n\n\nMaster’s Capstone Project\nCreating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis\nR | GitHub Page | Blog\nWorked with the Head of Sustainability at outdoor apparel company Darn Tough Vermont and three classmates to streamline the company’s spreadsheet method for calculating and analyzing data on yearly Scope 1, 2, and 3 GHG emissions. Built interactive web-application dashboard that allows the company to visualize emissions and conduct scenario analysis based on adjustable input variables (e.g., can evaluate emissions under scenarios with differing fiber procurements).\n\n\nProjects for Geospatial Analysis Class\nSpatial Analysis of Biodiversity Changes in Phoenix, AZ\nPython | Repository | Blog\nFetched items from Microsoft Planetary Computer (MPC) catalog based on search criteria, retrieving grid-cell data on 2017 and 2020 Biodiversity Intactness Index (BII) scores, which rate an area of land’s biodiversity from 0 to 1. Clipped raster of BII scores with polygon shapefile of Arizona subdivisions. For Phoenix’s subdivision, calculated percent of area where BII decreased from above 0.75 in 2017 to being below this threshold score in 2020. Created heatmap visualizing 2020 BII scores across Phoenix’s subdivision, highlighting areas that decreased below the 0.75 BII threshold.\nSpatial Analysis of 2021 Houston Power Crisis\nR | Repository | Blog\nUsed data from NASA’s VIIRS instrument, OpenStreetMap, and the U.S. Census Bureau to conduct a spatial analysis of the 2021 Houston Power Crisis. To determine census tracts where residential blackouts occurred, created a blackout mask excluding non-residential areas (excluded highways and commercial properties based on data from OpenStreetMap) and performed spatial joins with census tract data. Created visualization showing census tracts where residential blackouts occurred. As a second component to the analysis, joined census tract data with data on median income to create heatmap where fill color was based on median income and outline color was based on whether the census tract had residential blackouts. Lastly, created double-sided histogram demonstrating the lack of a clear relationship, except for census tracts at the highest end of the income distribution.\n\n\nProjects for Machine Learning Class\nRegression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems\nPython | Repository | Blog\nUsed data from a marine ecosystem research program to build three models (single decision tree, random forest, and stochastic gradient boosted trees) that predict dissolved inorganic carbon (DIC) based on other ocean chemistry features (e.g., sulfur trioxide concentration) that were also measured during water sampling. Developed visualizations comparing root mean squared error (RMSE) among the three models and analyzing feature importances in the best performing model.\nCluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay\nR | Repository | Blog)\nImplemented K-means clustering algorithm with data on metal contents (Cd, Cr, Cu, Mn, and Ni) in two species of co-occurring algae at 10 sample sites around Port Jackson Bay to plot the Total Within Sum of Square (TWSS) for different numbers of clusters and determine the optimal number of clusters that indicate distinct types of bio-contaminating algae. In addition, calculated Euclidean distance matrix to build hierarchical clustering model and inspect the resulting dendrogram for outlier points."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#background",
    "href": "blog/2024-4-1-post/index.html#background",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Background",
    "text": "Background\nTo practice clustering analysis, we’ll use data from Roberts et al. 2008 on biological contaminants in Port Jackson Bay (located in Sydney, Australia). The data are measurements of metal content in two types of co-occurring algae at 10 sample sites around the bay."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#setup-import-data",
    "href": "blog/2024-4-1-post/index.html#setup-import-data",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Setup & import data",
    "text": "Setup & import data\n\n# set seed\nset.seed(123)\n\n\n# load packages\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(cluster) # cluster analysis\nlibrary(factoextra) # cluster visualization\n\n\n# load data\nmetals_df &lt;- readr::read_csv(\"../../data/2024-4-1-post-data/harbour_metals.csv\")\n\n# select only columns for pollutant variables\nmetals_df &lt;- metals_df[, 4:8]"
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#k-means-clustering",
    "href": "blog/2024-4-1-post/index.html#k-means-clustering",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nFind optimal k value & build model\n\n# find optimal number of clusters using elbow method\nfviz_nbclust(metals_df, kmeans, method = \"wss\")\n\n\n\n\nThe elbow method indicates that k=3 is the ideal number of clusters, as at this point, we see a very significant drop in the Total Within Sum of Square compared to k=2, followed by a very insignificant drop at k=4.\n\n# build k-means cluster model with optimal number of clusters (3)\nkmeans_model &lt;- kmeans(metals_df, centers = 3, nstart = 25)\n\n\n\nInspect clustering model\n\n# plot the clusters\nfviz_cluster(kmeans_model, geom = \"point\", data = metals_df) + ggtitle(\"k=3\")\n\n\n\n\nWhen we connect the outermost points in each cluster, we see that there is overlap between the far right side of cluster 2 and the far left side of cluster 3. The points near this boundary might have been assigned this way just because of the other nearby points that belonged to each cluster, or this could indicate that we should have chosen a larger number of clusters.\n\n# inspect model object\nkmeans_model\n\nK-means clustering with 3 clusters of sizes 10, 22, 28\n\nCluster means:\n        Cd       Cr        Cu        Mn    Ni\n1 0.796000 6.530000 127.09000 126.28200 2.304\n2 1.076818 6.180455  70.28455  66.79864 2.385\n3 1.593214 3.462857  22.39893  18.85250 1.870\n\nClustering vector:\n [1] 2 2 1 1 1 1 3 2 2 2 2 2 2 2 1 2 2 1 3 3 3 3 3 3 3 3 2 2 2 2 3 3 3 3 3 3 1 3\n[39] 2 3 2 2 3 3 3 3 3 3 2 1 1 2 1 2 2 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 13159.695 22633.706  7511.094\n (between_SS / total_SS =  80.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nLooking at the specific values, ‘size’, which give the number of points allocated to each of the three clusters, stands out because cluster 2 and cluster 3 both have more than double the number of points as cluster 1. In addition, our ‘withinss’ values, which give the within-cluster variation for each of the three clusters, stands out because cluster 3 has over five times more variation than cluster 1 and over three times more variation than cluster 2."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#hierarchical-clustering",
    "href": "blog/2024-4-1-post/index.html#hierarchical-clustering",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\nCalculate Euclidean distance matrix & build model\n\n# calculate distance matrix\ndist_matrix &lt;- dist(metals_df, method = \"euclidean\")\n\nEach value in this matrix tells us the Euclidean distance between a set of two points in our data. There are 1770 rows in the table, one for each unique set of points.\n\n# build clustering model\nhc_model &lt;- hclust(dist_matrix)\n\n\n\nInspect clustering model\n\n# plot the dendrogram of clustering model\nplot(as.dendrogram(hc_model), main = \"Hierarchical Clustering Dendrogram\")\n\n\n\n\nThe dendrogram looks as expected. 51 is clearly an outlier point because starting from the bottom of the dendrogram and moving up, it is by far the last point to be assigned to a cluster that includes any other points besides itself, and when this assignment does occur, the point is about 80 distance units away from the centroid of the cluster it is assigned to. Comparatively, the point with the next highest distance away from the centroid of its initial assignment to a cluster containing more than just itself is 15, at about 40 distance units."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#conclusion",
    "href": "blog/2024-4-1-post/index.html#conclusion",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Conclusion",
    "text": "Conclusion\nOur clustering analysis indicates that there are 3 major clusters of biological contaminating algae. For each cluster of algae, there are differing expected ranges of metal content across the five types of metal looked at (Cd, Cr, Cu, Mn, and Ni)."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#question",
    "href": "blog/2023-12-12-post/index.html#question",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Question",
    "text": "Question\nSince the 2010 introduction of federal water quality requirements, what seasonal and non-seasonal trends are present for nitrogen and phosphorus concentrations in Chesapeake Bay tidal regions?"
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#introduction",
    "href": "blog/2023-12-12-post/index.html#introduction",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Introduction",
    "text": "Introduction\nThe Chesapeake Bay is the largest estuary in the United States, and the largest body of water that is regulated under the Clean Water Act (U.S. Environmental Protection Agency 2023). Federal regulation pertaining to the Bay took decades to implement, and this is in large part because of the Bay’s large size and the many stakeholders involved. In the 1970s, the Bay was identified as one of the first areas in the world to have a marine dead zone, a phenomenon that literally suffocates aquatic life due to lack of oxygen in the water. Despite dead zones being identified in the 1970s, it was not until 2000 that the Bay was designated as an “impaired water” under the Clean Water Act. Then, it took another ten years, until 2010, for the EPA to take the next step of issuing Total Maximum Daily Load (TMDL) requirements, the regulatory action mandating water quality improvement.\nSpecifically, a TMDL is the maximum amount of a particular pollutant that a body of water can receive and still meet applicable water quality standards (U.S. Environmental Protection Agency 2023). This maximum amount is calculated in pounds based on measurements taken at areas where pollution is likely to end up in the Bay. In their 2010 regulation, the EPA established TMDL requirements for nitrogen, phosphorus, and sediment. Nitrogen and phosphorus, referred to as nutrients because of their role in providing nutrition to many animals and plants, cause algal blooms, which cause marine dead zones through taking in dissolved oxygen and blocking sunlight. Sediment contributes to dead zones by blocking sunlight as well, leading it to also be included in the 2010 TMDL requirements.\nThis analysis will focus on nitrogen and phosphorus, the two pollutants responsible for algal blooms in the Chesapeake Bay. A 2022 study found that agricultural runoff was the largest source of nutrient pollution, accounting for 48% of nitrogen and 27% of phosphorus in the Chesapeake Bay (Chesapeake Progress, n.d.). Both of these pollutants also get to the Bay as a result of urban and suburban runoff, wastewater treatment plants releasing treated water, and natural sources (e.g., runoff from forests, wetlands, etc.). In addition, about 25% of nitrogen that ends up in the Bay comes from air pollution that is originally emitted to the atmosphere by sources such as cars and factories (Burns et al. 2021). Through a process called atmospheric deposition, these nitrogen compounds react with other chemicals to become nitrous oxides, which can be deposited back to Earth’s surface through precipitation or as dry deposition.\nThrough conducting a time series analysis of post-2010 nitrogen and phosphorus concentration measurements, my goal is to better understand how concentrations have changed since the introduction of TMDL requirements. I’m also interested in the nature of any seasonality and whether the three time series components (i.e., seasonal, trend, and random) are consistent across both nitrogen and phosphorus."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#data",
    "href": "blog/2023-12-12-post/index.html#data",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Data",
    "text": "Data\nYearly water quality data on the Chesapeake Bay’s tidal and non-tidal regions going back to 1984 is publicly available on the Chesapeake Bay Program (CBP) DataHub (Chesapeake Bay Program DataHub, n.d.). Data is organized into either Tier 1, 2, or 3 depending on how it was collected. While Tier 1 and 2 data can be collected by any interested group, Tier 3 data is collected by monitoring stations overseen by experienced professionals. Only Tier 3 data can be used for governmental regulatory assessments.\nFor my analysis, I will be using 2010 to 2019 Tier 3 data collected at 143 different monitoring stations positioned throughout the Chesapeake Bay tidal regions, which includes the mainstem Bay and tributary components. Across the 10 years that we are looking at, we’ll have a total of 43,809 nitrogen observations and 43,590 phosphorus observations.\nBelow, we import the R packages used in this analysis. Then, we read in the yearly water quality data using their CBP DataHub URL. We also process the data by creating separate data.frames for total nitrogen and phosphorus measurements.\n\n\nCode\n# Import necessary R packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(generics)\nlibrary(stargazer)\n\n# Create a vector of data URLs\nexcel_urls &lt;- c(\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2019_CEDR_tidal_data_01jun21.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2018_CEDR_tidal_data_01jun21.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2017_CEDR_tidal_data_11oct18.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2016_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2015_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2014_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2013_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2012_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2011_CEDR_tidal_data_15jun17.xlsx',\n  'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2010_CEDR_tidal_data_15jun17.xlsx')\n\n# Create a temporary directory to store downloaded files\ntemp_dir &lt;- tempdir()\n\n# Create an empty list to store data frames\ndfs &lt;- list()\n\n# Loop through each URL, extract file name, define local file path, download file, read into R, and append to list of data frames\nfor (url in excel_urls) {\n  file_name &lt;- basename(url)\n  local_path &lt;- file.path(temp_dir, file_name)\n  download.file(url, destfile = local_path, mode = \"wb\")\n  wq_data &lt;- readxl::read_excel(local_path, sheet = 1)\n  dfs[[file_name]] &lt;- wq_data\n}\n\n# Combine all data frames into a single data frame\nwq_data_combined &lt;- bind_rows(dfs)\n\n# Wrangle data for relevant column variables, and filter for TN (total nitrogen)\nnitr_data &lt;- wq_data_combined %&gt;%\n  dplyr::select(\"MonitoringLocation\", \"SampleDate\", \"Parameter\", \"MeasureValue\", \"Unit\", \"Latitude\", \"Longitude\") %&gt;% \n  filter(Parameter==\"TN\")\n\n# Wrangle data for relevant column variables, and filter for TP (total phosphorus)\nphos_data &lt;- wq_data_combined %&gt;%\n  dplyr::select(\"MonitoringLocation\", \"SampleDate\", \"Parameter\", \"MeasureValue\", \"Unit\", \"Latitude\", \"Longitude\") %&gt;% \n  filter(Parameter==\"TP\")\n\n# Remove unnecessary data and values from environment\nrm(wq_data, wq_data_combined, dfs)\nrm(excel_urls, file_name, local_path, temp_dir, url)"
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#exploratory-analysis",
    "href": "blog/2023-12-12-post/index.html#exploratory-analysis",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nTo get an idea of what is going on with the data, we’ll first calculate moving averages for each year-month, storing the resulting output as both a tsibble and data.frame. We’ll plot these moving averages just to get a general idea of what types of trends we might be looking at.\n\n\nCode\n# Compute nitrogen monthly moving average, and store as tsibble\nnitr_monthly_avgs_ts &lt;- nitr_data %&gt;% \n  mutate(yr_mo = tsibble::yearmonth(SampleDate)) %&gt;%\n  group_by(yr_mo) %&gt;%\n  summarize(monthly_avg = mean(MeasureValue, na.rm = TRUE)) %&gt;% \n  tsibble::as_tsibble()\n\n# Create data frame version, and convert year-months to Date class (helpful for plotting)\nnitr_monthly_avgs_df &lt;- as.data.frame(nitr_monthly_avgs_ts)\nnitr_monthly_avgs_df$yr_mo &lt;- as.Date(nitr_monthly_avgs_ts$yr_mo, format = \"%Y-%m\")\n\n# Plot monthly average nitrogen concentration as a function of year-month\nnitr_monthly_avgs_df %&gt;%\n  ggplot(aes(x = yr_mo, y = monthly_avg)) +\n  stat_summary(geom = 'line', fun = 'mean') +\n  labs(x = 'Year-Month', y = 'Monthly Mean Concentration (mg/L)', title = \"Nitrogen in Chesapeake Bay (2010-2019)\") +\n  scale_x_date(date_breaks = \"1 year\", date_minor_breaks = \"6 months\", date_labels = \"%Y-%m\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nFrom this plot, there does appear to be a seasonal trend for nitrogen concentrations, but there is no clear non-seasonal trend. In addition, there is a notable spike in early 2014.\n\n\nCode\n# Compute phosphorus monthly moving average, and store as tsibble\nphos_monthly_avgs_ts &lt;- phos_data %&gt;% \n  mutate(yr_mo = tsibble::yearmonth(SampleDate)) %&gt;%\n  group_by(yr_mo) %&gt;%\n  summarize(monthly_avg = mean(MeasureValue, na.rm = TRUE)) %&gt;% \n  tsibble::as_tsibble()\n\n# Create data frame version, and convert year-months to Date class (helpful for plotting)\nphos_monthly_avgs_df &lt;- as.data.frame(phos_monthly_avgs_ts)\nphos_monthly_avgs_df$yr_mo &lt;- as.Date(phos_monthly_avgs_ts$yr_mo, format = \"%Y-%m\")\n\n# Plot monthly average phosphorus concentration as a function of year-month\nphos_monthly_avgs_df %&gt;%\n  ggplot(aes(x = yr_mo, y = monthly_avg)) +\n  stat_summary(geom = 'line', fun = 'mean') +\n  labs(x = 'Year-Month', y = 'Monthly Mean Concentration (mg/L)', title = \"Phosphorus in Chesapeake Bay (2010-2019)\") +\n  scale_x_date(date_breaks = \"1 year\", date_minor_breaks = \"6 months\", date_labels = \"%Y-%m\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nSimilar to the nitrogen plot, phosphorus also seems to exhibit a distinct seasonal trend. Again, it is unclear whether there is a non-seasonal trend."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#methods",
    "href": "blog/2023-12-12-post/index.html#methods",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Methods",
    "text": "Methods\n\nAutocorrelation function\nThe autocorrelation function calculates the correlation between the dependent variable at a given point in time and various time lags for this same variable. Thus, the autocorrelation function provides us with a tool that allows us to better understand any seasonal trends present in our data. This will be useful for us in subsequent steps of our time series analysis.\n\n\nSTL decomposition\nAs a core part of this time series analysis, I’ll be constructing a seasonal trend decomposition using locally estimated scatterplot smoothing (LOESS), which is often abbreviated as a STL decomposition model. STL allows us to separate our monthly average concentrations into three components: seasonal trend, non-seasonal trend, and remainder. Through plotting these components next to each other, we gain a more intuitive understanding of the underlying forces contributing to variation in our dependent variable, which in this case is the monthly average concentration.\nAs opposed to other decomposition methods, one thing that is particular to STL models is the ability to specify the length of a season. It can be helpful to adjust this input depending on our desired level of smoothing for the non-seasonal trend. We will use our results from the autocorrelation function to inform our chosen length of seasons. The autocorrelation function is useful in this context because it can tell us after how many lags we see a drop-off in correlation, indicating there is a drop in the significance of the seasonal trend."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#results",
    "href": "blog/2023-12-12-post/index.html#results",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Results",
    "text": "Results\n\nAutocorrelation function\n\n\nCode\n# Plot autocorrelation function for nitrogen with lags going back three years\nacf(nitr_monthly_avgs_ts, lag.max = 36, i=2)\n\n\n\n\n\nLooking at this autocorrelation plot for nitrogen, I see that the t-16 lag is significant at the alpha equals 0.05 level, indicated by the black line extending beyond the blue dashed line. Meanwhile, the t-4 lag is not statistically significant. The rest of the lags remain the same or slightly decrease when comparing from the first and second year of lags. In the third year, there is a drop off in the t-28 lag compared to t-12, and there continues to be what seems like a marginal decrease in correlation across most lags. Considering all of this, I decided to set the seasonality of my STL model for nitrogen to 24 months.\n\n\nCode\n# Plot autocorrelation function for phosphorus with lags going back three years\nacf(phos_monthly_avgs_ts, lag.max = 36, i = 2)\n\n\n\n\n\nFor phosphorus, there is a very consistent marginal decline for each set of lags over the course of the three years. This is good news for our STL because it means that the seasonal trend will be easier to separate from the non-seasonal trend. Like I did with nitrogen, I’m also going to use two-year seasons for phosphorus. Similar to the case with nitrogen, it seems to me like the drop-off in lag correlations from year two to year three is a bit larger than from year one to year two. This suggests that a two-year seasonal cycle will give us an informative non-seasonal trend component that is neither too eager nor too hesitant to categorize differences as non-seasonal trends.\n\n\nSTL decomposition\n\n\nCode\n# Conduct STL for nitrogen with two-year seasons, and extract components\nnitr_decomp &lt;- nitr_monthly_avgs_ts %&gt;%\n  fabletools::model(feasts::STL(monthly_avg, t.window = 24)) %&gt;% \n  generics::components()\n\n# Plot STL model\nautoplot(nitr_decomp) +\n  labs(title = \"STL model of nitrogen concentration\", x = \"Year Month\")\n\n\n\n\n\nIn this plot of the three STL components for nitrogen, it is still difficult to see a long-term trend, despite the line being fairly smooth. There does seem to be a slight downward trend until 2018. From 2018 to 2019, there is a clear increase, but this change is then offset by an equivalent decrease over the course of 2019 to 2020. In addition the grey bars on the left all represent the same range, implying that the remainder components of our STL model contributed the most to variation in nitrogen concentration. However, this seems to be very influenced by the high spike in 2014.\n\n\nCode\n# Conduct STL for phosphorus with two-year seasons, and extract components\nphos_decomp &lt;- phos_monthly_avgs_ts %&gt;%\n  fabletools::model(feasts::STL(monthly_avg, t.window = 24)) %&gt;% \n  generics::components()\n\n# Plot STL model\nautoplot(phos_decomp) +\n  labs(title = \"STL model of phosphorus concentration\", x = \"Year Month\")\n\n\n\n\n\nThe STL plot for phosphorus does make it seem like there is a long-term downward trend, but it is difficult to tell how significant it is because of the long grey bar, which indicates it is least influential of the three components in our STL model. In this case, the grey bars show us that seasonality contributes the most to the variation in phosphorus concentration.\n\n\nCode\n# For nitrogen, plot monthly mean, seasonally adjusted monthly mean, STL seasonality, and STL trend\nggplot(nitr_monthly_avgs_df, aes(yr_mo)) +\n  scale_x_date(date_breaks = \"1 year\", date_minor_breaks = \"6 months\", date_labels = \"%Y-%m\") +\n  geom_line(aes(y=nitr_decomp$monthly_avg, color = \"Monthly mean\")) +\n  geom_line(aes(y=nitr_decomp$season_adjust, color = \"Seasonally adjusted monthly mean\"), linewidth=2) +\n  geom_line(aes(y=nitr_decomp$trend, color = \"STL trend\"), linewidth = 2) +\n  geom_line(aes(y=nitr_decomp$season_year, color = \"STL seasonality\")) +\n  labs(x = 'Year-Month',\n       y = 'Concentration (mg/L)',\n       title = \"Nitrogen in Chesapeake Bay (2010-2019)\") +\n  scale_color_manual(name = \"\", values = c(\"Monthly mean\" = \"black\", \"Seasonally adjusted monthly mean\" = \"cornflowerblue\", \"STL seasonality\" = \"seagreen\", \"STL trend\" = \"red\"), breaks = c(\"Monthly mean\", \"Seasonally adjusted monthly mean\", \"STL seasonality\", \"STL trend\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nI decided to make this visualization to get a better idea of exactly how these components map on to each other. This plot seems to confirm the idea of negligible trend for nitrogen. Since the x-axis is labeled for each year here, it is also easier to see the seasonal trend. Each year, nitrogen concentrations increase sharply around December. They then peak around February to March, before decreasing substantially and reaching their minimum around July.\n\n\nCode\n# For phosphorus, plot monthly mean, seasonally adjusted monthly mean, STL seasonality, and STL trend\nggplot(phos_monthly_avgs_df, aes(yr_mo)) +\n  scale_x_date(date_breaks = \"1 year\", date_minor_breaks = \"6 months\", date_labels = \"%Y-%m\") +\n  geom_line(aes(y=phos_decomp$monthly_avg, color = \"Monthly mean\")) +\n  geom_line(aes(y=phos_decomp$season_adjust, color = \"Seasonally adjusted monthly mean\"), linewidth=2) +\n  geom_line(aes(y=phos_decomp$trend, color = \"STL trend\"), linewidth = 2) +\n  geom_line(aes(y=phos_decomp$season_year, color = \"STL seasonality\")) +\n  labs(x = 'Year-Month',\n       y = 'Concentration (mg/L)',\n       title = \"Phosphorus in Chesapeake Bay (2010-2019)\") +\n  scale_color_manual(name = \"\", values = c(\"Monthly mean\" = \"black\", \"Seasonally adjusted monthly mean\" = \"cornflowerblue\", \"STL seasonality\" = \"seagreen\", \"STL trend\" = \"red\"), breaks = c(\"Monthly mean\", \"Seasonally adjusted monthly mean\", \"STL seasonality\", \"STL trend\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nOur plot for phosphorus further supports the idea that there is a slight downward trend over the decade. If you independently trace the maximums or minimums, the line does seem to be moving downward at an oscillating but fairly consistent rate. Unlike nitrogen, phosphorus concentrations shoot up in the middle of the year around May, have a relatively flat peak lasting from June to August, and then shoot down at the end of Summer.\n\n\nSimple linear regressions based on STL model parameters\n\nCode\n# Run regression of season component on monthly average nitrogen\nnitr_season_reg &lt;- lm(monthly_avg ~ season_year, data = nitr_decomp)\n\n# Print the formatted regression table\nstargazer(nitr_season_reg, title = \"Regression of monthly nitrogen concentration against seasonal trend component\", align = TRUE, digits = 3, type = 'html', notes.append = FALSE, notes = \"[***]p&lt;0.01\")\n\n\n\n\nRegression of monthly nitrogen concentration against seasonal trend component\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nmonthly_avg\n\n\n\n\n\n\n\n\nseason_year\n\n\n1.038***\n\n\n\n\n\n\n(0.086)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.898***\n\n\n\n\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n120\n\n\n\n\nR2\n\n\n0.551\n\n\n\n\nAdjusted R2\n\n\n0.547\n\n\n\n\nResidual Std. Error\n\n\n0.113 (df = 118)\n\n\n\n\nF Statistic\n\n\n144.981*** (df = 1; 118)\n\n\n\n\n\n\n\n\nNote:\n\n\n[***]p&lt;0.01\n\n\n\n\nThe adjusted R-squared of 0.55 indicates that seasonal trends can explain a bit over half (55%) of the variation in nitrogen monthly mean.\n\nCode\n# Run regression of season component on monthly average phosphorus\nphos_season_reg &lt;- lm(monthly_avg ~ season_year, data = phos_decomp)\n\n# Print the formatted regression table\nstargazer(phos_season_reg, title = \"Regression of monthly phosphorus concentration against seasonal trend component\", align = TRUE, digits = 3, type = 'html', notes.append = FALSE, notes = \"[***]p&lt;0.01\")\n\n\n\n\nRegression of monthly phosphorus concentration against seasonal trend component\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nmonthly_avg\n\n\n\n\n\n\n\n\nseason_year\n\n\n0.984***\n\n\n\n\n\n\n(0.051)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.057***\n\n\n\n\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n120\n\n\n\n\nR2\n\n\n0.759\n\n\n\n\nAdjusted R2\n\n\n0.757\n\n\n\n\nResidual Std. Error\n\n\n0.006 (df = 118)\n\n\n\n\nF Statistic\n\n\n371.062*** (df = 1; 118)\n\n\n\n\n\n\n\n\nNote:\n\n\n[***]p&lt;0.01\n\n\n\n\nFor phosphorus, adjusted R-squared of this regression is 0.76, confirming our idea that seasonality is more pronounced with phosphorus.\n\nCode\n# Run regression of year-month on trend component for nitrogen\nnitr_trend_reg &lt;- lm(trend ~ yr_mo, data = nitr_decomp)\n\n# Print the formatted regression table\nstargazer(nitr_trend_reg, title = \"Regression of nitrogen non-seasonal trend component against year-month\", align = TRUE, digits = 3, type = 'html', notes.append = FALSE, notes = \"[***]p&lt;0.01\")\n\n\n\n\nRegression of nitrogen non-seasonal trend component against year-month\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ntrend\n\n\n\n\n\n\n\n\nyr_mo\n\n\n-0.00001***\n\n\n\n\n\n\n(0.00000)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n1.121***\n\n\n\n\n\n\n(0.079)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n120\n\n\n\n\nR2\n\n\n0.065\n\n\n\n\nAdjusted R2\n\n\n0.057\n\n\n\n\nResidual Std. Error\n\n\n0.055 (df = 118)\n\n\n\n\nF Statistic\n\n\n8.176*** (df = 1; 118)\n\n\n\n\n\n\n\n\nNote:\n\n\n[***]p&lt;0.01\n\n\n\n\n\n\nCode\n# Compute the confidence interval for the coefficient of yr_mo\n# Multiply by 120 to estimate 10-year change in trend component\nnitr_trend_10_yr &lt;- 120 * confint(nitr_trend_reg, \"yr_mo\")\ncat('95% confidence interval for 10-year change in nitrogen non-seasonal trend component:',\n    '\\n[',\n    sprintf(\"%.4f\", nitr_trend_10_yr[1]),\n    ',',\n    sprintf(\"%.4f\", nitr_trend_10_yr[2]),\n    ']')\n\n\n95% confidence interval for 10-year change in nitrogen non-seasonal trend component: \n[ -0.0028 , -0.0005 ]\n\n\n\n\nCode\n# Convert to percent change since January 2010 observation\nnitr_percent_trend_10_yr &lt;- (nitr_trend_10_yr / nitr_decomp$trend[1]) * 100\ncat('95% confidence interval for 10-year percent change in nitrogen non-seasonal trend component:',\n    '\\n[',\n    sprintf(\"%.2f\", nitr_percent_trend_10_yr[1]),\n    ',',\n    sprintf(\"%.2f\", nitr_percent_trend_10_yr[2]),\n    ']')\n\n\n95% confidence interval for 10-year percent change in nitrogen non-seasonal trend component: \n[ -0.28 , -0.05 ]\n\n\nIn this linear regression, we look at the influence of year-month on our non-seasonal trend component for nitrogen. The regression output tells us that we can say at an alpha equals 0.01 significance level that the 10-year change in non-seasonal trend component was negative. However, the low adjusted R-squared also tells us that variation in year-month explains very little of the variation in trend component. Then, the first interval tells us that there is a 95% chance that the interval from -0.0028 mg/L to -0.0005 mg/L contains the true 10-year change in non-seasonal trend component. The second interval is telling us that this represents a -0.28% to -0.05% change as compared to the non-seasonal trend component in January 2010.\n\nCode\n# Run regression of year-month on trend component for phosphorus\nphos_trend_reg &lt;- lm(trend ~ yr_mo, data = phos_decomp)\n\n# Print the formatted regression table\nstargazer(phos_trend_reg, title = \"Regression of phosphorus non-seasonal trend component against year-month\", align = TRUE, digits = 3, type = 'html', notes.append = FALSE, notes = \"[***]p&lt;0.01\")\n\n\n\n\nRegression of phosphorus non-seasonal trend component against year-month\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ntrend\n\n\n\n\n\n\n\n\nyr_mo\n\n\n-0.00000***\n\n\n\n\n\n\n(0.00000)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.092***\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n120\n\n\n\n\nR2\n\n\n0.565\n\n\n\n\nAdjusted R2\n\n\n0.561\n\n\n\n\nResidual Std. Error\n\n\n0.002 (df = 118)\n\n\n\n\nF Statistic\n\n\n153.180*** (df = 1; 118)\n\n\n\n\n\n\n\n\nNote:\n\n\n[***]p&lt;0.01\n\n\n\n\n\n\nCode\n# Compute the confidence interval for the coefficient of yr_mo\n# Multiply by 120 to estimate 10-year change in trend component\nphos_trend_10_yr &lt;- 120 * confint(phos_trend_reg, \"yr_mo\")\ncat('95% confidence interval for 10-year change in phosphorus non-seasonal trend component:',\n    '\\n[',\n    sprintf(\"%.5f\", phos_trend_10_yr[1]),\n    ',',\n    sprintf(\"%.5f\", phos_trend_10_yr[2]),\n    ']')\n\n\n95% confidence interval for 10-year change in phosphorus non-seasonal trend component: \n[ -0.00030 , -0.00022 ]\n\n\n\n\nCode\n# Convert to percent change since January 2010 observation\nphos_percent_trend_10_yr &lt;- (phos_trend_10_yr / phos_decomp$trend[1]) * 100\ncat('95% confidence interval for 10-year percent change in phosphorus non-seasonal trend component:',\n    '\\n[',\n    sprintf(\"%.2f\", phos_percent_trend_10_yr[1]),\n    ',',\n    sprintf(\"%.2f\", phos_percent_trend_10_yr[2]),\n    ']')\n\n\n95% confidence interval for 10-year percent change in phosphorus non-seasonal trend component: \n[ -0.52 , -0.38 ]\n\n\nFor phosphorus, the linear regression output tells us that we are confident at the alpha equals 0.01 level that the 10-year change in non-seasonal trend component was negative. It is also worth noting that the adjusted R-squared tells us that over half (56%) of the the variation in trend component can be explained by variation in year-month. Lastly, we find that there is a 95% chance that the interval between -0.00030 mg/L and -0.00022 mg/L contains the true 10-year change in non-seasonal trend component. This represents a -0.52% to -0.38% change as compared to the non-seasonal trend component in January 2010."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#conclusion",
    "href": "blog/2023-12-12-post/index.html#conclusion",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Conclusion",
    "text": "Conclusion\nMy time series analysis suggests that seasonality plays a substantial role in contributing to variation in the monthly mean concentration of nitrogen and phosphorus in tidal regions of the Chesapeake Bay. For nitrogen, seasonal trends explained 55% of the variation in monthly means, and the relationship was even stronger for phosphorus, with seasonal trends explaining 76% of the variation. While the seasonal component for nitrogen was highest during Winter, the seasonal component for phosphorus was highest during Summer.\nThis analysis was also interested in any non-seasonal trend that has occurred since the introduction of TMDL requirements in 2010. For both nitrogen and phosphorus, we find evidence at an alpha level of 0.01 that the 10-year change in non-seasonal trend is negative. However, our confidence intervals suggest that, for both nutrient pollutants, these changes in trend represent a less than 1% decrease in concentration over the decade. It was also notable how the non-seasonal trend was much more consistent for phosphorus than nitrogen. This can be seen in our STL visualization and is also reflected by the larger adjusted R-squared for phosphorus (0.56) compared to nitrogen (0.06) computed in the linear regression of non-seasonal trend component against year-month.\nThe main limitation of this analysis was that no form of spatial interpolation was employed to estimate concentrations across the tidal region based on the location of measurements. It would be interesting to compare such an analysis to what we did here, as any significant differences would imply that sampled areas are not spread throughout the region in a representative manner. Further analysis might also investigate what happened at the beginning of 2014 that could have led to the high spike in nitrogen levels at that time, in addition to factors that might have fueled the increase seen over the course of 2018."
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#purpose",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#purpose",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "Purpose",
    "text": "Purpose\nIn this analysis, we will use a raster dataset from the U.S. Geological Survey (USGS) to extract land cover statistics in a small region surrounding Mount Whitney in California. We will also be creating a simple map showing the area of analysis relative to Mount Whitney. An analysis of this kind has potential use for environmental planners seeking to better understand land cover in a region.\n\nPenny Higgins, CC BY-SA 2.0, via Wikimedia Commons"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#about",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#about",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "About",
    "text": "About\nAt an elevation of 14,505 feet above sea level, Mount Whitney is the tallest mountain in the contiguous United States (Sierra Nevada Geotourism 2023). Located in the Sierra Nevada mountain range, Mount Whitney’s western slope and summit are located in Sequoia National Park, and the summit is also the southern terminus of the popular John Muir trail, which stretches over 200 miles from Yosemite Valley down to Mount Whitney. The eastern slope is part of the Inyo National Forest and is managed by the U.S. Forest Service."
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#highlights-of-analysis",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#highlights-of-analysis",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "Highlights of analysis",
    "text": "Highlights of analysis\n\nExracting pixels per land cover class from raster dataset\nMerging extracted pixels with tabular dataset matching numeric codes to character strings\nCreating horizontal bar plot showing percent area of different land cover classes\nExtracting bounding box of raster dataset\nCreating map that shows area of land cover analysis relative to Mount Whitney with California boundaries as a basemap"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#read-in-data",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#read-in-data",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "1) Read in data",
    "text": "1) Read in data\n\nUSGS land cover raster dataset\nThe primary dataset that we will be working with comes from the 2011 National Terrestrial Ecosystems data, which was collected as part of the USGS Gap Analysis Project (GAP) by the U.S. Forest Service and Department of the Interior (U.S. Geological Survey 2016). For the purposes of this analysis, the full, nationwide dataset was pre-processed in Microsoft Planetary Computer to only include the area around Mount Whitney. With 30 meter by 30 meter pixel resolution, this raster dataset is a TIF file and contains numbers representing land cover classification.\n\n\nCode\nimport os\nimport xarray as xr\nimport rioxarray as rioxr\n\n# Import land cover TIF as xarray.DataArray\nlulc_fp = os.path.join(os.getcwd(),'..','..','data','2023-12-11-post-data','land_cover.tif')\nlulc = rioxr.open_rasterio(lulc_fp)\n\n\n\n\nUSGS land cover tabular dataset\nOur second dataset is also from the 2011 National Terrestrial Ecosystems data and helps us make sense of values in the raster dataset. This tabular dataset is a CSV file and has the land cover classification names associated with each code used in the raster dataset. This dataset was accessed from the same online source as the raster dataset (U.S. Geological Survey 2016).\n\n\nCode\nimport pandas as pd\n\n# Import accompanying CSV as pandas.DataFrame\nclass_names = pd.read_csv('../../data/2023-12-11-post-data/GAP_National_Terrestrial_Ecosystems.csv')\n\n\n\n\nShapefile of California geographic boundaries\nThe final dataset that we will be using is a shapefile of California geographic boundaries, included in the U.S. Census Bureau’s 2016 Topologically Integrated Geographic Encoding and Referencing (TIGER) database (California Open Data 2019). We will use this shapefile to plot our basemap when visualizing our area of analysis.\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\n\n# Import CA shapefile as geopandas.GeoDataFrame\nca = gpd.read_file('https://data.ca.gov/dataset/e212e397-1277-4df3-8c22-40721b095f33/resource/3db1e426-fb51-44f5-82d5-a54d7c6e188b/download/ca-state-boundary.zip')"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#import-libraries-and-functions",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#import-libraries-and-functions",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "2) Import libraries and functions",
    "text": "2) Import libraries and functions\nWe’ll start by importing all our necessary libraries and functions for our analysis. Some of these packages we already imported before reading in our data, but we will again include them here just to remind us of all the packages that we are using.\n\n\nCode\n# General libraries and functions\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\n# Geospatial libraries and functions\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rioxr\nfrom shapely.geometry import box\nfrom shapely.geometry import Point"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#raster-reduction",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#raster-reduction",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "3) Raster reduction",
    "text": "3) Raster reduction\nNext, we reduce the size of the raster, which is stored in our Python environment as a xarray.DataArray named lulc. We want to remove these unnecessary raster componenets to simplify our Python environment, often helping our code run faster as well.\n\n\nCode\n# Remove band dimension\nlulc = lulc.squeeze()\n\n# Remove coordinates associated to band\nlulc = lulc.drop('band')"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#calculate-percent-area-of-land-cover-classes",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#calculate-percent-area-of-land-cover-classes",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "4) Calculate percent area of land cover classes",
    "text": "4) Calculate percent area of land cover classes\nNow that we are working with a reduced raster, we can move on to calculating percent area of the different land cover classes. To do this, we must extract the unique values contained in the lulc xarray.DataArray, in addition to the count of how many times each unique value appears in our raster. Since these codes correspond to land cover classes specified in our class_names pandas.DataFrame, we need to merge the extracted codes and counts with class_names, which requires storing our codes and counts as a pandas.DataFrame. Once we have this merged pandas.DataFrame, which we stored as classes, performing the actual calculation can be easily accomplished. We just use the count column in classes and size of lulc as object attributes.\n\n\nCode\n# Extract pixels per land cover class\ncodes, counts = np.unique(lulc, return_counts = True)\n\n# Create extracted pixels DataFrame\npix_counts = pd.DataFrame({'code': codes, 'count': counts})\n\n# Merge extracted pixels DataFrame with class names DataFrame\nclasses = pd.merge(pix_counts, class_names, on='code', how='inner')\n\n# Calculate the percentage of area covered by each class\nclasses['percentage'] = (classes['count'] / lulc.size) * 100"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#create-horizontal-bar-plot",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#create-horizontal-bar-plot",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "5) Create horizontal bar plot",
    "text": "5) Create horizontal bar plot\nNext, we will visualize our percent values with a horizontal bar plot, which shows us the percent for each land cover classification that takes up at least 1% of the area. We filter for just these land cover classes to reduce the amount of output, as it is likely irrelevant for an audience to see the percent for land cover classes that only take up a very small area. We also use the sort_values function, so we can plot our horizontal bars in ascending order, making our plot easier to interpret.\n\n\nCode\n# Filter for classes with more than 1% land cover\nfiltered_classes = classes[classes['percentage'] &gt; 1]\n\n# Sort classes in decreasing order of percentage\nsorted_classes = filtered_classes.sort_values(by='percentage', ascending=True)\n\n# Create a horizontal bar plot with axis label and title\nplt.barh(sorted_classes['class_label'], sorted_classes['percentage'], color='skyblue')\nplt.xlabel('Percent Area')\nplt.title('Land Cover Classes around Mount Whitney')\n\n# Show plot\nplt.show()\n\n\n\n\n\nThe horizontal bar plot shows the percent area of different land cover classes in the area around Mount Whitney. Of any one class, Mediterrabean California Alpine Bedrock and Scree takes up the most land, constituting just over 15% of the analysis area. California Central Valley and Southern Coastal Grassland, Sierra Nevada Subalpine Lodgepole Pine Forest and Woodland, and Mediterranean California Mesic Mixed Conifer Forest and Woodland each make up about 8-10% of the area. Several other land cover classifications make up less than 8% of the area."
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#create-geodataframes-for-area-of-analysis-and-mount-whitney",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#create-geodataframes-for-area-of-analysis-and-mount-whitney",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "6) Create GeoDataFrames for area of analysis and Mount Whitney",
    "text": "6) Create GeoDataFrames for area of analysis and Mount Whitney\nTo better understand our area of analysis relative to Mount Whitney, we want to plot the geographic coordinates of lulc and a point for Mount Whitney on the same map. To do this, we first extract the bounding box of lulc, which gives us four coordinate points that make up a rectangle denoting the area analyzed from our raster. In this case, we know that our raster area was a square, which means that extracting the bounding box does not include additional area that was not part of the analysis. Next, we convert these points into a shapely.Polygon, which then allows us to create a geopandas.GeoDataFrame containing this polygon. Lastley, we make another geopandas.GeoDataFrame that just contains a single point for Mount Whitney. We know to provide the coordinates for Mount Whitney in the EPSG:4326 Coordinate Reference System because this is the same CRS that is set for the USGS data that we read in as lulc.\n\n\nCode\n# Extract bounding box of LULC tile\nbbox_coords = lulc.rio.bounds()\n\n# Create shapely polygon from bounding box coordinates\nbbox_polygon = box(bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3])\n\n# Create GeoDataFrame for bounding box of LULC tile\nbbox = gpd.GeoDataFrame(geometry=[bbox_polygon], crs=lulc.rio.crs)\n\n# Create GeoDataFrame with a single point for Mount Whitney\nmt_whitney = gpd.GeoDataFrame(geometry=[Point(-118.2929, 36.5786)], crs='EPSG:4326')"
  },
  {
    "objectID": "blog/2023-12-11-post/mt-whitney-land-cover.html#plot-map-showing-area-of-land-cover-analysis-relative-to-mount-whitney",
    "href": "blog/2023-12-11-post/mt-whitney-land-cover.html#plot-map-showing-area-of-land-cover-analysis-relative-to-mount-whitney",
    "title": "Land Cover Analysis around Mount Whitney",
    "section": "7) Plot map showing area of land cover analysis relative to Mount Whitney",
    "text": "7) Plot map showing area of land cover analysis relative to Mount Whitney\nAfter changing the CRS of bbox and ca to also match lulc, we can finally create our map showing the area of analysis relative to Mount Whitney!\n\n\nCode\n# Change CRS of LULC tile bounding box to EPSG:4326\nbbox.to_crs('EPSG:4326', inplace=True)\n\n# Change CRS of CA boundaries to EPSG:4326\nca.to_crs('EPSG:4326', inplace = True)\n\n# Initialize figure and axis\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot CA basemap, LULC tile bounding box, and Mount Whitney point\nca.plot(ax=ax, color='lightgray', edgecolor='black', alpha=0.5, label='California, US')\nbbox.plot(ax=ax, color='skyblue', edgecolor='black', alpha=0.5, label='Area of analysis')\nmt_whitney.plot(ax=ax, marker='^', color='red', markersize=100, label='Mount Whitney')\n\n# Specify legend elements\nlegend_elements = [plt.Line2D([0], [0], color='lightgray', lw=5, label='California'),\n                   plt.Line2D([0], [0], color='skyblue', lw=5, label='Area of analysis'),\n                   mlines.Line2D([0], [0], marker='^', color='w', markerfacecolor='red', markersize=10, label='Mount Whitney', linestyle='None')]\n\n# Add legend, title, and labels\nax.legend(handles=legend_elements, loc='upper right')\nplt.xlabel('Latitude')\nplt.ylabel('Longitude')\n\n# Show plot\nplt.show()\n\n\n\n\n\nThis map shows our area of land cover analysis in relation to Mount Whitney."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#background",
    "href": "blog/2024-4-3-post/dic-ml-models.html#background",
    "title": "Regression Models to predict Dissolved Inorganic Carbon using Ocean Chemistry features",
    "section": "Background",
    "text": "Background\nIn this blog post, I’ll build three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples. The features being used to make these predictions will be other ocean chemistry measurements that were also measured during water sampling. Our data set comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI), an oceanographic and marine ecosystem research program located in California (link to data set source). All water samples were taken off the California coast.\nThe first of the three models, which I’m including just for the sake of comparison, uses only a single decision tree. The other two models will be a random forest model and a stochastic gradient boosted (SGB) trees model."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#setup",
    "href": "blog/2024-4-3-post/dic-ml-models.html#setup",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport numpy as np\n\n# functions from sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# data viz libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#data-import-basic-pre-processing",
    "href": "blog/2024-4-3-post/dic-ml-models.html#data-import-basic-pre-processing",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Data import & basic pre-processing",
    "text": "Data import & basic pre-processing\n\n# import training data\ndf = pd.read_csv('data/train.csv')\n\n# inspect data\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1454 entries, 0 to 1453\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 1454 non-null   int64  \n 1   Lat_Dec            1454 non-null   float64\n 2   Lon_Dec            1454 non-null   float64\n 3   NO2uM              1454 non-null   float64\n 4   NO3uM              1454 non-null   float64\n 5   NH3uM              1454 non-null   float64\n 6   R_TEMP             1454 non-null   float64\n 7   R_Depth            1454 non-null   int64  \n 8   R_Sal              1454 non-null   float64\n 9   R_DYNHT            1454 non-null   float64\n 10  R_Nuts             1454 non-null   float64\n 11  R_Oxy_micromol.Kg  1454 non-null   float64\n 12  Unnamed: 12        0 non-null      float64\n 13  PO4uM              1454 non-null   float64\n 14  SiO3uM             1454 non-null   float64\n 15  TA1.x              1454 non-null   float64\n 16  Salinity1          1454 non-null   float64\n 17  Temperature_degC   1454 non-null   float64\n 18  DIC                1454 non-null   float64\ndtypes: float64(17), int64(2)\nmemory usage: 216.0 KB\nNone\n\n\n\n# remove 'id' and 'unnamed:_12' columns\ndf = df.drop(df.columns[[0, 12]], axis=1)"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#data-exploration-additional-pre-processing",
    "href": "blog/2024-4-3-post/dic-ml-models.html#data-exploration-additional-pre-processing",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Data exploration & additional pre-processing",
    "text": "Data exploration & additional pre-processing\n\n# heatmap of correlation between features\nplt.figure(figsize=(12, 10))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', square=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\nI’ll use this correlation heatmap to conduct feature selection, removing features that are highly correlated with other features. Specifically, I’ll remove ‘Temperature_degC’ (correlation with ‘R_TEMP’ is 1) and ‘R_Nuts’ (correlation with ‘NH3uM’ is 1).\n\n# define highly correlated features to remove when creating feature matrix\ncolumns_to_remove = ['Temperature_degC', 'R_Nuts']\n\n# define feature matrix and target vector\nX = df.drop(['DIC'] + columns_to_remove, axis=1)\ny = df['DIC']\n\n# split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n\n# check shapes of training and testing sets\nprint(f'X_train : {X_train.shape}')\nprint(f'y_train : {y_train.shape}')\nprint(f'X_test : {X_test.shape}')\nprint(f'y_test : {y_test.shape}')\n\nX_train : (1163, 14)\ny_train : (1163,)\nX_test : (291, 14)\ny_test : (291,)"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#model-using-single-decision-tree",
    "href": "blog/2024-4-3-post/dic-ml-models.html#model-using-single-decision-tree",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Model using single decision tree",
    "text": "Model using single decision tree\nFor our first model, we’ll build just a single decision tree. A decision tree generates predictions by asking simple yes-or-no questions about the features. Which question to ask is determined by the partitioning objective. For our partitioning objective, we’ll be minimizing mean squared error (MSE), which is the most common objective used for regression tasks. After we build all three models, we’ll compare them based on root mean squared error (RMSE).\n\n# define tree model\ntree_regressor = DecisionTreeRegressor()\n\n# create tuning grid for hyperparameters\nparam_grid = {\n    'decisiontreeregressor__max_depth': [None, 10, 20, 30, 40, 50], # max depth of tree\n    'decisiontreeregressor__min_samples_split': [2, 10, 20], # min number of samples required to split an internal node\n    'decisiontreeregressor__min_samples_leaf': [1, 5, 10] # min number of samples required to be at a leaf node\n}\n\n# setup for Pipeline and GridSearchCV\npipeline = Pipeline([\n    ('decisiontreeregressor', tree_regressor)\n])\ngrid_search = GridSearchCV(pipeline,\n                           param_grid,\n                           cv=5, # split data into 5 folds for CV\n                           scoring='neg_mean_squared_error') # determine best version of model based on MSE\n\n# fit model to training data and extract best version\ngrid_search.fit(X_train, y_train)\nbest_tree = grid_search.best_estimator_\n\n# predict testing data\ny_pred = best_tree.predict(X_test)\n\n# show RMSE and best parameters\nmse = mean_squared_error(y_test, y_pred)\ntree_rmse = np.sqrt(mse)\nprint(f'RMSE: {tree_rmse:.2f}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nRMSE: 8.21\nBest parameters: {'decisiontreeregressor__max_depth': None, 'decisiontreeregressor__min_samples_leaf': 5, 'decisiontreeregressor__min_samples_split': 10}"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#model-using-random-forest",
    "href": "blog/2024-4-3-post/dic-ml-models.html#model-using-random-forest",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Model using random forest",
    "text": "Model using random forest\nRandom forest models build a large collection of de-correlated trees to improve predictive performance. We now define an additional hyperparameter for the number of unique features that will be considered at each split in the decision tree. This hyperparameter, called mtry, makes it so we don’t have to worry about the trees being correlated with one another because we are only looking at a randomized subset of the features at each split in each tree. Having these un-correlated trees allows us to build many trees that are also deep, without overfitting to the training data. Because there are many trees in this model and these trees are also built to be deep based on a randomized set of features, they are referred to as a random forest.\n\n# define RF model\nrandom_forest_regressor = RandomForestRegressor()\n\n# create tuning grid for hyperparameters\nparam_grid = {\n    'randomforestregressor__n_estimators': [100],  # number of trees in forest\n    'randomforestregressor__max_depth': [30],  # max depth of each tree\n    'randomforestregressor__min_samples_split': [10],  # min number of samples required to split an internal node\n    'randomforestregressor__min_samples_leaf': [5]  # min number of samples required to be at a leaf node\n}\n\n# setup for Pipeline and GridSearchCV\npipeline = Pipeline([\n    ('randomforestregressor', random_forest_regressor)\n])\ngrid_search = GridSearchCV(pipeline,\n                           param_grid,\n                           cv=5, # split data into 5 folds for CV\n                           scoring='neg_mean_squared_error') # determine best version of model based on MSE\n\n# fit model to training data and extract best version\ngrid_search.fit(X_train, y_train)\nbest_forest = grid_search.best_estimator_\n\n# predict testing data\ny_pred = best_forest.predict(X_test)\n\n# show RMSE and best parameters\nmse = mean_squared_error(y_test, y_pred)\nrf_rmse = np.sqrt(mse)\nprint(f'RMSE: {rf_rmse:.2f}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nRMSE: 6.86\nBest parameters: {'randomforestregressor__max_depth': 30, 'randomforestregressor__min_samples_leaf': 5, 'randomforestregressor__min_samples_split': 10, 'randomforestregressor__n_estimators': 100}"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#model-using-stochastic-gradient-boosted-trees",
    "href": "blog/2024-4-3-post/dic-ml-models.html#model-using-stochastic-gradient-boosted-trees",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Model using stochastic gradient boosted trees",
    "text": "Model using stochastic gradient boosted trees\nBoosting is a general algorithm that is often applied to decision tree models as a way to improve predictive performance through introducing another form of randomization. Boosted models are built sequentially, as each version of the model is fit to the residuals from the previous version.\nBoosted tree models use a large number of shallow decision trees as a base learner. These early versions of the model, which are called “weak models” are improved sequentially based on the residuals of the previous version.\nIn SGB tree models, these “weak models” are improved using the sequential fitting algorithm of stochastic gradient descent, which uses random sampling of features to optimize the defined loss function (in this case, MSE) for each iteration based on the defined learning rate. If we choose too low of a learning rate, it may require too many iterations for our model to improve at all, but if we choose a learning rate that is too high, we may accidently skip over a better performing version of the model.\nAnother important thing to note is that even though SGB models use decision trees, the number of trees is no longer a hyperparameter. While number of trees served as the base estimator in each of the three previous models, the base estimator for SGB models is the number of iterations of the sequential fitting algorithm (i.e., boosting stages) to perform. In SGB models, the number of trees is just an “ordinary” parameter being set based on the results of the sequential fitting algorithm.\n\n# define SGB model\ngradient_boosting_regressor = GradientBoostingRegressor()\n\n# create tuning grid for hyperparameters\nparam_grid = {\n    'gradientboostingregressor__n_estimators': [3000],  # number of boosting stages to perform\n    'gradientboostingregressor__learning_rate': [0.01, 0.1],  # learning rate\n    'gradientboostingregressor__max_depth': [10],  # max depth of individual regression estimators\n    'gradientboostingregressor__min_samples_split': [10],  # min number of samples required to split an internal node\n    'gradientboostingregressor__min_samples_leaf': [5]  # min number of samples required to be at a leaf node\n}\n\n# setup for Pipeline and GridSearchCV\npipeline = Pipeline([\n    ('gradientboostingregressor', gradient_boosting_regressor)\n])\ngrid_search = GridSearchCV(pipeline,\n                           param_grid,\n                           cv=5, # split data into 5 folds for CV\n                           scoring='neg_mean_squared_error') # determine best version of model based on MSE\n\n# fit model to training data and extract best version\ngrid_search.fit(X_train, y_train)\nbest_gradient_boosting = grid_search.best_estimator_\n\n# predict testing data\ny_pred = best_gradient_boosting.predict(X_test)\n\n# show RMSE and best parameters\nmse = mean_squared_error(y_test, y_pred)\nsgb_rmse = np.sqrt(mse)\nprint(f'RMSE: {sgb_rmse:.2f}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nRMSE: 6.91\nBest parameters: {'gradientboostingregressor__learning_rate': 0.1, 'gradientboostingregressor__max_depth': 10, 'gradientboostingregressor__min_samples_leaf': 5, 'gradientboostingregressor__min_samples_split': 10, 'gradientboostingregressor__n_estimators': 3000}"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#compare-models",
    "href": "blog/2024-4-3-post/dic-ml-models.html#compare-models",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Compare models",
    "text": "Compare models\n\n# create lists of RMSE values and models\nrmse_values = [sgb_rmse, rf_rmse, tree_rmse]\nmodels = ['SG Boosted Trees\\n(n=3000)', 'Random Forest\\n(n=100)', 'Single Tree\\n(n=1)']\n\n# create bar plot\nplt.figure(figsize=(10, 6))\nbars = plt.bar(models, rmse_values, color=['cornflowerblue'])\nplt.title('Model Comparison')\nplt.xlabel('Models')\nplt.ylabel('RMSE')\n\n# add value labels on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='bottom')\n\nplt.show()\n\n\n\n\nOur best performing model was the random forest model, with a RMSE of 6.856. In normal English, a RMSE of 6.856 means that, on average, the predictions made by this model deviate from the actual DIC values by 6.856 micromoles per kilogram."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#compare-feature-importance",
    "href": "blog/2024-4-3-post/dic-ml-models.html#compare-feature-importance",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Compare feature importance",
    "text": "Compare feature importance\n\n# extract importance of each feature from best version of model\nimportances = best_forest.named_steps['randomforestregressor'].feature_importances_\n\n# create df to hold feature names and their importance\nimportance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': importances\n})\n\n# sort by importance in descending order\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# create plot\nplt.figure(figsize=(12, 8))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\nplt.gca().invert_yaxis()\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances in RF Model')\nplt.grid(True, which='both', axis = 'x', linestyle='--', linewidth=0.5)\n\nplt.show()\n\n\n\n\nHere, we see that the most important feature for predicting DIC in the random forest model was ‘SiO3uM’, with a feature importance over 0.70. This was significantly higher than the second most important feature, ‘PO4uM’, which had a feature importance of close to 0.2."
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#background",
    "href": "blog/2024-3-29-post/index.html#background",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Background",
    "text": "Background\nThe idea for this blog post comes from a group assignment in my machine learning class. For this assignment, my classmate Maxwell and I started by each using the Spotify Web API to access our recent liked songs data, and we each retrieved data on our 200 most recently liked songs (cite spotify web API). We then trained three decision tree models using 75% of the data (training set) and compared performance based on performance on the remaining 25% (testing set). Now, I’ve decided to go back and also build a model that uses Stochastic Gradient Boosted (SGB) decision trees and also update my model comparisons to include this SGB model."
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#setup-data-import",
    "href": "blog/2024-3-29-post/index.html#setup-data-import",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Setup & data import",
    "text": "Setup & data import\nAccess the Spotify Web API requires having an existing Spotify account and creating a Spotify for Developers account on the Spotify for Developers website. For the purposes of this blog, which focuses on the model-building process, I’ll skip over the API access steps. Instead, I’ll just start by importing the CSV files, which were written using information from the API.\n\n\nCode\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(baguette)\nlibrary(vip)\n\n# read in my data (CSV that was previously written)\nlinus_tracks &lt;- read.csv(here::here(\"data\", \"2024-3-29-post-data\", \"linus_tracks.csv\"))\n\n# read in partner data\nmaxwell_tracks &lt;- read.csv(here::here(\"data\", \"2024-3-29-post-data\", \"maxwell_songs.csv\")) %&gt;% \n  mutate(name = \"maxwell\")\n\n# bind my liked songs df with partner df\ncombined_tracks &lt;- rbind(linus_tracks, maxwell_tracks) %&gt;% \n  rename(time_sig = time_signature) %&gt;% \n  select(-track.name, -type, -id, -uri, -track_href, -analysis_url) # remove irrelevent columns"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#data-exploration",
    "href": "blog/2024-3-29-post/index.html#data-exploration",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Data exploration",
    "text": "Data exploration\nOur data set contains 13 features that might be useful for predicting whether a song is in my collection or Maxwell’s. I’ll start by exploring what these 13 features are and how some of them vary between Maxwell’s liked songs and my own. Note that the first 13 columns contain the features we are looking at, and the 14th column contains our outcome variable ‘name’, which is either ‘linus’ or ‘maxwell’ depending on whose collection it is from.\nIn the following code, we look at a summarized breakdown of each column in the combined tracks data frame and then take a closer look at how Maxwell and I differ in terms of the tempo and danceability of our liked songs.\n\n\nCode\n# look at summary of columns\nsummary(combined_tracks)\n\n\n  danceability        energy            key           loudness      \n Min.   :0.1440   Min.   :0.0844   Min.   : 0.00   Min.   :-20.149  \n 1st Qu.:0.5570   1st Qu.:0.5350   1st Qu.: 2.00   1st Qu.: -9.371  \n Median :0.6580   Median :0.6865   Median : 5.00   Median : -7.524  \n Mean   :0.6428   Mean   :0.6608   Mean   : 5.27   Mean   : -8.053  \n 3rd Qu.:0.7642   3rd Qu.:0.8215   3rd Qu.: 8.00   3rd Qu.: -5.899  \n Max.   :0.9730   Max.   :0.9840   Max.   :11.00   Max.   : -2.421  \n      mode         speechiness       acousticness       instrumentalness   \n Min.   :0.0000   Min.   :0.02450   Min.   :0.0000075   Min.   :0.0000000  \n 1st Qu.:0.0000   1st Qu.:0.03610   1st Qu.:0.0121500   1st Qu.:0.0000229  \n Median :1.0000   Median :0.04685   Median :0.0721000   Median :0.0198500  \n Mean   :0.5975   Mean   :0.07576   Mean   :0.2068194   Mean   :0.2509544  \n 3rd Qu.:1.0000   3rd Qu.:0.07193   3rd Qu.:0.3255000   3rd Qu.:0.5235000  \n Max.   :1.0000   Max.   :0.51900   Max.   :0.9780000   Max.   :0.9670000  \n    liveness          valence           tempo         duration_ms    \n Min.   :0.02990   Min.   :0.0322   Min.   : 61.85   Min.   : 57877  \n 1st Qu.:0.09557   1st Qu.:0.2640   1st Qu.:112.08   1st Qu.:169776  \n Median :0.11900   Median :0.4530   Median :126.03   Median :210192  \n Mean   :0.16955   Mean   :0.4711   Mean   :125.03   Mean   :219639  \n 3rd Qu.:0.19250   3rd Qu.:0.6843   3rd Qu.:135.12   3rd Qu.:257514  \n Max.   :0.93300   Max.   :0.9810   Max.   :208.65   Max.   :627097  \n    time_sig         name          \n Min.   :1.000   Length:400        \n 1st Qu.:4.000   Class :character  \n Median :4.000   Mode  :character  \n Mean   :3.947                     \n 3rd Qu.:4.000                     \n Max.   :5.000                     \n\n\n\n\nCode\n# compare mean of tempo and danceability for Linus and Maxwell\ncombined_tracks %&gt;%\n  group_by(name) %&gt;%\n  summarise(mean_tempo = mean(tempo),\n            mean_danceability = mean(danceability),\n            mean_instrumentalness = mean(instrumentalness)) %&gt;% \n  ungroup()\n\n\n# A tibble: 2 × 4\n  name    mean_tempo mean_danceability mean_instrumentalness\n  &lt;chr&gt;        &lt;dbl&gt;             &lt;dbl&gt;                 &lt;dbl&gt;\n1 linus         125.             0.609                 0.157\n2 maxwell       125.             0.676                 0.344\n\n\n\n\nCode\n# compare distribution of tempo for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$tempo, combined_tracks$name),\n             main = \"Spotify liked songs comparison of tempo\", \n             ylab = \"tempo\",\n             xlab = c(\"linus\", \"maxwell\"))\n\n\n\n\n\nWhile the liked songs of Maxwell and I have a very similar mean tempo, the tempo of my liked songs exhibits a significantly wider distribution.\n\n\nCode\n# compare distribution of danceability for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$danceability, combined_tracks$name),\n             main = \"Spotify liked songs comparison of danceability\", \n             ylab = \"danceability\",\n             xlab = c(\"linus\", \"maxwell\"))\n\n\n\n\n\nMaxwell’s collection has a slightly higher mean danceability, and the distribution for his songs is more left skewed compared to mine.\n\n\nCode\n# compare distribution of instrumentalness for Linus and Maxwell\nHmisc::histbackback(split(combined_tracks$instrumentalness, combined_tracks$name),\n             main = \"Spotify liked songs comparison of instrumentalness\", \n             ylab = \"instrumentalness\",\n             xlab = c(\"linus\", \"maxwell\"))\n\n\n\n\n\nThe mean instrumentalness of Maxwell’s songs is more than twice that of the mean for my collection, and this is reflected in the histogram by the larger proportion of my songs with an instrumentalness of zero."
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#data-pre-processing",
    "href": "blog/2024-3-29-post/index.html#data-pre-processing",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Data pre-processing",
    "text": "Data pre-processing\nTo start, we’ll set the seed. This sets the randomization for creating our cross validation folds such that our results will be reproduced if ran on my local device again. We set the seed in its own code chunk because sometimes it can interfere with other process if included in a larger code chunk.\n\n\nCode\n# set seed\nset.seed(123)\n\n\nWe split our data in training and testing sets. We’ll use the training set to train the model during cross validation and the testing set to compare the performance of the different models. Next, we pre-process the data by specifying and prepping a recipe that converts all nominal features to dummy variables and normalizes all numeric features. We also create 10 folds of the training data to use for cross validation.\n\n\nCode\n# initial split of data into training and testing sets (default 75/25)\ntracks_split &lt;- initial_split(combined_tracks)\ntracks_test &lt;- testing(tracks_split)\ntracks_train &lt;- training(tracks_split)\n\n# specify recipe for model preprocessing\ntracks_recipe &lt;- recipe(name ~ ., data = tracks_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  prep() # prep recipe\n\n# create 10 folds of the training data set for CV\ncv_folds &lt;- tracks_train %&gt;% vfold_cv(v = 10)"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#decision-tree-model",
    "href": "blog/2024-3-29-post/index.html#decision-tree-model",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Decision tree model",
    "text": "Decision tree model\nFor our first model, we’ll build just a single decision tree. A decision tree generates predictions by asking simple yes-or-no questions about the features. Which question to ask is determined by the partitioning objective. For our partitioning objective, we will be minimizing cross-entropy, which is the most common objective used for classification tasks.\n\nBuild preliminary model & tune hyperparameters\n\n\nCode\n# specify model for tuning hyperparameters\nsingle_tree_spec &lt;- decision_tree(\n  cost_complexity = tune(), # tune cost complexity for pruning tree\n  tree_depth = tune(), # tune maximum tree depth\n  min_n = tune()) %&gt;% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# create tuning grid for hyperparameters\ntuning_grid &lt;- grid_latin_hypercube(cost_complexity(),\n                                    tree_depth(),\n                                    min_n(),\n                                    size = 10)\n\n# create workflow for tuning hyperparameters\nsingle_tree_wf &lt;- workflow() %&gt;%\n  add_recipe(tracks_recipe) %&gt;%\n  add_model(single_tree_spec)\n\n# tune hyperparameters using CV\nsingle_tree_tune &lt;- tune_grid(single_tree_spec, \n                              as.factor(name) ~ ., \n                              resamples = cv_folds,\n                              grid = tuning_grid,\n                              metrics = metric_set(accuracy))\n\n\n\n\nBuild final model & predict testing data\n\n\nCode\n# specify final model with optimized hyperparameters\nsingle_tree_final &lt;- finalize_model(single_tree_spec, select_best(single_tree_tune))\n\n# fit final model to training data\nsingle_tree_fit &lt;- fit(single_tree_final, as.factor(name)~., tracks_train)\n\n# predict testing data\nsingle_tree_predict &lt;- predict(single_tree_fit, tracks_test) %&gt;%\n  bind_cols(tracks_test) %&gt;%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nsingle_tree_predict &lt;- predict(single_tree_fit, tracks_test, type = \"prob\") %&gt;%\n  bind_cols(single_tree_predict) %&gt;%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nsingle_tree_conf_matrix &lt;- single_tree_predict %&gt;% \n  conf_mat(truth = name, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Single DT\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nsingle_tree_accuracy &lt;- accuracy(single_tree_predict, truth = name, estimate = .pred_class)\nsingle_tree_roc_auc &lt;- roc_auc(single_tree_predict, truth = name, .pred_linus)\nsingle_tree_sensitivity &lt;- sensitivity(single_tree_predict, truth = name, estimate = .pred_class)\nsingle_tree_specificity &lt;- specificity(single_tree_predict, truth = name, estimate = .pred_class)"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#bagged-trees-model",
    "href": "blog/2024-3-29-post/index.html#bagged-trees-model",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Bagged trees model",
    "text": "Bagged trees model\nBagged, or “bootstrap aggregating”, prediction models train multiple shallow decision tree models and then combines them to generate an aggregated prediction. Compared to building a single deep decision tree, building multiple shallow decision trees greatly reduces the potential for overfitting. However, in a bagged decision tree model, there is concern about the trees being correlated with one another, meaning they may not provide a substantial improvement in predictive power.\n\nBuild final model & predict testing data (no tuning required)\n\n\nCode\n# specify model\nbagged_trees_spec &lt;- bag_tree() %&gt;%\n  set_engine(\"rpart\", times = 50) %&gt;% # specify number of trees (50-500 trees is usually sufficient)\n  set_mode(\"classification\")\n\n# create workflow\nbagged_trees_wf &lt;- workflow() %&gt;%\n  add_recipe(tracks_recipe) %&gt;%\n  add_model(bagged_trees_spec)\n\n# fit model to training data\nbagged_trees_fit &lt;- bagged_trees_wf %&gt;%\n  fit(data = tracks_train)\n\n# predict testing data\nbagged_trees_predict &lt;- predict(bagged_trees_fit, tracks_test) %&gt;% \n  bind_cols(tracks_test) %&gt;%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nbagged_trees_predict &lt;- predict(bagged_trees_fit, tracks_test, type = \"prob\") %&gt;%\n  bind_cols(bagged_trees_predict) %&gt;%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nbagged_trees_conf_matrix &lt;- bagged_trees_predict %&gt;% \n  conf_mat(truth = name, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Bagged DTs\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nbagged_trees_accuracy &lt;- accuracy(bagged_trees_predict, truth = name, estimate = .pred_class)\nbagged_trees_roc_auc &lt;- roc_auc(bagged_trees_predict, truth = name, .pred_linus)\nbagged_trees_sensitivity &lt;- sensitivity(bagged_trees_predict, truth = name, estimate = .pred_class)\nbagged_trees_specificity &lt;- specificity(bagged_trees_predict, truth = name, estimate = .pred_class)"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#random-forest-model",
    "href": "blog/2024-3-29-post/index.html#random-forest-model",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Random forest model",
    "text": "Random forest model\nRandom forest models are a modification of bagged decision trees that builds a large collection of de-correlated trees to further improve predictive performance. Unlike with bagged decision trees, we now define an additional hyperparameter for the number of unique features that will be considered at each split in the decision tree. This hyperparameter, called mtry, makes it so we don’t have to worry about the trees being correlated with one another because we are only looking at a randomized subset of the features at each split in each tree. Having these un-correlated trees allows us to build many trees that are also deep, without overfitting to the training data. Because there are many trees in this model and these trees are also built to be deep based on a randomized set of features, they are referred to as a random forest.\n\nBuild preliminary model & tune hyperparameters\n\n\nCode\n# specify model for tuning hyperparameters\nrf_spec &lt;- rand_forest(trees = 500, # set number of trees to 500\n                       mtry = tune(), # tune mtry (number of unique feature variables that will be considered at each split)\n                       min_n = tune()) %&gt;% # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# create tuning grid for hyperparameters\n  tuning_grid &lt;- grid_latin_hypercube(mtry(range = c(2, 4)), \n                                      min_n(c(1, 10)),\n                                      size = 10)\n\n# create workflow for tuning hyperparameters\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(tracks_recipe) %&gt;%\n  add_model(rf_spec)\n\n# tune hyperparameters using CV\nrf_tune &lt;- tune_grid(rf_wf,\n                     resamples = cv_folds,\n                     grid = tuning_grid,\n                     metrics = metric_set(accuracy))\n\n\n\n\nBuild final model & predict testing data\n\n\nCode\n# specify final model with optimized hyperparameters\nrf_final &lt;- finalize_model(rf_spec, select_best(rf_tune))\n\n# create workflow for final version of model\nrf_final_wf &lt;- workflow() %&gt;%\n  add_recipe(tracks_recipe) %&gt;%\n  add_model(rf_final)\n\n# fit final workflow to training data\nrf_fit &lt;- rf_final_wf %&gt;%\n  fit(data = tracks_train)\n\n# predict testing data\nrf_predict &lt;- predict(rf_fit, tracks_test) %&gt;%\n  bind_cols(tracks_test) %&gt;%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nrf_predict &lt;- predict(rf_fit, tracks_test, type = \"prob\") %&gt;%\n  bind_cols(rf_predict) %&gt;%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nrf_conf_matrix &lt;- rf_predict %&gt;% \n  conf_mat(truth = name, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"Random Forest\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nrf_accuracy &lt;- accuracy(rf_predict, truth = name, estimate = .pred_class)\nrf_roc_auc &lt;- roc_auc(rf_predict, truth = name, .pred_linus)\nrf_sensitivity &lt;- sensitivity(rf_predict, truth = name, estimate = .pred_class)\nrf_specificity &lt;- specificity(rf_predict, truth = name, estimate = .pred_class)"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#stochastic-gradient-boosting-sgb-model",
    "href": "blog/2024-3-29-post/index.html#stochastic-gradient-boosting-sgb-model",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Stochastic Gradient Boosting (SGB) model",
    "text": "Stochastic Gradient Boosting (SGB) model\nBoosting is a general algorithm that is often applied to decision tree models as a way to improve predictive performance through introducing another form of randomization. Boosted models are built sequentially, as each version of the model is fit to the residuals from the previous version.\nSGB models use a large number of shallow decision trees as a base learner. These early versions of the model, which are called “weak models” are improved sequentially based on the residuals of the previous version. At each sequential step, these weak models are improved using the sequential fitting algorithm of stochastic gradient descent, which uses random sampling of features to optimize the defined loss function (for this classification problem, we will look to optimize accuracy) for each iteration based on the defined learning rate. We start by tuning the learning rate, which specifies the extent to which we want to change our weak models at each iteration. If we choose to low of a learning rate, it may require too many iterations for our model to improve at all, but if we choose a learning rate that is too high, we may accidently skip over a better performing version of the model.\n\nBuild preliminary model & tune learning rate\n\n\nCode\n# specify model for tuning learning rate\nsgb_lr_spec &lt;- boost_tree(mode = \"classification\",\n                      engine = \"xgboost\",\n                      learn_rate = tune())\n\n# create tuning grid for learning rate\ntuning_grid &lt;- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n# create workflow for tuning learning rate\nsgb_lr_wf &lt;- workflow() %&gt;%\n  add_model(sgb_lr_spec) %&gt;%\n  add_recipe(tracks_recipe)\n\n# tune learning rate using CV\nsgb_lr_tune &lt;- tune_grid(sgb_lr_wf,\n                         resamples = cv_folds,\n                         grid = tuning_grid,\n                         metrics = metric_set(accuracy))\n\n# store optimized learning rate\nbest_lr &lt;- select_best(sgb_lr_tune)\n\n\n\n\nBuild preliminary model & tune tree parameters\n\n\nCode\n# specify model for tuning tree parameters\nsgb_tree_spec &lt;- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate from previous step\n                            trees = 3000, # set number of trees to 3000\n                            tree_depth = tune(), # tune maximum tree depth\n                            min_n = tune(), # tune minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n                            loss_reduction = tune(), # tune loss reduction (minimum loss required for further splits)\n                            mode = \"classification\",\n                            engine = \"xgboost\")\n\n# create tuning grid for tree parameters\ntuning_grid &lt;- grid_latin_hypercube(tree_depth(),\n                                    min_n(),\n                                    loss_reduction(),\n                                    size = 10)\n\n# create workflow for tuning tree parameters\nsgb_tree_wf &lt;- workflow() %&gt;%\n  add_model(sgb_tree_spec) %&gt;%\n  add_recipe(tracks_recipe)\n\n# tune tree parameters using CV\nsgb_tree_tune &lt;- tune_grid(sgb_tree_wf,\n                           resamples = cv_folds,\n                           grid = tuning_grid,\n                           metrics = metric_set(accuracy))\n\n# store optimized tree parameters\nbest_tree &lt;- select_best(sgb_tree_tune)\n\n\n\n\nBuild preliminary model & tune stochasticity parameters\n\n\nCode\n# specify model for tuning stochasticity parameters\nsgb_stochastic_spec &lt;- boost_tree(learn_rate = best_lr$learn_rate, # use optimized learning rate\n                                  trees = 3000, # set number of trees to 3000\n                                  tree_depth = best_tree$tree_depth, # use optimized maximum tree depth\n                                  min_n = best_tree$min_n, # use optimized minimum n for a terminal node (minimum number of data points in a node that is required for the node to be split further)\n                                  loss_reduction = best_tree$loss_reduction, # use optimized loss reduction (minimum loss required for further splits)\n                                  mtry = tune(), # tune mtry (number of unique feature variables in each subsample)\n                                  sample_size = tune(), # tune sample size (amount of randomly selected data exposed to the fitting routine when conducting stochastic gradient descent at each split)\n                                  mode = \"classification\",\n                                  engine = \"xgboost\")\n\n# specify mtry range based on the number of predictors\nmtry_final &lt;- finalize(mtry(), tracks_train)\n\n# create tuning grid for stochasticity parameters\ntuning_grid &lt;- grid_latin_hypercube(mtry_final,\n                                    sample_size = sample_prop(),\n                                    size = 10)\n\n# create workflow for tuning stochasticity parameters\nsgb_stochastic_wf &lt;- workflow() %&gt;%\n  add_model(sgb_stochastic_spec) %&gt;%\n  add_recipe(tracks_recipe)\n\n# tune stochasticity parameters using CV\nsgb_stochastic_tune &lt;- tune_grid(sgb_stochastic_wf,\n                                 resamples = cv_folds,\n                                 grid = tuning_grid,\n                                 metrics = metric_set(accuracy))\n\n# store optimized stochasticity parameters\nbest_stochastic &lt;- select_best(sgb_stochastic_tune)\n\n\n\n\nBuild final model & predict testing data\n\n\nCode\n# specify final model with optimized parameters\nsgb_final &lt;- finalize_model(sgb_stochastic_spec, best_stochastic)\n\n# fit final model to training data\nsgb_fit &lt;- fit(sgb_final, as.factor(name)~., tracks_train)\n\n# predict testing data\nsgb_predict &lt;- predict(sgb_fit, tracks_test) %&gt;%\n  bind_cols(tracks_test) %&gt;%  # bind to testing df\n  mutate(name = as.factor(name))\n\n# get probabilities for predictions made on testing data (to calculate ROC AUC)\nsgb_predict &lt;- predict(sgb_fit, tracks_test, type = \"prob\") %&gt;%\n  bind_cols(sgb_predict) %&gt;%  # bind to df that was just created\n  mutate(name = as.factor(name))\n\n# store confusion matrix for predictions made on testing data\nsgb_conf_matrix &lt;- sgb_predict %&gt;% \n  conf_mat(truth = name, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  ggtitle(\"SGB\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# store error metrics of testing data predictions\nsgb_accuracy &lt;- accuracy(sgb_predict, truth = name, estimate = .pred_class)\nsgb_roc_auc &lt;- roc_auc(sgb_predict, truth = name, .pred_linus)\nsgb_sensitivity &lt;- sensitivity(sgb_predict, truth = name, estimate = .pred_class)\nsgb_specificity &lt;- specificity(sgb_predict, truth = name, estimate = .pred_class)"
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#compare-models",
    "href": "blog/2024-3-29-post/index.html#compare-models",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Compare models",
    "text": "Compare models\n\n\nCode\n# display confusion matrices of all four models\nsingle_tree_conf_matrix + bagged_trees_conf_matrix + rf_conf_matrix + sgb_conf_matrix +\n  plot_layout(nrow = 2, ncol = 2)\n\n\n\n\n\n\n\nCode\n# create tibble of accuracy and ROC AUC for all four models\nmetrics_tibble &lt;- tibble(\n  Method = factor(rep(c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\"), times = 2),\n                  levels = c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\")),\n  Metric = rep(c(\"Accuracy\", \"Area under Receiver Operating Characteristic (ROC) curve\"), each = 4),\n  Value = c(single_tree_accuracy$.estimate[1], bagged_trees_accuracy$.estimate[1],\n            rf_accuracy$.estimate[1], sgb_accuracy$.estimate[1],\n            single_tree_roc_auc$.estimate[1], bagged_trees_roc_auc$.estimate[1],\n            rf_roc_auc$.estimate[1], sgb_roc_auc$.estimate[1]))\n\n# create bar plot comparing accuracy and ROC AUC across all four models\nggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  geom_text(aes(label = sprintf(\"%.2f\", Value),\n                y = Value + 0.02),\n            position = position_dodge(width = 0.9),\n            vjust = 0,\n            size = 4) +\n  theme_minimal() +\n  labs(y = \"Metric Value\", x = \"Model\", title = \"Model Comparison\") +\n  scale_fill_brewer(palette = \"BuPu\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\nCode\n# create tibble of accuracy and ROC AUC for all four models\nmetrics_tibble &lt;- tibble(\n  Method = factor(rep(c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\"), times = 2),\n                  levels = c(\"Single DT\", \"Bagged DTs\", \"Random Forest\", \"SGB\")),\n  Metric = rep(c(\"Sensitivity\\n(Accuracy when truth was Linus)\", \"Specificity\\n(Accuracy when truth was Maxwell)\"), each = 4),\n  Value = c(single_tree_sensitivity$.estimate[1], bagged_trees_sensitivity$.estimate[1],\n            rf_sensitivity$.estimate[1], sgb_sensitivity$.estimate[1],\n            single_tree_specificity$.estimate[1], bagged_trees_specificity$.estimate[1],\n            rf_specificity$.estimate[1], sgb_specificity$.estimate[1]))\n\n\n# create bar plot comparing sensitivity and specificity across all four models\nggplot(metrics_tibble, aes(x = Method, y = Value, fill = Metric)) + \n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  geom_text(aes(label = sprintf(\"%.2f\", Value),\n                y = Value + 0.02),\n            position = position_dodge(width = 0.9),\n            vjust = 0,\n            size = 4) +\n  theme_minimal() +\n  labs(y = \"Metric Value\", x = \"Model\", title = \"Model Comparison\") +\n  scale_fill_brewer(palette = \"Greens\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        legend.position = \"top\",\n        legend.title = element_blank(),\n        legend.key.height = unit(10, \"mm\"))\n\n\n\n\n\nThe Stochastic Gradient Boosting (SGB) model performed the best at predicting the testing data, slightly outperforming the random forest model. While both the SGB and random forest models had the same accuracy for correctly classifying songs that were in my collection, the SGB model was slightly better at accurately classifying songs that were in Maxwell’s collection. When using a single decision tree, there was a significant drop-off in accuracy, largely due to difficulty classifying songs that were in Maxwell’s collection."
  },
  {
    "objectID": "blog/2024-3-29-post/index.html#compare-importance-of-predictor-variables",
    "href": "blog/2024-3-29-post/index.html#compare-importance-of-predictor-variables",
    "title": "Classification Models using data from Spotify Web API",
    "section": "Compare importance of predictor variables",
    "text": "Compare importance of predictor variables\n\n\nCode\n# compare importance of different predictor variables in best performing model\nvip(sgb_fit, method = \"model\", num_features = 13) +\n  ggtitle(\"Importance of features in SGB model\") +\n  labs(caption = \"Note: Importance of time_sig is &lt;0.01\") +\n  ylim(0.00, 0.20) +\n  geom_text(aes(label = sprintf(\"%.2f\", Importance), # label values\n                x = Variable,\n                y = Importance + 0.001),\n            hjust = 0,\n            color = \"black\",\n            size = 3.5) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        axis.text.y = element_text(color = \"black\", size = 12))\n\n\n\n\n\nIn the SGB model, which was the best performing model, tempo, valence, danceability, energy, and instrumentalness were the most important feature for predicting whether a song was in my collection or Maxwell’s."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Sustainable Urban Development in São Paulo\n\n\n\n\n\n\n\nAcademic Writing\n\n\n\n\n8-page essay on sustainable development goals in São Paulo, Brazil, looking at factors such the city’s Ecological Footprint and Gini Coefficient.\n\n\n\n\n\n\nMay 27, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nEconomic Analysis of EV Tax Credits from the Inflation Reduction Act\n\n\n\n\n\n\n\nMemo Writing\n\n\n\n\n6-page memo explaining the likely impacts of new federal policy incentives regarding Light-Duty Elective Vehicles.\n\n\n\n\n\n\nApr 26, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nThe Ethics of Lawn Mowing\n\n\n\n\n\n\n\nGeneral Audience\n\n\n\n\n11-page essay examining the social history of lawns, in addition to their ecological consequences, to consider the ethical implications of mowing one’s lawn.\n\n\n\n\n\n\nApr 3, 2023\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nEcology and Human Health in California’s Central Valley: Historical Perceptions of a Complex Relationship\n\n\n\n\n\n\n\nAcademic Writing\n\n\n\n\n8-page essay based on Historian Linda Nash’s 2006 book Inescapable Ecologies, which traces evolving views of health and disease in California’s Central Valley during the 19th and 20th centuries.\n\n\n\n\n\n\nNov 1, 2022\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nPolicy Memo on U.S. Offshore Wind Development\n\n\n\n\n\n\n\nMemo Writing\n\n\n\n\nThis 3-page memo and presentation slide-deck on offshore wind development were produced for an internship with MARAD, which is an agency of the USDOT focusing on domestic maritime transportation.\n\n\n\n\n\n\nAug 27, 2022\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nComparison of Approaches to Carbon Pricing\n\n\n\n\n\n\n\nMemo Writing\n\n\n\n\n7-page memo on market-based climate change mitigation policies. After comparing different approaches from around the world, the memo recommends for the U.S. to adopt a federal cap-and-trade system for the electricity generation sector and a federal tax on GHG emissions from industrial buildings, transportation fuels, fuels used for heating and cooling, and imported goods.\n\n\n\n\n\n\nApr 27, 2022\n\n\nLinus Ghanadan\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the Impact of California Disposable Carryout Bag Policies\n\n\n\n\n\n\n\nSlide-Deck Presentation\n\n\n\n\n20-minute group presentation on research design and econometric results from Rebecca L.C. Taylor’s 2019 paper in the Journal of Environmental Economics and Management.\n\n\n\n\n\n\nApr 27, 2022\n\n\nLinus Ghanadan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writing.html#nature",
    "href": "writing.html#nature",
    "title": "Writing",
    "section": "",
    "text": "The Ethics of Lawn Mowing: Description of the third post. Date: 2023-06-01"
  },
  {
    "objectID": "writing/post1/post1.html",
    "href": "writing/post1/post1.html",
    "title": "The Ethics of Lawn Mowing",
    "section": "",
    "text": "View as PDF\n\n\n\nCitationBibTeX citation:@online{ghanadan2023,\n  author = {Ghanadan, Linus},\n  title = {The {Ethics} of {Lawn} {Mowing}},\n  date = {2023-04-29},\n  url = {https://linusghanadan.github.io/blog/lawn-ethics-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGhanadan, Linus. 2023. “The Ethics of Lawn Mowing.” April\n29, 2023. https://linusghanadan.github.io/blog/lawn-ethics-post/."
  },
  {
    "objectID": "writing/lawn-ethics/lawn-ethics.html",
    "href": "writing/lawn-ethics/lawn-ethics.html",
    "title": "The Ethics of Lawn Mowing",
    "section": "",
    "text": "This paper was written for a writing class that I took at the University of Maryland in Spring 2023."
  },
  {
    "objectID": "writing/carbon-pricing/carbon-pricing.html",
    "href": "writing/carbon-pricing/carbon-pricing.html",
    "title": "Comparison of Approaches to Carbon Pricing",
    "section": "",
    "text": "This paper was written for a class on Energy & Environmental Policy that I took at the University of Maryland in Spring 2022."
  },
  {
    "objectID": "writing/env-health/env-health.html",
    "href": "writing/env-health/env-health.html",
    "title": "Ecology and Human Health in California’s Central Valley: Historical Perceptions of a Complex Relationship",
    "section": "",
    "text": "This paper was written for a class on Environmental History that I took at the University of Maryland in Fall 2022."
  },
  {
    "objectID": "writing/wildfire/wildfire.html",
    "href": "writing/wildfire/wildfire.html",
    "title": "The Environmental, Economic, and Social Impacts of California’s Wildfires",
    "section": "",
    "text": "This paper was written for a class on Environmental Science that I took at the University of Maryland in Fall 2021."
  },
  {
    "objectID": "writing/evs/ev.html",
    "href": "writing/evs/ev.html",
    "title": "Economic Analysis of EV Tax Credits from the Inflation Reduction Act",
    "section": "",
    "text": "This memo was written for a class on Energy & Environmental Economics that I took at the University of Maryland in Spring 2023."
  },
  {
    "objectID": "writing/offshore-wind/offshore-wind.html#view-25-slide-presentation-as-pdf",
    "href": "writing/offshore-wind/offshore-wind.html#view-25-slide-presentation-as-pdf",
    "title": "Policy Memo on U.S. Offshore Wind Development",
    "section": "View 25-slide presentation as PDF",
    "text": "View 25-slide presentation as PDF\n\n\n\nDionysos1970, CC BY-SA 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "writing/offshore-wind/offshore-wind.html",
    "href": "writing/offshore-wind/offshore-wind.html",
    "title": "Policy Memo on U.S. Offshore Wind Development",
    "section": "",
    "text": "Dionysos1970, CC BY-SA 4.0, via Wikimedia Commons\nThis 3-page memo and presentation slide-deck on offshore wind development were produced for an internship with the U.S. Maritime Administration (MARAD), which is an agency of the U.S. Department of Transportation focusing on domestic maritime transportation."
  },
  {
    "objectID": "writing/offshore-wind/offshore-wind.html#view-3-page-memo-as-pdf",
    "href": "writing/offshore-wind/offshore-wind.html#view-3-page-memo-as-pdf",
    "title": "Policy Memo on U.S. Offshore Wind Development",
    "section": "View 3-page memo as PDF",
    "text": "View 3-page memo as PDF"
  },
  {
    "objectID": "writing/carbon-pricing/carbon-pricing.html#view-7-page-memo-as-pdf",
    "href": "writing/carbon-pricing/carbon-pricing.html#view-7-page-memo-as-pdf",
    "title": "Comparison of Approaches to Carbon Pricing",
    "section": "View 7-page memo as PDF",
    "text": "View 7-page memo as PDF\n\n\n\nOur World In Data, CC BY-SA 4.0"
  },
  {
    "objectID": "writing/env-health/env-health.html#view-8-page-essay-as-pdf",
    "href": "writing/env-health/env-health.html#view-8-page-essay-as-pdf",
    "title": "Ecology and Human Health in California’s Central Valley: Historical Perceptions of a Complex Relationship",
    "section": "View 8-page essay as PDF",
    "text": "View 8-page essay as PDF\n\n\n\nCristiano Tomás, CC BY-SA 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "writing/evs/ev.html#view-6-page-paper-as-pdf",
    "href": "writing/evs/ev.html#view-6-page-paper-as-pdf",
    "title": "Economic Analysis of EV Tax Credits from the Inflation Reduction Act",
    "section": "View 6-page paper as PDF",
    "text": "View 6-page paper as PDF\n\n\n\ndronepicr, CC BY 2.0, via Wikimedia Commons"
  },
  {
    "objectID": "writing/lawn-ethics/lawn-ethics.html#view-11-page-essay-as-pdf",
    "href": "writing/lawn-ethics/lawn-ethics.html#view-11-page-essay-as-pdf",
    "title": "The Ethics of Lawn Mowing",
    "section": "View 11-page essay as PDF",
    "text": "View 11-page essay as PDF\n\n\n\nMoofpocket, CC BY-SA 3.0, via Wikimedia Commons"
  },
  {
    "objectID": "writing/wildfire/wildfire.html#view-10-page-paper-as-pdf",
    "href": "writing/wildfire/wildfire.html#view-10-page-paper-as-pdf",
    "title": "The Environmental, Economic, and Social Impacts of California’s Wildfires",
    "section": "View 10-page paper as PDF",
    "text": "View 10-page paper as PDF\n\n\n\nVentura County Fire Department Public Information Officer, Public Domain, via Wikimedia Commons"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html",
    "href": "blog/2024-6-7-post/index.html",
    "title": "Global sensitivity analysis with an atmospheric conductance model",
    "section": "",
    "text": "# load libraries\nlibrary(sensitivity)\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#setup",
    "href": "blog/2024-6-7-post/index.html#setup",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Setup",
    "text": "Setup\n\n# load libraries\nlibrary(sensitivity)\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#source-atmospheric-conductance-function",
    "href": "blog/2024-6-7-post/index.html#source-atmospheric-conductance-function",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Source atmospheric conductance function",
    "text": "Source atmospheric conductance function\nFor the purposes of showing the complete documentation for the function that I am working with, I have included the full contents of the R script that was sourced when I originally conducted this analysis.\n\n#' Compute Atmospheric Conductance\n#'\n#' This function atmospheric conductance as a function of windspeed, and vegetation cahracteristics\n#' @param       v windspeed (m/s)\n#' @param      height vegetation height (m)\n#' @param       zm measurement height of wind (m) (default 2m)\n#' @param      k_o scalar for roughness (default 0.1)\n#' @param      k_d scalar for zero plane displacement (default 0.7)\n#' @author Naomi\n#'\n#' @return  Conductance (mm/s)\n\nCatm = function(v, height, k_o=0.1, k_d=0.7) {\n\n    zm_add = 2\n  \n    zd = k_d*height\n    zo = k_o*height\n\n    zm = height+zm_add\n\n    zd = ifelse(zd &lt; 0, 0, zd)\n    Ca = ifelse(zo &gt; 0,\n     v / (6.25*log((zm-zd)/zo)**2),0)\n\n# convert to mm\n    Ca = Ca*1000\n   return(Ca)\n}"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#generate-inputparameter-values",
    "href": "blog/2024-6-7-post/index.html#generate-inputparameter-values",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Generate input/parameter values",
    "text": "Generate input/parameter values\n\nset.seed(123)\n\n# generate input/parameter samples\nnp &lt;- 1000\nk_o &lt;- rnorm(np, mean=0.1, sd=0.1*0.1)\nk_d &lt;- rnorm(np, mean=0.7, sd=0.7*0.1)\nv &lt;- rnorm(np, mean=300, sd=50)\nheight &lt;- runif(np, min=3.5, max=5.5)\n\nX1 &lt;- cbind.data.frame(k_o, k_d, v, height=height)\n\n# generate another set of input/parameter samples\nk_o &lt;- rnorm(np, mean=0.1, sd=0.1*0.1)\nk_d &lt;- rnorm(np, mean=0.7, sd=0.7*0.1)\nv &lt;- rnorm(np, mean=300, sd=50)\nheight &lt;- runif(np, min=3.5, max=5.5)\n\nX2 &lt;- cbind.data.frame(k_o, k_d, v, height=height)"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#run-atmospheric-conductance-model",
    "href": "blog/2024-6-7-post/index.html#run-atmospheric-conductance-model",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Run atmospheric conductance model",
    "text": "Run atmospheric conductance model\n\n# use Jansen approach implemented by sobolSalt\nsens_Catm_Sobol &lt;- sobolSalt(model = NULL, X1, X2, nboot = 100)\n\n# define input/parameters\nparms &lt;- as.data.frame(sens_Catm_Sobol$X)\ncolnames(parms) &lt;- colnames(X1)\n\n# run the model\nres = pmap_dbl(parms, Catm)\n\n# update the sensitivity analysis object with the results\nsens_Catm_Sobol &lt;- sensitivity::tell(sens_Catm_Sobol, res, res.names = \"ga\")"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#plot-conductance-estimates",
    "href": "blog/2024-6-7-post/index.html#plot-conductance-estimates",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Plot conductance estimates",
    "text": "Plot conductance estimates\n\n# plot distribution of conductance estimates\nggplot(data = data.frame(Ca = res), aes(x = Ca)) +\n  geom_density(fill=\"cornflowerblue\", alpha=0.5) +\n  labs(title = \"Conductance Estimates\", x = \"Conductance (mm/s)\", y = \"Density\")"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#plot-total-sensitivity-indices-of-all-inputsparameters",
    "href": "blog/2024-6-7-post/index.html#plot-total-sensitivity-indices-of-all-inputsparameters",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Plot total sensitivity indices of all inputs/parameters",
    "text": "Plot total sensitivity indices of all inputs/parameters\n\n# calculate total sensitivity indices\ntotal_sensitivity &lt;- apply(sens_Catm_Sobol$T, 1, mean)\n\n# plot total sensitivity indices to identify second most influential parameter\nsensitivity_df &lt;- data.frame(\n  Parameters = colnames(X1),\n  TotalEffect = total_sensitivity\n)\nggplot(sensitivity_df, aes(x = Parameters, y = TotalEffect)) +\n  geom_bar(stat = \"identity\", fill = \"darkseagreen\") +\n  labs(title = \"Total Sensitivity Indices\", y = \"Total Effect\", x = \"Parameters\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#plot-conductance-against-two-most-sensitive-inputsparameters",
    "href": "blog/2024-6-7-post/index.html#plot-conductance-against-two-most-sensitive-inputsparameters",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Plot conductance against two most sensitive inputs/parameters",
    "text": "Plot conductance against two most sensitive inputs/parameters\n\n# plot two most sensitive inputs/parameters\nboth = cbind.data.frame(parms, gs=sens_Catm_Sobol$y)\nggplot(both, aes(v,gs, col=k_o))+geom_point()+labs(y=\"Conductance (mm/s)\", x=\"Windspeed (cm/s)\", title=\"Global Sensitivity Analysis: Impact of Windspeed and k_o on Conductance\")"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#estimate-sobol-indices",
    "href": "blog/2024-6-7-post/index.html#estimate-sobol-indices",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Estimate Sobol indices",
    "text": "Estimate Sobol indices\n\n# add parameter names to main effect and total sensitivity indices\nrow.names(sens_Catm_Sobol$S) = colnames(parms)\nrow.names(sens_Catm_Sobol$T) = colnames(parms)\n\n# print all sobol indices\nprint(sens_Catm_Sobol)\n\n\nCall:\nsobolSalt(model = NULL, X1 = X1, X2 = X2, nboot = 100)\n\nModel runs: 6000 \n\nModel variance: 8417654 \n\nFirst order indices:\n        original          bias std. error  min. c.i. max. c.i.\nk_o    0.2093548  0.0002043903 0.03498043 0.14767984 0.2931129\nk_d    0.1993786  0.0042697793 0.03682199 0.12276724 0.2778684\nv      0.4753836 -0.0016100511 0.02071453 0.44440102 0.5241987\nheight 0.1043329  0.0003478314 0.03216599 0.03546999 0.1750344\n\nTotal indices:\n        original          bias  std. error min. c.i. max. c.i.\nk_o    0.1690400  0.0004720644 0.010837170 0.1477810 0.1895922\nk_d    0.1646352  0.0005778152 0.010384375 0.1446359 0.1836950\nv      0.5199054 -0.0007921543 0.027909766 0.4669257 0.5705891\nheight 0.1276424  0.0003586299 0.007147771 0.1114200 0.1406503"
  },
  {
    "objectID": "blog/2024-6-7-post/index.html#conclusion",
    "href": "blog/2024-6-7-post/index.html#conclusion",
    "title": "Global Sensitivity Analysis for an Atmospheric Conductance Model",
    "section": "Conclusion",
    "text": "Conclusion\nIn this setting (compared to the setting looked at in class), windspeed was higher and less variable, while vegetation was shorter. Comparing the two sensitivity analyses, we can see that these differences in conditions was associated with a decrease in model variance, as the model variance determined here (~8.4 million) was less than half the variance we found in class (~19.7 million). In addition, for the model we look at here, the total sensitivity index for k_d is significantly lower, while that for k_o, windspeed, and height are significantly higher. Because the total sensitivity index for windspeed is significantly higher, this tells us that atmospheric conductance is more sensitive to windspeed in areas where windspeed is high and less variable and vegetation is shorter. However, the lower model variance tells us that overall, atmospheric conductance has less variation in these conditions."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html",
    "href": "blog/2024-6-20-post/index.html",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "",
    "text": "This Master’s capstone project was completed with the client Darn Tough Vermont and three classmates for my Master’s program at UC Santa Barbara."
  },
  {
    "objectID": "blog/2023-6-1-post/index.html",
    "href": "blog/2023-6-1-post/index.html",
    "title": "Spatial Analysis of Allometric Equations used for estimating Carbon Sequestration from Agroforestry Systems",
    "section": "",
    "text": "Figures 5-8 from final report. Maps on left show biomes where agroforestry studies were conducted, and maps on right display where allometric equations were originally developed."
  },
  {
    "objectID": "blog/2023-6-1-post/index.html#view-final-report-as-pdf",
    "href": "blog/2023-6-1-post/index.html#view-final-report-as-pdf",
    "title": "Spatial Analysis of Allometric Equations used for estimating Carbon Sequestration from Agroforestry Systems",
    "section": "",
    "text": "Figures 5-8 from final report. Maps on left show biomes where agroforestry studies were conducted, and maps on right display where allometric equations were originally developed."
  },
  {
    "objectID": "writing/offshore-wind/offshore-wind.html#view-slide-deck-as-pdf-used-for-30-minute-presentation",
    "href": "writing/offshore-wind/offshore-wind.html#view-slide-deck-as-pdf-used-for-30-minute-presentation",
    "title": "Policy Memo on U.S. Offshore Wind Development",
    "section": "View slide-deck as PDF (used for 30-minute presentation)",
    "text": "View slide-deck as PDF (used for 30-minute presentation)"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html",
    "href": "blog/2024-6-10-post/index.html",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "",
    "text": "This project was completed for my Modeling Environmental Systems class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate modeling techniques."
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#setup",
    "href": "blog/2024-6-10-post/index.html#setup",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Setup",
    "text": "Setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(deSolve)\nlibrary(sensitivity)"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#source-forest-growth-function",
    "href": "blog/2024-6-10-post/index.html#source-forest-growth-function",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Source forest growth function",
    "text": "Source forest growth function\nFor the purposes of showing the complete documentation for the function that I am working with, I have included the full contents of the R script that was sourced when I originally conducted this analysis.\n\n#' Forest growth derivative\n#' @param time time since start\n#' @param C forest size (carbon)\n#' @param parms - as list with four values: r, g, K, threshold\n#' @param r exponential growth rate before canopy closure\n#' @param g linear growth rate after canopy closure\n#' @param K carrying capacity\n#' @param threshold canopy closure threshold\n#' @return derivative of forest size with time\ndforestgrowth = function(Time, C, parms) {\n  if (C &lt; parms$threshold) {\n    dC = parms$r * C\n  } else {\n    dC = parms$g * (1 - C/parms$K)\n  }\n  return(list(dC))\n}"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#run-model-for-300-years-using-the-ode-solver",
    "href": "blog/2024-6-10-post/index.html#run-model-for-300-years-using-the-ode-solver",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Run model for 300 years (using the ODE solver)",
    "text": "Run model for 300 years (using the ODE solver)\n\n# set number of parameters\nnp = 2000\n\n# generate parameter sets for sensitivity analysis\nr_exp = rnorm(mean = 0.01, sd = 0.002, n = np)\ng_linear = rnorm(mean = 2, sd = 0.5, n = np)\nK_capacity = rnorm(mean = 250, sd = 50, n = np)\nthreshold_closure = rnorm(mean = 50, sd = 10, n = np)\n\nX1 = cbind.data.frame(r = r_exp, g = g_linear, K = K_capacity, threshold = threshold_closure)\n\nr_exp = rnorm(mean = 0.01, sd = 0.002, n = np)\ng_linear = rnorm(mean = 2, sd = 0.5, n = np)\nK_capacity = rnorm(mean = 250, sd = 50, n = np)\nthreshold_closure = rnorm(mean = 50, sd = 10, n = np)\n\nX2 = cbind.data.frame(r = r_exp, g = g_linear, K = K_capacity, threshold = threshold_closure)\n\n# fix negative values\nX1 = X1 %&gt;% map_df(pmax, 0.0)\nX2 = X2 %&gt;% map_df(pmax, 0.0)\n\n# create Sobol object and get parameter sets for running model\nsens_P = sobolSalt(model = NULL, X1, X2, nboot = 300)\n\n# add names to parameter sets\ncolnames(sens_P$X) = c(\"r\", \"g\", \"K\", \"threshold\")\n\n# set initial forest size and simulation times\nCinitial = 10\nsimtimes = seq(from = 1, to = 300)\n\n# run model with first parameter set\nparms = list(r = sens_P$X[1, \"r\"], g = sens_P$X[1, \"g\"], K = sens_P$X[1, \"K\"], threshold = sens_P$X[1, \"threshold\"])\n\nresult = ode(y = Cinitial, times = simtimes, func = dforestgrowth, parms = parms)\nresult = as.data.frame(result)\ncolnames(result) = c(\"time\", \"C\")\n\n# plot results\nggplot(result, aes(time, C)) +\n  geom_line() +\n  labs(x = \"Time (years)\", y = \"Forest Size (kg C)\", title = \"Forest Growth Trajectory\") +\n  theme_bw()"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#conduct-a-global-sensitivity-analysis-looking-at-variation-in-maximum-forest-size",
    "href": "blog/2024-6-10-post/index.html#conduct-a-global-sensitivity-analysis-looking-at-variation-in-maximum-forest-size",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Conduct a global sensitivity analysis looking at variation in maximum forest size",
    "text": "Conduct a global sensitivity analysis looking at variation in maximum forest size\n\n# function to compute maximum forest size\ncompute_max_forest_size &lt;- function(carbontime) {\n  max_size &lt;- max(carbontime$C)\n  return(list(max_size = max_size))\n}\n\n# set number of parameters\nnp &lt;- 2000\n\n# generate parameter sets for sensitivity analysis\nr_exp &lt;- rnorm(mean = 0.01, sd = 0.001, n = np)\ng_linear &lt;- rnorm(mean = 2, sd = 0.2, n = np)\nK_capacity &lt;- rnorm(mean = 250, sd = 25, n = np)\nthreshold_closure &lt;- rnorm(mean = 50, sd = 5, n = np)\n\nX1 &lt;- cbind.data.frame(r = r_exp, g = g_linear, K = K_capacity, threshold = threshold_closure)\n\nr_exp &lt;- rnorm(mean = 0.01, sd = 0.001, n = np)\ng_linear &lt;- rnorm(mean = 2, sd = 0.2, n = np)\nK_capacity &lt;- rnorm(mean = 250, sd = 25, n = np)\nthreshold_closure &lt;- rnorm(mean = 50, sd = 5, n = np)\n\nX2 &lt;- cbind.data.frame(r = r_exp, g = g_linear, K = K_capacity, threshold = threshold_closure)\n\n# create Sobol object and get parameter sets for running model\nsens_forest &lt;- sobolSalt(model = NULL, X1, X2, nboot = 300)\ncolnames(sens_forest$X) &lt;- c(\"r\", \"g\", \"K\", \"threshold\")\n\n# wrapper function to run model and compute maximum forest size\np_wrapper &lt;- function(r, g, K, threshold, Cinitial, simtimes, odefunc, metricfunc) {\n  parms &lt;- list(r = r, g = g, K = K, threshold = threshold)\n  result &lt;- ode(y = Cinitial, times = simtimes, func = odefunc, parms = parms)\n  result &lt;- as.data.frame(result)\n  colnames(result) &lt;- c(\"time\", \"C\")\n  metrics &lt;- metricfunc(result)\n  return(metrics)\n}\n\n# set initial forest size and simulation times\nCinitial &lt;- 10\nsimtimes &lt;- seq(from = 1, to = 300)\n\n# run model for all parameter sets and compute maximum forest size\nallresults &lt;- as.data.frame(sens_forest$X) %&gt;%\n  pmap(p_wrapper, Cinitial = Cinitial, simtimes = simtimes, odefunc = dforestgrowth, metricfunc = compute_max_forest_size)\n\n# extract maximum forest size results into a data frame\nallres &lt;- allresults %&gt;% map_dfr(`[`, \"max_size\")\ncolnames(allres) &lt;- \"max_size\"\n\n# create sensitivity analysis box plot\nggplot(allres, aes(x = \"All Parameter Sets\", y = max_size)) +\n  geom_boxplot(color = \"black\") +\n  labs(x = NULL, y = \"Maximum Forest Size (kg C)\", title = \"Global Sensitivity Analysis: Impact of Varying Parameters on Maximum Forest Size\") +\n  theme_minimal()"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#compute-sobol-indices",
    "href": "blog/2024-6-10-post/index.html#compute-sobol-indices",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Compute Sobol indices",
    "text": "Compute Sobol indices\n\n# perform sensitivity analysis\nsens_result &lt;- tell(sens_forest, allres$max_size)\n\n# label rows in the Sobol indices\nrownames(sens_result$S) &lt;- c(\"r\", \"g\", \"K\", \"threshold\")\nrownames(sens_result$T) &lt;- c(\"r\", \"g\", \"K\", \"threshold\")\n\n# print Sobol indices\nprint(sens_result)\n\n\nCall:\nsobolSalt(model = NULL, X1 = X1, X2 = X2, nboot = 300)\n\nModel runs: 12000 \n\nModel variance: 252.2042 \n\nFirst order indices:\n            original          bias std. error min. c.i. max. c.i.\nr         0.39571046 -0.0002746905 0.02060599 0.3577022 0.4402525\ng         0.22495867 -0.0004818521 0.02056884 0.1851049 0.2671306\nK         0.35035033  0.0016724858 0.02134235 0.3097060 0.3919492\nthreshold 0.07765435  0.0006529218 0.01934635 0.0409994 0.1155947\n\nTotal indices:\n            original          bias  std. error  min. c.i.  max. c.i.\nr         0.40089954 -0.0008001244 0.014722735 0.37442823 0.43175337\ng         0.23416898  0.0003114037 0.010707429 0.21334963 0.25548364\nK         0.35776005  0.0016638360 0.015336145 0.32677488 0.38549061\nthreshold 0.07044764  0.0005814836 0.003466147 0.06274054 0.07710709"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#conclusion",
    "href": "blog/2024-6-10-post/index.html#conclusion",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Conclusion",
    "text": "Conclusion\nThis sensitivity analysis suggests that the carrying capacity (K) and the pre-canopy closure growth rate (r) are the most influential parameters in determining the maximum forest size estimates. Variations in either of these parameters could lead to significant differences in the maximum size that a forest can reach. Because of this, it is crucial to have accurate estimates of all parameters, but especially K and r, so that predictions of the maximum forest size are accurate."
  },
  {
    "objectID": "blog/2024-1-10-post/index.html",
    "href": "blog/2024-1-10-post/index.html",
    "title": "Visualizing PM 2.5 concentrations in Maryland alongside demographic data",
    "section": "",
    "text": "For this data visualization assignment, I will explore the spatial link between Particulate Matter (PM) 2.5 and race demographics in Maryland. According to the EJScreen technical documentation, the PM 2.5 indicator reported in this database gauges potential exposure to particles that are 2.5 micrometers or less. Specifically, the EJScreen database includes data for the annual average concentration in the air, measured in micrograms per cubic meter, at designated locations across the United States.\nUnderstanding PM 2.5 concentration is important because this air pollutant has major effects on human health and the environment. Many of these harmful effects are detailed on the EPA website, which is cited in the EJScreen technical documentation. For example, there are serious health effects to the human heart and lungs that can directly result from the inhalation of PM 2.5. In terms of environmental effects, PM 2.5 can be carried long distances by the wind and settle on ground or water, negatively impacting the chemical composition of important ecosystems.\nMoreover, the EPA website provides information on major sources of PM 2.5 in the environment. Most PM 2.5 is not emitted directly from a source but instead is formed in the atmosphere following reactions between other pollutants like sulfur dioxide and nitrogen oxides, which are commonly emitted by automobiles, power plants, and industrial processes. Sources of PM 2.5 that are directly emitted into the air generally come from sources such as construction sites, unpaved roads, smokestacks, and fires.\nI chose to investigate the spatial link between PM 2.5 and race demographics in my home State of Maryland because I think it has the potential to highlight an example of the environmental disparities faced by people of color. I’ve heard that Prince George’s County, home to my alma mater the University of Maryland, has a great deal of air pollution as a result of being directly downwind of Washington DC. I also know from living in the DC area for over 10 years that this county has many more people of color compared to neighboring Montgomery County, which lies northwest (and thus upwind) of DC. I’m very curious to see whether if there is an environmental justice issue here to highlight and how pronounced this issue might be."
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#introduction",
    "href": "blog/2024-1-10-post/index.html#introduction",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Introduction",
    "text": "Introduction\nFor this project, I will explore the spatial link between Particulate Matter (PM) 2.5 and race demographics in Maryland. According to the EJScreen technical documentation, the PM 2.5 indicator reported in this database gauges potential exposure to particles that are 2.5 micrometers or less. Specifically, the EJScreen database includes data for the annual average concentration in the air, measured in micrograms per cubic meter, at designated locations across the United States.\nUnderstanding PM 2.5 concentration is important because this air pollutant has major effects on human health and the environment. Many of these harmful effects are detailed on the EPA website, which is cited in the EJScreen technical documentation. For example, there are serious health effects to the human heart and lungs that can directly result from the inhalation of PM 2.5. In terms of environmental effects, PM 2.5 can be carried long distances by the wind and settle on ground or water, negatively impacting the chemical composition of important ecosystems.\nMoreover, the EPA website provides information on major sources of PM 2.5 in the environment. Most PM 2.5 is not emitted directly from a source but instead is formed in the atmosphere following reactions between other pollutants like sulfur dioxide and nitrogen oxides, which are commonly emitted by automobiles, power plants, and industrial processes. Sources of PM 2.5 that are directly emitted into the air generally come from sources such as construction sites, unpaved roads, smokestacks, and fires.\nI chose to investigate the spatial link between PM 2.5 and race demographics in my home State of Maryland because I think it has the potential to highlight an example of the environmental disparities faced by people of color. I’ve heard that Prince George’s County, home to my alma mater the University of Maryland, has a great deal of air pollution as a result of being directly downwind of Washington DC. I also know from living in the DC area for over 10 years that this county has many more people of color compared to neighboring Montgomery County, which lies northwest (and thus upwind) of DC. I’m very curious to see whether if there is an environmental justice issue here to highlight and how pronounced this issue might be."
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#setup",
    "href": "blog/2024-1-10-post/index.html#setup",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n# load packages\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)"
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#import-data",
    "href": "blog/2024-1-10-post/index.html#import-data",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Import data",
    "text": "Import data\n\n\nCode\n# Read in gdb of EJScreen data at the Census Block Group level\nejscreen &lt;- st_read(here(\"data\", \"2024-1-10-post-data\", \"EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\"))\n\n\nReading layer `EJSCREEN_StatePctiles_with_AS_CNMI_GU_VI' from data source \n  `/Users/linusghanadan/Documents/MEDS/other/linusghanadan.github.io/data/2024-1-10-post-data/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 243021 features and 223 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -19951910 ymin: -1617130 xmax: 16259830 ymax: 11554350\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\n# Wrangle data for Maryland and variables being looked at\nejscreen &lt;- ejscreen %&gt;%\n  filter(ST_ABBREV == \"MD\") %&gt;% \n  filter(!is.na(PEOPCOLORPCT)) %&gt;%\n  filter(!is.na(PM25))"
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#wrangle-data",
    "href": "blog/2024-1-10-post/index.html#wrangle-data",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Wrangle data",
    "text": "Wrangle data\n\n\nCode\n# Find the average values for all variables within counties\nmaryland_counties &lt;- aggregate(ejscreen, by = list(ejscreen$CNTY_NAME), FUN = mean) %&gt;%\n  mutate(\"People of Color (%)\" = PEOPCOLORPCT * 100, # Change variable names for plotting\n         \"Mean PM 2.5 (ug/m^3)\" = PM25)\n\n# Filter for PG County\npg_county &lt;- ejscreen %&gt;%\n  filter(CNTY_NAME == \"Prince George's County\") %&gt;% \n    mutate(\"People of Color (%)\" = PEOPCOLORPCT * 100, # Change variable names for plotting\n         \"Mean PM 2.5 (ug/m^3)\" = PM25)"
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#create-visualization-for-maryland",
    "href": "blog/2024-1-10-post/index.html#create-visualization-for-maryland",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Create visualization for Maryland",
    "text": "Create visualization for Maryland\n\n\nCode\n# Create PM map of Maryland\npm_map &lt;- tm_shape(maryland_counties) +\n  tm_polygons(col = \"Mean PM 2.5 (ug/m^3)\", palette = \"Reds\",\n              breaks = seq(from=6.0, to=8.5, by=0.25)) +\n  tm_compass(type = \"4star\", position = c(\"left\", \"bottom\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.position = c(\"right\", \"top\")) +\n  tm_layout(legend.height = 0.5)\n\n# Create demographics map of Maryland\ndemographic_map &lt;- tm_shape(maryland_counties) +\n  tm_polygons(col = \"People of Color (%)\", palette = \"Blues\",\n          breaks = seq(from=0, to=100, by=10)) +\n  tm_compass(type = \"4star\", position = c(\"left\", \"bottom\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.position = c(\"right\", \"top\")) +\n  tm_layout(legend.height = 0.5)\n\n# Stack the maps\ntmap_arrange(pm_map, demographic_map, nrow=2)"
  },
  {
    "objectID": "blog/2024-1-10-post/index.html#create-visualization-for-prince-georges-county",
    "href": "blog/2024-1-10-post/index.html#create-visualization-for-prince-georges-county",
    "title": "Spatial Analysis of PM 2.5 and Race Demographics in Maryland",
    "section": "Create visualization for Prince George’s County",
    "text": "Create visualization for Prince George’s County\n\n\nCode\n# Create PM map of PG county\npm_map &lt;- tm_shape(pg_county) +\n  tm_fill(col = \"Mean PM 2.5 (ug/m^3)\", palette = \"Reds\",\n          breaks = seq(from=7.0, to=8.5, by=0.1)) +\n  tm_scale_bar(position = c(\"right\", \"top\")) +\n  tm_compass(type = \"4star\", position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.position = c(\"left\", \"center\"))\n\n# Create demographics map of PG county\ndemographic_map &lt;- tm_shape(pg_county) +\n  tm_fill(col = \"People of Color (%)\", palette = \"Blues\",\n          breaks = seq(from=0, to=100, by=5)) +\n  tm_scale_bar(position = c(\"right\", \"top\")) +\n  tm_compass(type = \"4star\", position = c(\"right\", \"bottom\")) +\n  tm_layout(legend.position = c(\"left\", \"center\"))\n\n# Stack the maps\ntmap_arrange(pm_map, demographic_map, nrow=2)"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html",
    "title": "Regression Models using Deep Learning to predict Phosphorus Concentration based on Time and Location",
    "section": "",
    "text": "In this blog post, I’ll build and compare two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample."
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#purpose",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#purpose",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Purpose",
    "text": "Purpose\nIn this blog post, I’ll build and compare two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample."
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#background",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#background",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Background",
    "text": "Background\nThis analysis will focus on phosphorus pollution. Phosphorus, along with nitrogen, are the two major pollutants responsible for algal blooms in the Chesapeake Bay. A 2022 study found that agricultural runoff was the largest source of nutrient pollution, accounting for 48% of nitrogen and 27% of phosphorus in the Chesapeake Bay (Chesapeake Progress), n.d.). Both phosphorus and nitrogen also get to the Bay as a result of urban and suburban runoff, wastewater treatment plants releasing treated water, and natural sources like runoff from forests and wetlands."
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#data",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#data",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Data",
    "text": "Data\nYearly water quality data on the Chesapeake Bay’s tidal and non-tidal regions going back to 1984 is publicly available on the Chesapeake Bay Program (CBP) DataHub (Chesapeake Bay Program DataHub, n.d.).\nFor my analysis, I will be using 2010 to 2019 data collected at 143 different monitoring stations positioned throughout the Chesapeake Bay tidal regions, which includes the mainstem Bay and tributary components. Across the 10 years that we are looking at, we’ll have a total of 43,590 phosphorus observations."
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#data-import",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#data-import",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Data import",
    "text": "Data import\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport requests\nfrom io import BytesIO\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nfrom tensorflow import keras\n\n\n\n\nCode\n# Create a list of data URLs\nexcel_urls = [\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2019_CEDR_tidal_data_01jun21.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2018_CEDR_tidal_data_01jun21.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2017_CEDR_tidal_data_11oct18.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2016_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2015_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2014_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2013_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2012_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2011_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2010_CEDR_tidal_data_15jun17.xlsx'\n]\n\n# Create an empty list to store data frames\ndfs = []\n\n# Loop through each URL, read the Excel file directly into pandas, and append to list of data frames\nfor url in excel_urls:\n    # Get the content of the Excel file\n    response = requests.get(url)\n    \n    # Read the Excel file directly from the content\n    wq_data = pd.read_excel(BytesIO(response.content), sheet_name=0)\n    dfs.append(wq_data)\n\n# Combine all data frames into a single data frame\nwq_data_combined = pd.concat(dfs, ignore_index=True)"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#data-exploration",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#data-exploration",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Data exploration",
    "text": "Data exploration\n\n\nCode\n# Wrangle data for relevant column variables, and filter for TP (total phosphorus)\nphos_data = wq_data_combined[[\"SampleDate\", \"Parameter\", \"MeasureValue\", \"Latitude\", \"Longitude\"]]\nphos_data = phos_data[phos_data[\"Parameter\"] == \"TP\"]\n\nphos_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 43590 entries, 60 to 2484391\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   SampleDate    43590 non-null  datetime64[ns]\n 1   Parameter     43590 non-null  object        \n 2   MeasureValue  43590 non-null  float64       \n 3   Latitude      43590 non-null  float64       \n 4   Longitude     43590 non-null  float64       \ndtypes: datetime64[ns](1), float64(3), object(1)\nmemory usage: 2.0+ MB\n\n\n\n\nCode\n# Initialize figure\nplt.figure(figsize=(12, 6))\n\n# Plot phosphorus data\nplt.plot(phos_data['SampleDate'], phos_data['MeasureValue'], label='Total Phosphorus (TP)', color='darkred', alpha=0.7)\n\n# Customize the plot\nplt.title('Concentration of Phosphorus Samples (2010-2019)')\nplt.xlabel('Year')\nplt.ylabel('Concentration (mg/L)')\n\n# Rotate and align tick labels\nplt.gcf().autofmt_xdate()\n\n# Use tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Resample to monthly frequency and calculate the mean\nmonthly_avg = phos_data.resample('M').mean()\n\n# Initialize figure\nplt.figure(figsize=(12, 6))\n\n# Plot monthly average phosphorus data\nplt.plot(monthly_avg.index, monthly_avg['MeasureValue'], color='darkred', alpha=0.7)\n\n# Customize the plot\nplt.title('Monthly Average Concentration of Phosphorus Samples (2010-2019)')\nplt.xlabel('Year')\nplt.ylabel('Monthly Average Concentration (mg/L)')\n\n# Rotate and align tick labels\nplt.gcf().autofmt_xdate()\n\n# Use tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Bin latitude and longitude into groups\nphos_data_copy = pd.DataFrame(phos_data)\nphos_data_copy['Latitude Group'] = pd.cut(phos_data['Latitude'], bins=10)\nphos_data_copy['Longitude Group'] = pd.cut(phos_data['Longitude'], bins=10)\n\n# Pivot the data to create a heatmap\nheatmap_data = phos_data_copy.pivot_table(index='Latitude Group', columns='Longitude Group', values='MeasureValue', aggfunc='mean')\n\n# Initialize the figure\nplt.figure(figsize=(10, 8))\n\n# Set style\nplt.style.use('seaborn-whitegrid')  # You can choose 'seaborn-dark', 'seaborn-notebook', or any other style\n\n# Plot heatmap\nsns.heatmap(heatmap_data, cmap='Reds')\n\n# Customize the plot\nplt.title('Heatmap of Mean Phosphorus Concentration (mg/L) by Location')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#data-pre-processing",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#data-pre-processing",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\n\nCode\n# Create numerical feature for days since start date\nphos_data['Days'] = (phos_data['SampleDate'] - phos_data['SampleDate'].min()).dt.days\n\n# Prepare features and target\nX = phos_data[['Days', 'Latitude', 'Longitude']]\ny = phos_data['MeasureValue']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape input for RNN (samples, time steps, features)\nX_train_rnn = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\nX_test_rnn = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#basic-rnn-model",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#basic-rnn-model",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Basic RNN model",
    "text": "Basic RNN model\nA Recurrent Neural Network (RNN) is a complex Artificial Neural Network (ANN) architecture that can be applied to sequence prediction for classification and regression tasks (in addition, they are often applied to sequence prediction for text data in generative modeling). Specifically, I will use the Long Short-Term Memory (LSTM) network, which many experts consider to be the most useful version of RNNs.\n\n\nCode\ndef create_rnn_model(input_shape):\n    model = keras.Sequential([\n        keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n        keras.layers.LSTM(64),\n        keras.layers.Dense(1)\n    ])\n    return model\n\n# Create the RNN model\nrnn_model = create_rnn_model((1, 3))  # (time steps, features)\n\n# Compile the model\nrnn_model.compile(optimizer='adam', loss='mse')\n\n# Print model summary\nrnn_model.summary()\n\n\nModel: \"sequential_37\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_84 (LSTM)              (None, 1, 64)             17408     \n                                                                 \n lstm_85 (LSTM)              (None, 64)                33024     \n                                                                 \n dense_92 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 50,497\nTrainable params: 50,497\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nCode\n# Train the model\nhistory = rnn_model.fit(\n    X_train_rnn, y_train,\n    epochs=100,\n    batch_size=32,\n    verbose=0,\n    validation_split=0.2,\n    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n\n\n\n\nCode\n# Make predictions\nrnn_predictions = rnn_model.predict(X_test_rnn).flatten()\n\n# Calculate error metrics\nrnn_mae = mean_absolute_error(y_test, rnn_predictions)\nrnn_mse = mean_squared_error(y_test, rnn_predictions)\nrnn_rmse = np.sqrt(rnn_mse)\n\n\n273/273 [==============================] - 3s 4ms/step"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#hybrid-rnn-mlp-model",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#hybrid-rnn-mlp-model",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Hybrid RNN-MLP model",
    "text": "Hybrid RNN-MLP model\nA Multilayer Perceptron (MLP) is a simple and flexible ANN architecture that can be applied to an extremely wide range of predictive classification and regression tasks involving tabular data. A simple MLP for regression tasks could just include two hidden layers: a dense layer with a specialized activation function (e.g., ReLU function), followed by a one-neuron dense layer.\nThe goal of our analysis is to assess the value of combining a RNN with a MLP for the task of predicting phosphorus concentration, so to add on to our RNN model, we will just add a very simple 16-neuron dense layer with the ReLU activation function before our one-neuron dense layer.\n\n\nCode\ndef create_hybrid_model(input_shape):\n    model = keras.Sequential([\n        keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n        keras.layers.LSTM(64),\n        keras.layers.Dense(16, activation='relu'),\n        keras.layers.Dense(1)\n    ])\n    return model\n\n# Create model\nhybrid_model = create_hybrid_model((1, 3))  # (time steps, features)\n\n# Compile model\nhybrid_model.compile(optimizer='adam', loss='mse')\n\n# Print model summary\nhybrid_model.summary()\n\n\nModel: \"sequential_38\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_86 (LSTM)              (None, 1, 64)             17408     \n                                                                 \n lstm_87 (LSTM)              (None, 64)                33024     \n                                                                 \n dense_93 (Dense)            (None, 16)                1040      \n                                                                 \n dense_94 (Dense)            (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 51,489\nTrainable params: 51,489\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nCode\n# Train the model\nhistory = hybrid_model.fit(\n    X_train_rnn, y_train,\n    epochs=100,\n    batch_size=32,\n    verbose=0,\n    validation_split=0.2,\n    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n\n\n\n\nCode\n# Make predictions\nhybrid_predictions = hybrid_model.predict(X_test_rnn).flatten()\n\n# Calculate error metrics\nhybrid_mae = mean_absolute_error(y_test, hybrid_predictions)\nhybrid_mse = mean_squared_error(y_test, hybrid_predictions)\nhybrid_rmse = np.sqrt(hybrid_mse)\nhybrid_mae\n\n\n273/273 [==============================] - 2s 4ms/step"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#model-comparison",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#model-comparison",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Model comparison",
    "text": "Model comparison\n\n\nCode\n# Create figure and subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Plot for the first model (left subplot)\nax1 = axes[0]\n\n# Compare MAE\nbars1 = ax1.bar(['RNN Model', 'RNN-MLP Model'], [rnn_mae, hybrid_mae])\nax1.set_title('MAE Comparison')\n\n# Add text annotations for MAE\nfor bar in bars1:\n    yval = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()*3/7, yval, round(yval, 4), va='bottom')  # va='bottom' aligns text at the bottom of the bar\n\n# Plot for the second model (right subplot)\nax2 = axes[1]\n\n# Compare RMSE\nbars2 = ax2.bar(['RNN Model', 'RNN-MLP Model'], [rnn_rmse, hybrid_rmse])\nax2.set_title('RMSE Comparison')\n\n# Add text annotations for RMSE\nfor bar in bars2:\n    yval = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()*3/7, yval, round(yval, 4), va='bottom')  # va='bottom' aligns text at the bottom of the bar\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nCode\n# Create figure and subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Plot for the first model (left subplot)\nax1 = axes[0]\n\n# Add a line y = x\nx = np.linspace(0, 1.4, 100)\nax1.plot(x, x, color='black', linestyle='--', linewidth=1)\n\n# Scatter plot for Basic RNN model predictions\nax1.scatter(y_test, rnn_predictions, color='blue', alpha=0.5, label='RNN')\n\n# Set axis labels and title\nax1.set_xlabel('Actual Values')\nax1.set_ylabel('Predicted Values')\nax1.set_title('Actual vs Predicted (RNN)')\n\n# Set axis limits\nax1.set_xlim(0, 0.5)\nax1.set_ylim(0, 0.5)\n\n# Plot for the second model (right subplot)\nax2 = axes[1]\n\n# Add a line y = x\nax2.plot(x, x, color='black', linestyle='--', linewidth=1)\n\n# Scatter plot for Hybrid model predictions\nax2.scatter(y_test, hybrid_predictions, color='blue', alpha=0.5, label='RNN-MLP')\n\n# Set axis labels and title\nax2.set_xlabel('Actual Values')\nax2.set_ylabel('Predicted Values')\nax2.set_title('Actual vs Predicted (RNN-MLP)')\n\n# Set axis limits\nax2.set_xlim(0, 0.5)\nax2.set_ylim(0, 0.5)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "blog/2024-7-16-post/ml_chesapeakebay.html#conclusion",
    "href": "blog/2024-7-16-post/ml_chesapeakebay.html#conclusion",
    "title": "Regression Models to predict Phosphorus Concentration based on Time and Location",
    "section": "Conclusion",
    "text": "Conclusion\nFrom adding an additional 16-neuron dense layer with a ReLU activation function to our LSTM network architecture, we saw a 3.8% decrease in MAE and a 1.6% decrease in RMSE, suggesting that there are potential advantages from using a hybrid RNN-MLP model in this context. Looking at side-by-side scatter plots of actual values and predicted values, it appears the reductions in MAE and RMSE can be attributed to enhanced outlier detection among actual concentration values greater than 0.15 mg/L.\nFurther analysis would be helpful to see how employing cross-validation to tune for number of ReLU layers and number of neurons per layer (in LSTM layers and ReLU layers) might impact these results. Unfortunetly, I did not have access to an accelerator for this project, so I was unable to conduct an analysis with that level of granularity."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html",
    "href": "blog/2024-7-24-post/index.html",
    "title": "Visualization Portfolio (R)",
    "section": "",
    "text": "Context for plots: As a final project for my Statistics class, I conducted a 2010-2019 time series analysis of nutrient concentrations in the Chesapeake Bay. I used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 different monitoring stations positioned throughout the Bay’s tidal regions. Specifically, I constructed two STL decomposition models (one for nitrogen and one for phosphorus), which is a statistical model that separates data into seasonal trend, non-seasonal trend, and random components. Since 2010 marked the beginning of federal water quality requirements under the Clean Water Act, the goal of my analysis was mainly to identify any overarching non-seasonal trends in nitrogen and phosphorus levels over the ten years, but I was also interested in understanding the extent to which seasonal trends and randomness contributed to the variation in monthly average concentrations.\n\nCaption: The non-seasonal trend component (red) indicates that there was a negligible downward trend (95% CI: -0.28% to -0.05%) over the span of the ten years. Interestingly, this component also tells us that the increase in concentrations seen over the course of 2018, as well as the subsequent decrease seen during 2019, were both indicative of non-seasonal trends. Furthermore, there is a distinct seasonal trend component (green) to nitrogen concentrations in the Chesapeake Bay tidal regions (explains 55% of total variation). Each year, concentrations increased sharply around December. They then peaked around February to March, before decreasing substantially and reaching their minimum around July. Lastly, looking at the random component (blue), this analysis tells us that the spike at the beginning of 2014 was likely due to an isolated event and not indicative of a trend in nitrogen concentrations, as our model classified this spike as attributable to randomness.\n\nCaption: The non-seasonal trend component (red) indicates a slightly more pronounced downward trend (95% CI: -0.52% to -0.38%) in comparison to nitrogen. Compared to nitrogen, there was also a more distinct seasonal trend component (green) for phosphorus concentrations (explains 76% of total variation). Unlike nitrogen, phosphorus concentrations shot up in the middle of the year around May, had a relatively flat peak lasting from June to August, and then shot back down at the end of the Summer."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#visualizations-using-r-ggplot2-and-tmap-packages",
    "href": "blog/2024-7-24-post/index.html#visualizations-using-r-ggplot2-and-tmap-packages",
    "title": "Highlighted Data Visualizations",
    "section": "",
    "text": "Time series analysis visualization for final project in my Statistics class. Each year, nitrogen concentrations increase sharply around December. They then peak around February to March, before decreasing substantially and reaching their minimum around July. Overall change in nitrogen levels over the ten years, adjusting for seasonal and random components, appears to be negligible.\n\n\n\n\n\nTreemap for final project in my Data Visualization class. Energy makes up 55% of global emissions, about half of which comes from the China, Russia, the U.S., Iran, and India. Agriculture makes up 29% of global emissions, waste makes up 14%, and 2% are from other sources.\n\n\n\n\n\nScatterplot for final project in my Data Visualization class. In terms of emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the amount emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. The highest per-capita emissions among the highlighted countries were in Australia and Russia, with ~300 tons per person.\n\n\n\n\n\nScreenshot of the scenario analysis tool (with fake data)"
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#visualizations-using-python-matplotlib-and-seaborn-packages",
    "href": "blog/2024-7-24-post/index.html#visualizations-using-python-matplotlib-and-seaborn-packages",
    "title": "Highlighted Data Visualizations",
    "section": "Visualizations using Python (matplotlib and seaborn packages)",
    "text": "Visualizations using Python (matplotlib and seaborn packages)"
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#time-series-analysis-visualization-for-final-project-in-my-statistics-class",
    "href": "blog/2024-7-24-post/index.html#time-series-analysis-visualization-for-final-project-in-my-statistics-class",
    "title": "Highlighted Data Visualizations",
    "section": "",
    "text": "Each year, nitrogen concentrations increase sharply around December. They then peak around February to March, before decreasing substantially and reaching their minimum around July. Overall change in nitrogen levels over the ten years, adjusting for seasonal and random components, appears to be negligible.\n\n\n\n\nTreemap for final project in my Data Visualization class. Energy makes up 55% of global emissions, about half of which comes from the China, Russia, the U.S., Iran, and India. Agriculture makes up 29% of global emissions, waste makes up 14%, and 2% are from other sources.\n\n\n\n\n\nScatterplot for final project in my Data Visualization class. In terms of emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the amount emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. The highest per-capita emissions among the highlighted countries were in Australia and Russia, with ~300 tons per person.\n\n\n\n\n\nScreenshot of the scenario analysis tool (with fake data)"
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#time-series-analysis-visualization-for-final-statistics-project",
    "href": "blog/2024-7-24-post/index.html#time-series-analysis-visualization-for-final-statistics-project",
    "title": "Highlighted Data Visualizations",
    "section": "",
    "text": "Caption: Each year, nitrogen concentrations increase sharply around December. They then peak around February to March, before decreasing substantially and reaching their minimum around July. When adjusted for seasonal and random components, there appears to be a positive trend over the course of 2018 and a negative trend over the course of 2019."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#treemap-for-final-data-visualization-project",
    "href": "blog/2024-7-24-post/index.html#treemap-for-final-data-visualization-project",
    "title": "Highlighted Data Visualizations",
    "section": "Treemap for final Data Visualization project",
    "text": "Treemap for final Data Visualization project\nCaption: The energy sector made up 55% of global anthropocentric methane emissions in 2021, about half of which came from China, Russia, the U.S., Iran, and India. Meanwhile, agriculture and waste contributed 29% and 14% of global emissions, with the remaining 2% coming from other sources."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#scatterplot-for-final-data-visualization-project",
    "href": "blog/2024-7-24-post/index.html#scatterplot-for-final-data-visualization-project",
    "title": "Highlighted Data Visualizations",
    "section": "Scatterplot for final Data Visualization project",
    "text": "Scatterplot for final Data Visualization project\nCaption: In terms of 2021 anthropogenic methane emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the amount emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. The highest per-capita emissions among the highlighted countries were in Australia and Russia, with ~300 tons per person.\n[]](plot-2.jpeg)"
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#interactive-sunburst-plot-for-capstone-project",
    "href": "blog/2024-7-24-post/index.html#interactive-sunburst-plot-for-capstone-project",
    "title": "Highlighted Data Visualizations",
    "section": "Interactive sunburst plot for Capstone Project",
    "text": "Interactive sunburst plot for Capstone Project\nContext: As part of the interactive application that my capstone group made for the clothing manufacturer Darn Tough Vermont, we included this interactive sunburst plot for visualizing their greenhouse gas emissions for a given year in terms of scope, category, and input variable. Through clicking on a specific segment of the plot, the application user can access more granular data making up the selected segment. In addition, hovering allows the user to access the exact values for tons of emissions and percentage of total emissions."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#time-series-visual-for-statistics-final-project",
    "href": "blog/2024-7-24-post/index.html#time-series-visual-for-statistics-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "",
    "text": "Context for plots: As a final project for my Statistics class, I conducted a 2010-2019 time series analysis of nutrient concentration in the Chesapeake Bay. I used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 different monitoring stations positioned throughout the Bay’s tidal regions. Specifically, I constructed two STL decomposition models (one for nitrogen and one for phosphorus), which is a statistical model that separates data into seasonal trend, non-seasonal trend, and random components. Since 2010 marked the beginning of federal water quality requirements under the Clean Water Act, the goal of my analysis was mainly to identify any overarching non-seasonal trends in nitrogen and phosphorus levels over the ten years, but I was also interested in understanding the extent to which seasonal trends and randomness contributed to the variation in monthly average concentrations.\n\nCaption: The non-seasonal trend component (red) indicates that there was a negligible downward trend (95% CI: -0.28% to -0.05%) over the span of the ten years. Interestingly, this component also tells us that the increase in concentrations seen over the course of 2018, as well as the subsequent decrease seen during 2019, were both indicative of non-seasonal trends. Furthermore, there is a distinct seasonal trend component (green) to nitrogen concentrations in the Chesapeake Bay tidal regions (explains 55% of total variation). Each year, concentrations increased sharply around December. They then peaked around February to March, before decreasing substantially and reaching their minimum around July. Lastly, looking at the random component (blue), this analysis tells us that the spike at the beginning of 2014 was likely due to an isolated event and not indicative of a trend in nitrogen concentrations, as our model classified this spike as attributable to randomness.\n\nCaption: The non-seasonal trend component (red) indicates a slightly more pronounced downward trend (95% CI: -0.52% to -0.38%) in comparison to nitrogen. Compared to nitrogen, there was also a more distinct seasonal trend component (green) for phosphorus concentrations (explains 76% of total variation). Unlike nitrogen, phosphorus concentrations shot up in the middle of the year around May, had a relatively flat peak lasting from June to August, and then shot back down at the end of the Summer."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#treemap-for-data-visualization-final-project",
    "href": "blog/2024-7-24-post/index.html#treemap-for-data-visualization-final-project",
    "title": "Highlighted Data Visualizations",
    "section": "Treemap for Data Visualization Final Project",
    "text": "Treemap for Data Visualization Final Project\nContext for plot: This plot was part of an infographic that I made as a final project in my Data Visualization class. For my infographic, the overarching question was where anthropogenic methane emissions came from in 2021. This includes both the countries where emissions were occurring most frequently and also the human activities (e.g., energy production, agriculture, etc.) that contributed the most to these emissions.\nCaption: The energy sector made up 55% of global anthropocentric methane emissions in 2021, about half of which came from China, Russia, the U.S., Iran, and India. Meanwhile, agriculture and waste contributed 29% and 14% of global emissions, with the remaining 2% coming from other sources."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#scatterplot-for-data-visualization-final-project",
    "href": "blog/2024-7-24-post/index.html#scatterplot-for-data-visualization-final-project",
    "title": "Highlighted Data Visualizations",
    "section": "Scatterplot for Data Visualization Final Project",
    "text": "Scatterplot for Data Visualization Final Project\nContext for plot: This plot was part of an infographic that I made as a final project in my Data Visualization class. For my infographic, the overarching question was where anthropogenic methane emissions came from in 2021. This includes both the countries where emissions were occurring most frequently and also the human activities (e.g., energy production, agriculture, etc.) that contributed the most to these emissions.\nCaption: In terms of methane emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the ~50 tons emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. Among the highlighted countries, Australia and Canada had the highest emissions per person, at ~300 tons."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#interactive-sunburst-plot-for-masters-capstone-project",
    "href": "blog/2024-7-24-post/index.html#interactive-sunburst-plot-for-masters-capstone-project",
    "title": "Visualization Portfolio (R)",
    "section": "Interactive Sunburst Plot for Master’s Capstone Project",
    "text": "Interactive Sunburst Plot for Master’s Capstone Project\n\nNote: Plot is not interactive on this webpage (the image below is a screenshot from an application developed for Darn Tough Vermont)\nContext for plot: For my Master’s capstone project, I spent six months working with three classmates and the outdoor apparel manufacturer Darn Tough Vermont to develop a Microsoft Excel template and an interactive application. Together, these two products streamline Darn Tough’s workflow for carbon accounting and sustainability analysis. Specifically, the application allows for a user to upload their Excel template (containing input values) to calculate yearly greenhouse gas emissions, visualize historical data, and conduct scenario analysis based on adjustable input variables (e.g., compare scenarios for differing levels of wool procurement).\n\nCaption: The interactive sunburst plot allows users to analyze emissions in terms of Scope, Category (for Scope 3), and input variables (e.g., Wool Fiber). Through clicking on a segment of the plot, users can more easily see the granular data. In addition, the user can hover over a segment to access exact values for emissions and percent of total emissions (Note: Fake Data)."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#column-plot-for-data-visualization-final-project",
    "href": "blog/2024-7-24-post/index.html#column-plot-for-data-visualization-final-project",
    "title": "Highlighted Data Visualizations",
    "section": "Column plot for Data Visualization Final Project",
    "text": "Column plot for Data Visualization Final Project\nContext for plot: This plot was part of an infographic that I made as a final project in my Data Visualization class. For my infographic, the overarching question was where anthropogenic methane emissions came from in 2021. This includes both the countries where emissions were occurring most frequently and also the human activities (e.g., energy production, agriculture, etc.) that contributed the most to these emissions.\nCaption: As a percent of country-level methane emissions, Russia’s energy sector (85%) and Brazil’s agricultural sector (65%) stand out as particularly high. Meanwhile, energy-related emissions in the EU (29%) and Brazil (16%) make up a relatively low share of their total emissions, compared to about 60-70% in China, the U.S., Canada, and Australia."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#heatmap-for-geospatial-analysis-project-created-using-python",
    "href": "blog/2024-7-24-post/index.html#heatmap-for-geospatial-analysis-project-created-using-python",
    "title": "Highlighted Data Visualizations",
    "section": "Heatmap for Geospatial Analysis Project (created using Python)",
    "text": "Heatmap for Geospatial Analysis Project (created using Python)\nContext for plot: For a project in my Geospatial Analysis class, I retrieved raster data of Biodiversity Intactness Index (BII) from the Microsoft Planetary Computer catalog. This allowed me to calculate areas of interest based on changes in biodiversity and show my results on a heatmap.\nCaption: The edges of certain areas in Phoenix, which had relatively high BII, decrease from &gt;0.75 to &lt;0.75. In particular, this can be seen in the North East area of Phoenix, in addition to one spot in South Central Phoenix."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#heatmap-for-deep-learning-project-created-using-python",
    "href": "blog/2024-7-24-post/index.html#heatmap-for-deep-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations",
    "section": "Heatmap for Deep Learning Project (created using Python)",
    "text": "Heatmap for Deep Learning Project (created using Python)\nContextual Description: For this project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. As part of my data exploration, I created this heatmap using the Python packages matplotlib seaborn of phosphorus concentration based on latitude and longitude."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#scatterplots-for-deep-learning-project-created-using-python",
    "href": "blog/2024-7-24-post/index.html#scatterplots-for-deep-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations",
    "section": "Scatterplots for Deep Learning Project (created using Python)",
    "text": "Scatterplots for Deep Learning Project (created using Python)\nContext for plot: In this deep learning project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,809 nitrogen samples from 143 different monitoring stations across the tidal regions of the Chesapeake Bay.\nCaption: From adding an additional 16-neuron dense layer with a ReLU activation function to our LSTM network architecture, we saw a 3.8% decrease in MAE and a 1.6% decrease in RMSE, suggesting that there are potential advantages from using a hybrid RNN-MLP model in this context. From these scatterplots of actual values and predicted values, it appears the reductions in MAE and RMSE can be attributed to enhanced outlier detection among actual concentration values &gt;0.15 mg/L."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#heatmap-for-ensemble-learning-project-created-using-python",
    "href": "blog/2024-7-24-post/index.html#heatmap-for-ensemble-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations",
    "section": "Heatmap for Ensemble Learning Project (created using Python)",
    "text": "Heatmap for Ensemble Learning Project (created using Python)\nContext for plot: In this ensemble learning project, I built three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples off the coast of California. The features being used to make these predictions were other ocean chemistry measurements that were also measured during water sampling.\nCaption: Based on this correlation heatmap, the variables ‘Temperature_degC’ (correlation with ‘R_TEMP’ is 1) and ‘R_Nuts’ (correlation with ‘NH3uM’ is 1) were removed from the feature matrix due to high correlation with the target variable."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#treemap-for-data-visualization-communication-final-project",
    "href": "blog/2024-7-24-post/index.html#treemap-for-data-visualization-communication-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "Treemap for Data Visualization & Communication Final Project",
    "text": "Treemap for Data Visualization & Communication Final Project\nContext for plot: For a final project in my Data Visualization & Communication class, I created this plot as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\n\nCaption: The energy sector made up 55% of global anthropogenic methane emissions in 2021, about half of which came from China, Russia, the U.S., Iran, and India. Meanwhile, agriculture and waste contributed 29% and 14% of global emissions, with the remaining 2% coming from other sources."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#scatterplot-for-data-visualization-communication-final-project",
    "href": "blog/2024-7-24-post/index.html#scatterplot-for-data-visualization-communication-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "Scatterplot for Data Visualization & Communication Final Project",
    "text": "Scatterplot for Data Visualization & Communication Final Project\nContext for plot: For a final project in my Data Visualization & Communication class, I created this plot as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\n\nCaption: In terms of methane emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the ~50 tons emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. Among the highlighted countries, Australia and Canada had the highest emissions per person, at ~300 tons."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#bar-plot-for-data-visualization-communication-final-project",
    "href": "blog/2024-7-24-post/index.html#bar-plot-for-data-visualization-communication-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "Bar plot for Data Visualization & Communication Final Project",
    "text": "Bar plot for Data Visualization & Communication Final Project\nContext for plot: For a final project in my Data Visualization & Communication class, I created this plot as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\n\nCaption: As a percent of country-level methane emissions, Russia’s energy sector (85%) and Brazil’s agricultural sector (65%) stand out as particularly high. Meanwhile, energy-related emissions in the EU (29%) and Brazil (16%) make up a relatively low share of their total emissions, compared to about 60-70% in China, the U.S., Canada, and Australia."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#chloropleth-map-for-geospatial-analysis-project",
    "href": "blog/2024-7-24-post/index.html#chloropleth-map-for-geospatial-analysis-project",
    "title": "Visualization Portfolio (R)",
    "section": "Chloropleth map for Geospatial Analysis Project",
    "text": "Chloropleth map for Geospatial Analysis Project\nContext for plot: For a project in my Geospatial Analysis class, I used data from the NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Specifically, I looked at census tracts in the Houston metropolitan area where residential blackouts occurred and how this relates to median income of census tracts."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html",
    "href": "blog/2024-7-23-post/index.html",
    "title": "Visualization Portfolio (Python)",
    "section": "",
    "text": "Context for plot: For a project in my Geospatial Analysis class, I retrieved grid-cell data on Biodiversity Intactness Index (BII), a score rating an area of land’s biodiversity from 0 to 1, in the city of Phoenix, Arizona. The purpose of this analysis was to better understand 2020 biodiversity in Phoenix, as well as how biodiversity had changed since 2017.\n\nCaption: The North East area of Phoenix and South Central Phoenix had the highest BII in 2020. In addition, some locations at the outer edges of these areas declined from being highly biodiverse (BII&gt;0.75) in 2017 to being only moderately biodiverse (0.75&gt;BII&gt;0.5) in 2020 (these areas are shown in red)."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#time-series-visual-for-deep-learning-project-created-using-python",
    "href": "blog/2024-7-23-post/index.html#time-series-visual-for-deep-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations created using Python",
    "section": "",
    "text": "Context for plot: In this deep learning project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,809 samples taken over 10 years (2010-2019) from 143 different monitoring stations across the tidal regions of the Chesapeake Bay. Created as plot shown here shows monthly concentration over time, indicating distinct seasonal trends in the data.\nCaption: Data exploration plot tracing monthly phosphorus concentration over time. This time series visual indicates distinct seasonal trends, with average concentration significantly higher during months in the summer and early fall seasons."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#scatterplots-for-deep-learning-project-created-using-python",
    "href": "blog/2024-7-23-post/index.html#scatterplots-for-deep-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations created using Python",
    "section": "Scatterplots for Deep Learning Project (created using Python)",
    "text": "Scatterplots for Deep Learning Project (created using Python)\nContext for plot: In this deep learning project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,809 nitrogen samples taken over 10 years (2010-2019) from 143 different monitoring stations across the tidal regions of the Chesapeake Bay.\nCaption: From adding an additional 16-neuron dense layer with a ReLU activation function to our LSTM network architecture, we saw a 3.8% decrease in MAE and a 1.6% decrease in RMSE, suggesting that there are potential advantages from using a hybrid RNN-MLP model in this context. From these scatterplots of actual values and predicted values, it appears the reductions in MAE and RMSE can be attributed to enhanced outlier detection among actual concentration values &gt;0.15 mg/L."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#bar-plot-for-geospatial-analysis-project-created-using-python",
    "href": "blog/2024-7-23-post/index.html#bar-plot-for-geospatial-analysis-project-created-using-python",
    "title": "Highlighted Data Visualizations created using Python",
    "section": "Bar plot for Geospatial Analysis Project (created using Python)",
    "text": "Bar plot for Geospatial Analysis Project (created using Python)\nContext for plot: In this project from my Geospatial Analysis class, I extracted land cover statistics for the area surrounding Mount Whitney in California. My data came from a U.S. Geological Survey (USGS) raster dataset on land cover classification.\nCaption: Of any one class, Mediterranean California Alpine Bedrock and Scree takes up the most land, constituting just over 15% of the analysis area. California Central Valley and Southern Coastal Grassland, Sierra Nevada Subalpine Lodgepole Pine Forest and Woodland, and Mediterranean California Mesic Mixed Conifer Forest and Woodland each make up about 8-10% of the area."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#correlation-heatmap-for-ensemble-learning-project-created-using-python",
    "href": "blog/2024-7-23-post/index.html#correlation-heatmap-for-ensemble-learning-project-created-using-python",
    "title": "Highlighted Data Visualizations created using Python",
    "section": "Correlation Heatmap for Ensemble Learning Project (created using Python)",
    "text": "Correlation Heatmap for Ensemble Learning Project (created using Python)\nContext for plot: In this ensemble learning project, I built three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples off the coast of California. The features being used to make these predictions were other ocean chemistry measurements that were also measured during water sampling.\nCaption: Based on this correlation heatmap, the variables ‘Temperature_degC’ (correlation with ‘R_TEMP’ is 1) and ‘R_Nuts’ (correlation with ‘NH3uM’ is 1) were removed from the feature matrix due to high correlation with the target variable."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#heatmap-for-geospatial-analysis-project-created-using-python",
    "href": "blog/2024-7-23-post/index.html#heatmap-for-geospatial-analysis-project-created-using-python",
    "title": "Highlighted Data Visualizations created using Python",
    "section": "Heatmap for Geospatial Analysis Project (created using Python)",
    "text": "Heatmap for Geospatial Analysis Project (created using Python)\nContext for plot: For a project in my Geospatial Analysis class, I retrieved raster data of Biodiversity Intactness Index (BII) from the Microsoft Planetary Computer catalog. This allowed me to calculate areas of interest based on changes in biodiversity and show my results on a heatmap.\nCaption: The edges of certain areas in Phoenix, which had relatively high BII, decrease from &gt;0.75 to &lt;0.75. In particular, this can be seen in the North East area of Phoenix, in addition to one spot in South Central Phoenix."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#map-for-data-visualization-communication-final-project",
    "href": "blog/2024-7-24-post/index.html#map-for-data-visualization-communication-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "Map for Data Visualization & Communication Final Project",
    "text": "Map for Data Visualization & Communication Final Project\nContext for plot: For a final project in my Data Visualization & Communication class, I created this plot as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\nCaption: Six countries and the European Union accounted for nearly half (47%) of anthropogenic methane emissions in 2021, while only containing 33% of global population."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#time-series-visual-for-deep-learning-project",
    "href": "blog/2024-7-23-post/index.html#time-series-visual-for-deep-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Time series visual for Deep Learning Project",
    "text": "Time series visual for Deep Learning Project\nContext for plot: In this deep learning project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,590 samples taken over 10 years (2010-2019) from 143 different monitoring stations across the tidal regions of the Chesapeake Bay.\n\nCaption: Data exploration plot tracing monthly phosphorus concentration over time. This time series visual indicates distinct seasonal trends, with average concentration significantly higher during months in the summer and early fall seasons."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#scatterplots-for-deep-learning-project",
    "href": "blog/2024-7-23-post/index.html#scatterplots-for-deep-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Scatterplots for Deep Learning Project",
    "text": "Scatterplots for Deep Learning Project\nContext for plot: In this deep learning project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,809 nitrogen samples taken over 10 years (2010-2019) from 143 different monitoring stations across the tidal regions of the Chesapeake Bay.\n\nCaption: From adding an additional 16-neuron dense layer with a ReLU activation function to our LSTM network architecture, we saw a 3.8% decrease in MAE and a 1.6% decrease in RMSE, suggesting that there are potential advantages from using a hybrid RNN-MLP model in this context. From these scatterplots of actual values and predicted values, it appears the reductions in MAE and RMSE can be attributed to enhanced outlier detection among actual concentration values &gt;0.15 mg/L."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#bar-plot-for-ensemble-learning-project",
    "href": "blog/2024-7-23-post/index.html#bar-plot-for-ensemble-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Bar plot for Ensemble Learning Project",
    "text": "Bar plot for Ensemble Learning Project\nContext for plot: In this ensemble learning project, I built three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples off the coast of California. The features being used to make these predictions were other ocean chemistry measurements that were also measured during water sampling.\n\nCaption: The most important feature for predicting DIC in the random forest model was Sulfur trioxide (SiO3) concentration, with a feature importance over 0.7. This was significantly higher than the second most important feature, Phosphate (PO3) concentration, which had a feature importance of close to 0.2. The third and fourth most important features, reported Oxygen concentration and Nitrate (NO3) concentration, only had a features importance of about 0.03."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#correlation-heatmap-for-ensemble-learning-project",
    "href": "blog/2024-7-23-post/index.html#correlation-heatmap-for-ensemble-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Correlation Heatmap for Ensemble Learning Project",
    "text": "Correlation Heatmap for Ensemble Learning Project\nContext for plot: In this ensemble learning project, I built three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples off the coast of California. The features being used to make these predictions were other ocean chemistry measurements that were also measured during water sampling.\n\nCaption: Based on this correlation heatmap, the variables Temperature_degC and R_Nuts were removed from the feature matrix due to high correlation with the target variable."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#heatmap-for-geospatial-analysis-project",
    "href": "blog/2024-7-23-post/index.html#heatmap-for-geospatial-analysis-project",
    "title": "Visualization Portfolio (Python)",
    "section": "",
    "text": "Context for plot: For a project in my Geospatial Analysis class, I retrieved grid-cell data on Biodiversity Intactness Index (BII), a score rating an area of land’s biodiversity from 0 to 1, in the city of Phoenix, Arizona. The purpose of this analysis was to better understand 2020 biodiversity in Phoenix, as well as how biodiversity had changed since 2017.\n\nCaption: The North East area of Phoenix and South Central Phoenix had the highest BII in 2020. In addition, some locations at the outer edges of these areas declined from being highly biodiverse (BII&gt;0.75) in 2017 to being only moderately biodiverse (0.75&gt;BII&gt;0.5) in 2020 (these areas are shown in red)."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#purpose",
    "href": "blog/2024-1-20-post/index.html#purpose",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Purpose",
    "text": "Purpose\nIn February 2021, Texas faced an unprecedented power crisis that left millions without electricity as a result of three winter storms (Ramsey, 2021). Struggling to meet the extraordinary demand for heating amid freezing temperatures, the Electric Reliability Council of Texas (ERCOT) implemented widespread blackouts to prevent a total grid collapse. In addition to exposing the vulnerabilities of Texas’s energy infrastructure, the crisis also prompted a nationwide discussion on the resilience of power grids in the face of extreme weather events.\nThis analysis will look at where residential blackouts occurred during February 2021 in the Houston area. After mapping the blackout data onto census tracts, I’ll look at the median income of the census tracts that were affected by residential blackouts compared to those that were not affected."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#setup",
    "href": "blog/2024-1-20-post/index.html#setup",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Setup",
    "text": "Setup\n\n# general libraries\nlibrary(here)\nlibrary(tidyverse)\n\n# geospatial libraries\nlibrary(sf)\nlibrary(stars)\nlibrary(raster)\nlibrary(ggspatial)"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#data",
    "href": "blog/2024-1-20-post/index.html#data",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Data",
    "text": "Data\n\nBlackout TIF files\nData on blackouts comes from NASA’s Visible Infrared Imaging Radiometer Suite (VIIRS) instrument and were accessed from NASA’s website (NASA, n.d.). We will look at 2021-02-07 and 2021-02-16, as two days provide us with contrasting images to visualize the extent of the power outage in Texas. Houston lies on the border of two tiles (h08v06 and h08v05) that are measured by the VIIRS instrument, so we will download two tiles per date.\n\n# Read in and combine night lights data\n\n## Loading 02/07/2021 tiles as stars objects\nh08v05_lights1 &lt;- read_stars(here(\"data\", \"2024-1-20-post-data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nh08v06_lights1 &lt;- read_stars(here(\"data\", \"2024-1-20-post-data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\n\n## Loading 02/16/2021 tiles as stars objects\nh08v05_lights2 &lt;- read_stars(here(\"data\", \"2024-1-20-post-data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nh08v06_lights2 &lt;- read_stars(here(\"data\", \"2024-1-20-post-data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n## Combining tiles for each day\nlights1 &lt;- st_mosaic(c(h08v05_lights1, h08v06_lights1, along = \"y\"))\nlights2 &lt;- st_mosaic(c(h08v05_lights2, h08v06_lights2, along = \"y\"))\n\n\n# Check that tiles for day 1 were successfully combined\n## For x, value in \"to\" column should be the same in all three outputs\n## For y, value in \"to\" column of first output df should be sum of \"to\" values in second and third outputs)\nst_dimensions(lights1)\n\n  from   to offset     delta refsys x/y\nx    1 2400   -100  0.004167 WGS 84 [x]\ny    1 4800     40 -0.004167 WGS 84 [y]\n\nst_dimensions(h08v05_lights1)\n\n  from   to offset     delta refsys point x/y\nx    1 2400   -100  0.004167 WGS 84 FALSE [x]\ny    1 2400     40 -0.004167 WGS 84 FALSE [y]\n\nst_dimensions(h08v06_lights1)\n\n  from   to offset     delta refsys point x/y\nx    1 2400   -100  0.004167 WGS 84 FALSE [x]\ny    1 2400     30 -0.004167 WGS 84 FALSE [y]\n\n\n\n# Check that tiles for day 2 were successfully combined\n## For x, value in \"to\" column should be the same in all three outputs\n## For y, value in \"to\" column of first output df should be sum of \"to\" values in second and third outputs)\nst_dimensions(lights2)\n\n  from   to offset     delta refsys x/y\nx    1 2400   -100  0.004167 WGS 84 [x]\ny    1 4800     40 -0.004167 WGS 84 [y]\n\nst_dimensions(h08v05_lights2)\n\n  from   to offset     delta refsys point x/y\nx    1 2400   -100  0.004167 WGS 84 FALSE [x]\ny    1 2400     40 -0.004167 WGS 84 FALSE [y]\n\nst_dimensions(h08v06_lights2)\n\n  from   to offset     delta refsys point x/y\nx    1 2400   -100  0.004167 WGS 84 FALSE [x]\ny    1 2400     30 -0.004167 WGS 84 FALSE [y]\n\n\n\n\nHighway locations GeoPackage\nTypically highways account for a large portion of the night lights observable from space. To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways. We used Geofabrik to retrieve a shapefile of all highways in Texas and prepared a GeoPackage containing just the subset of roads that intersect the Houston metropolitan area (Geofabrik, 2022).\n\n# Read in highways data and reproject CRS\n## Defining SQL query\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n## Loading highways data and storing as sf object\nhighways &lt;- st_read(here(\"data\", \"2024-1-20-post-data\", \"gis_osm_roads_free_1.gpkg\"), query = query) %&gt;% \n  st_make_valid()\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway''\nfrom data source `/Users/linusghanadan/Documents/MEDS/other/linusghanadan.github.io/data/2024-1-20-post-data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n## Reprojecting CRS of to EPSG:3083\nhighways &lt;- st_transform(highways, crs = st_crs(3083))\n## Checking that CRS was changed (should print CRS 3083 at bottom of output)\ncrs(highways)\n\n[1] \"PROJCRS[\\\"NAD83 / Texas Centric Albers Equal Area\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Texas Centric Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",18,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-100,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",27.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",1500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",6000000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"State-wide spatial data presentation requiring true area measurements.\\\"],\\n        AREA[\\\"United States (USA) - Texas.\\\"],\\n        BBOX[25.83,-106.66,36.5,-93.5]],\\n    ID[\\\"EPSG\\\",3083]]\"\n\n\n\n\nHouse locations GeoPackage\nFor data on houses in Houston, we again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area (Geofabrik, 2022).\n\n# Read in homes data and reproject CRS\n## Defining query\nquery &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type is NULL AND name is NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n## Loading buildings data and storing as sf object\nhomes &lt;- st_read(here(\"data\", \"2024-1-20-post-data\", \"gis_osm_buildings_a_free_1.gpkg\"), query = query) %&gt;% \n  st_make_valid()\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type is NULL AND name is NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')'\nfrom data source `/Users/linusghanadan/Documents/MEDS/other/linusghanadan.github.io/data/2024-1-20-post-data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n## Reprojecting CRS to EPSG:3083\nhomes &lt;- st_transform(homes, crs = st_crs(3083))\n## Checking that CRS was changed (should print CRS 3083 at bottom of output)\ncrs(homes)\n\n[1] \"PROJCRS[\\\"NAD83 / Texas Centric Albers Equal Area\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Texas Centric Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",18,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-100,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",27.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",1500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",6000000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"State-wide spatial data presentation requiring true area measurements.\\\"],\\n        AREA[\\\"United States (USA) - Texas.\\\"],\\n        BBOX[25.83,-106.66,36.5,-93.5]],\\n    ID[\\\"EPSG\\\",3083]]\"\n\n\n\n\nIncome data GeoDataBase\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the census tracts in 2019 from the U.S. Census Bureau’s American Community Survey (U.S. Census Bureau, 2020). The folder is an ArcGIS, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. Using st_layers(), we can explore the contents of the GeoDataBase. We can combine the geometry with the attributes to get a feature layer that sf can use.\n\n# Read in geodatabase layers, select relevant columns, and reproject CRS\n## Loading income layer as regular dataframe\nincome &lt;- st_read(here(\"data\", \"2024-1-20-post-data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"X19_INCOME\")\n\nReading layer `X19_INCOME' from data source \n  `/Users/linusghanadan/Documents/MEDS/other/linusghanadan.github.io/data/2024-1-20-post-data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n## Loading geometry layer as sf object\nacs_geom &lt;- st_read(here(\"data\", \"2024-1-20-post-data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `/Users/linusghanadan/Documents/MEDS/other/linusghanadan.github.io/data/2024-1-20-post-data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\n## Selecting GEOID (for join) and median income columns from 'income' dataframe\nincome &lt;- subset(income, select = c(\"GEOID\", \"B19013e1\"))\n## Reprojecting 'acs_geom' sfc to EPSG:3083\nacs_geom &lt;- st_transform(acs_geom, crs = st_crs(3083))\n## Checking that CRS was changed (should print CRS 3083 at bottom of output)\ncrs(acs_geom)\n\n[1] \"PROJCRS[\\\"NAD83 / Texas Centric Albers Equal Area\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Texas Centric Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",18,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-100,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",27.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",1500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",6000000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"State-wide spatial data presentation requiring true area measurements.\\\"],\\n        AREA[\\\"United States (USA) - Texas.\\\"],\\n        BBOX[25.83,-106.66,36.5,-93.5]],\\n    ID[\\\"EPSG\\\",3083]]\""
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#part-1-find-locations-of-blackouts",
    "href": "blog/2024-1-20-post/index.html#part-1-find-locations-of-blackouts",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Part 1: Find locations of blackouts",
    "text": "Part 1: Find locations of blackouts\n\n1) Create a blackout mask\nFirst, I’ll create a blackout mask by finding the change in night lights intensity that was presumably caused by the storm, reclassifying the difference raster (assuming that any location that experienced a drop of more than 200 nW cm-2sr^-1 experienced a blackout), and assigning NA to all locations that experienced a drop of less than 200 nW cm-2sr^-1)\n\n# Create difference raster for change in night lights caused by storm\nlights_diff &lt;- lights2 - lights1\n\n\n# Check that difference raster has expected dimensions\n## For both x and y, values in \"to\" column should be the same for all three outputs\nst_dimensions(lights_diff)\n\n  from   to offset     delta refsys x/y\nx    1 2400   -100  0.004167 WGS 84 [x]\ny    1 4800     40 -0.004167 WGS 84 [y]\n\nst_dimensions(lights1)\n\n  from   to offset     delta refsys x/y\nx    1 2400   -100  0.004167 WGS 84 [x]\ny    1 4800     40 -0.004167 WGS 84 [y]\n\nst_dimensions(lights2)\n\n  from   to offset     delta refsys x/y\nx    1 2400   -100  0.004167 WGS 84 [x]\ny    1 4800     40 -0.004167 WGS 84 [y]\n\n\n\n# Reclassify values less than 200 as NA and values greater than or equal to 200 as \"blackout\"\nblackout_mask &lt;- st_apply(lights_diff, c(\"x\", \"y\"), function(x) ifelse(x &lt; 200, NA, \"blackout\"))\n\n\n# Check that NA values were generated in reclassified difference raster (output should just be NA)\nunique(blackout_mask$VNP46A1.A2021047.h08v05.001.2021048091106.tif[\"Mode\"])\n\n[1] NA\n\n\n\n# Vectorize blackout mask\nblackout_mask &lt;- st_as_sf(blackout_mask)\n\n\n# Fix any invalid geometries\nblackout_mask &lt;- st_make_valid(blackout_mask)\n\n\n# Check that only \"blackout\" character strings are showing up in vectorized mask (output should just be \"blackout\")\nunique(blackout_mask$VNP46A1.A2021047.h08v05.001.2021048091106.tif)\n\n[1] \"blackout\"\n\n\n\n\n2) Crop blackout mask to region of interest (Houston metropolitan area)\nNext, I’ll crop the blackout mask to the region of interest, which in this case is the metropolitan area in and around Houston, Texas. I’ll start by defining the Houston metropolitan area in terms of geographic coordinates, turning these coordinates into a polygon using st_polygon, converting the polygon into a simple feature collection using st_sfc(), and assigning a CRS. Then, I’ll crop (i.e., spatially subset) the blackout mask to our region of interest and re-project the cropped blackout dataset to EPSG:3083 (Texas Centric Albers Equal Area)\n\n# Prepare to crop vectorized map to only include observations in Houston\n## Defining Houston metropolitan area coordinates as matrix\nhouston_coords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n## Creating polygons from Houston coordinates matrix\nhouston_polygon &lt;- st_polygon(list(houston_coords))\n## Converting Houston polygon into sfc\nhouston &lt;- st_sfc(houston_polygon)\n## Assigning CRS\nhouston &lt;- st_set_crs(houston, \"WGS84\")\n## Fixing any invalid geometries\nhouston &lt;- st_make_valid(houston)\n\n\n# Check that CRS of 'blackout_mask' and 'houston' are both the same (output should be TRUE)\ncompareCRS(blackout_mask, houston)\n\n[1] TRUE\n\n\n\n# Crop vectorized map to only include observations in Houston and check that result makes sense\n## Printing number of blackout observations before crop\nprint(nrow(blackout_mask))\n\n[1] 140179\n\n## Cropping 'blackout_mask' sf by selecting the spatial observations it shares with 'houston' sfc\nblackout_mask &lt;- blackout_mask[houston, ] %&gt;% \n  st_make_valid()\n## Printing number of blackout observations after crop (should be significantly less)\nprint(nrow(blackout_mask))\n\n[1] 7247\n\n\n\n# Reproject CRS of 'blackout_mask' to EPSG:3083 and check that change was made (should print CRS 3083 at bottom of output)\nblackout_mask &lt;- st_transform(blackout_mask, crs = st_crs(3083))\ncrs(blackout_mask)\n\n[1] \"PROJCRS[\\\"NAD83 / Texas Centric Albers Equal Area\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Texas Centric Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",18,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-100,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",27.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",35,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",1500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",6000000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting (X)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"northing (Y)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"State-wide spatial data presentation requiring true area measurements.\\\"],\\n        AREA[\\\"United States (USA) - Texas.\\\"],\\n        BBOX[25.83,-106.66,36.5,-93.5]],\\n    ID[\\\"EPSG\\\",3083]]\"\n\n\n\n\n3) Exclude highways from blackout mask\nI’ll exclude highways from the blackout mask through identifying areas within 200m of all highways using st_buffer. This will then allow me to isolate the areas that experienced blackouts that are further than 200m from a highway.\n\n# Find areas within 200 meters of all highways\n## Creating 200 meter undissolved buffers around highways lines\nbuffers &lt;- st_buffer(highways$geom, dist = 200)\n## Printing number of features before dissolving\nlength(buffers)\n\n[1] 6085\n\n## Dissolving buffers\nbuffers &lt;- st_union(buffers) %&gt;%\n  st_make_valid\n## Printing number of spatial features after dissolving (should be just 1)\nlength(buffers)\n\n[1] 1\n\n\n\n# Find blackout areas that are further than 200 meters from a highway and check that answer makes sense\n## Printing number of blackout observations in 'blackout_mask'\nprint(nrow(blackout_mask))\n\n[1] 7247\n\n## Clipping 'blackout_mask' to exclude spatial observations in buffer zones\nnon_highway_blackouts &lt;- st_difference(blackout_mask, buffers) %&gt;% \n  st_make_valid\n## Printing number of blackout observations in 'non_highway_blackouts' (should be less)\nprint(nrow(non_highway_blackouts))\n\n[1] 7240\n\n\n\n\n4) Find homes impacted by blackouts\nNow I can find the homes that were impacted by blackouts by filtering to homes within blackout areas and counting the number of impacted homes.\n\n# Filter to homes within blackout areas and check that answer makes sense\n## Printing number of home observations in 'homes'\nprint(nrow(homes))\n\n[1] 475941\n\n## Selecting the spatial observations 'homes' shares with 'blackout_mask' and storing as new variable\nhomes_blackout_mask &lt;- homes[blackout_mask, ]\n## Printing number of home observations in 'homes_blackout_mask' (should be less)\nprint(nrow(homes_blackout_mask))\n\n[1] 86253\n\n\n\n# Count number of impacted homes\n## Printing number of homes experiencing blackout\nprint(nrow(homes_blackout_mask))\n\n[1] 86253\n\n\n\n\n5) Join income data to census tract geometries\n\n# Prepare for join by reformatting 'GEOID' column in 'income' dataframe\n## Creating function to keep only the last 11 characters of a column\nkeep_last_10_digits &lt;- function(x) {\n  substr(x, nchar(x) - 10, nchar(x))\n}\n## Applying new function to 'GEOID' column in 'income'\nincome$GEOID &lt;- keep_last_10_digits(income$GEOID)\n## Checking that all 'GEOID' values have exactly 11 characters (output should be 11)\nunique(str_count(income$GEOID))\n\n[1] 11\n\n\n\n# Conduct join to add census tract geometry column to 'income' and check that new geometry column is populated\n## Merging 'income' dataframe with 'acs_geom' sfc based on the common column 'GEOID'\nincome &lt;- left_join(income, acs_geom, by = \"GEOID\")\n## Converting new dataframe to sf object\nincome &lt;- st_as_sf(income)\n## Checking to make sure that geometry column is populated (output should be \"Geometry column is populated\")\nif (any(!st_is_empty(st_geometry(income)))) {\n  print(\"Geometry column is populated.\")\n} else {\n  print(\"Geometry column is empty.\")\n}\n\n[1] \"Geometry column is populated.\"\n\n\n\n\n6) Spatially join census tract data with buildings determined to be impacted by blackouts\n\n# Conduct spatial join of census tract polygons and home polygons impacted by blackout\n## Printing number of census tract observations in 'income'\nprint(nrow(income))\n\n[1] 5265\n\n## Creating variable for 'homes_blackout_mask' polygons contained in 'income' polygons, keeping geometry column of census tracts from 'income'\nsel_blackouts &lt;- st_contains(x = income, y = homes_blackout_mask)\n\n\n\n7) Find which census tracts had blackouts and plot\n\n## Printing number of census tract observations in 'sel_blackouts' (should be same)\nprint(nrow(sel_blackouts))\n\n[1] 5265\n\n\n\n# Prepare to plot\n## Creating variable converting previous variable to bool values\nsel_logical &lt;- lengths(sel_blackouts) &gt; 1\n## Creating new column containing bools (=1 if has home blackouts, otherwise =0)\nincome$has_blackouts_bool &lt;- sel_logical\n## Creating new column containing characters based on bool\nincome &lt;- mutate(income, has_blackouts_char = if_else(has_blackouts_bool, \"Impacted by Blackouts\", \"Unimpacted by Blackouts\"))\n## Reprojecting CRS of Houston coordinates so can use to crop 'income'\nhouston &lt;- st_transform(houston, crs = st_crs(3083))\n## Checking that 'income' and 'houston' have same CRS (output should be TRUE)\ncompareCRS(income, houston)\n\n[1] TRUE\n\n## Printing number of census tract observations in 'income' before crop\nprint(nrow(income))\n\n[1] 5265\n\n## Cropping 'income' polygons based on Houston coordinates\nincome &lt;- income[houston, ]\n## Printing number of census tract observations in 'income' after crop (should be less)\nprint(nrow(income))\n\n[1] 1112\n\n\n\n# Plot Houston census tracts with homes impacted by blackouts\nggplot() +\n  geom_sf(data = income, aes(fill = has_blackouts_char), color = \"black\") +\n  labs(title = \"Houston Census Tracts with Homes impacted by Blackouts\") +\n  scale_fill_manual(values = c(\"Unimpacted by Blackouts\" = \"white\", \"Impacted by Blackouts\" = \"red\"), name = \"\") +\n  annotation_scale(location = \"br\", width_hint = 0.2) +\n  annotation_north_arrow(location = \"br\", which_north = \"true\", pad_x = unit(0.1, \"npc\"), pad_y = unit(0.1, \"npc\"))"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#part-2-income-analysis",
    "href": "blog/2024-1-20-post/index.html#part-2-income-analysis",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Part 2: Income analysis",
    "text": "Part 2: Income analysis\n\nCreate map showing which census tracts had blackout (discrete variable) and median income (continuous variable)\n\n# Plot median income of Houston census tracts as continuous variable and whether census tract was impacted as discrete variable\nggplot() +\n  geom_sf(data = income, aes(fill = B19013e1), color = \"transparent\") +\n  scale_fill_distiller(name = \"Median Income (USD)\", na.value = \"darkgrey\", palette = \"Purples\") +\n  geom_sf(data = income, fill = \"transparent\", aes(color = has_blackouts_char)) +\n  scale_color_discrete(name = \"\") +\n  labs(title = \"Median Income of Houston Census Tracts\") +\n  annotation_scale(location = \"br\", width_hint = 0.2) +\n  annotation_north_arrow(location = \"br\", which_north = \"true\", pad_x = unit(0.1, \"npc\"), pad_y = unit(0.1, \"npc\"))\n\n\n\n\n\n\nCompare median incomes of impacted and unimpacted census tracts\n\n# Plot side-by-side histogram showing income distribution of impacted and unimpacted Houston census tracts\nggplot(income, aes(x = B19013e1, fill = has_blackouts_char)) +\n  geom_histogram(binwidth = 10000) +\n  labs(title = \"Income Distribution of Houston Census Tracts\",\n       x = \"Median Income (USD)\",\n       y = \"Frequency\") +\n  facet_grid(~has_blackouts_char) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#conclusion",
    "href": "blog/2024-1-20-post/index.html#conclusion",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Conclusion",
    "text": "Conclusion\nFor most Houston census tracts, our side-by-side histograms indicate that there was not a difference based on median income, as the distribution shape and center appear to be about the same for census tracts that were and were not impacted by home blackouts. However, our histograms do show that census tracts with median income of $250,000, which was the highest median income on our distribution, avoided impacts from home blackouts at a disproportionately high rate compared to other census tracts. Furthermore, a limitation of our study is that it does not tell us about why this was the case. For example, it is possible that people in these high income census tracts owned backup generators that they used, or it could be that these census tracts have special access to more reliable forms of electricity from local utilities. Some combination of both of these explanations is also possible."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#map-for-geospatial-analysis-project",
    "href": "blog/2024-7-24-post/index.html#map-for-geospatial-analysis-project",
    "title": "Visualization Portfolio (R)",
    "section": "Map for Geospatial Analysis Project",
    "text": "Map for Geospatial Analysis Project\nContext for plot: For a project in my Geospatial Analysis class, I used data from the NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Specifically, I looked at census tracts in the Houston metropolitan area where residential blackouts occurred and how this relates to median income of census tracts."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#chloropleth-maps-for-geospatial-analysis-project",
    "href": "blog/2024-7-24-post/index.html#chloropleth-maps-for-geospatial-analysis-project",
    "title": "Visualization Portfolio (R)",
    "section": "Chloropleth Maps for Geospatial Analysis Project",
    "text": "Chloropleth Maps for Geospatial Analysis Project\nContext for plots: For a project in my Geospatial Analysis class, I used data from the NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Specifically, I looked at census tracts in the Houston metropolitan area where residential blackouts occurred and how this relates to median income of census tracts.\n\nCaption: Only a couple of the census tracts on the border of the Houston metropolitan area were impacted by residential blackouts. Otherwise, there are no obvious spatial patterns in where blackouts occurred.\n\nCaption: There does not appear to be a significant relationship between median income and whether a census tract was impacted by residential blackouts. However, upon closer examination, it can be seen that 9 of the 10 census tracts with a median income above $240,000 were unimpacted by residential blackouts, demonstrating that very wealthy census tracts avoided residential blackouts at a disproportionately high rate compared to other census tracts. It is possible that people in these high income census tracts owned backup generators that they used, or it could be that these census tracts have special access to more reliable forms of electricity from local utilities."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#about-darn-tough-vermont",
    "href": "blog/2024-6-20-post/index.html#about-darn-tough-vermont",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "About Darn Tough Vermont",
    "text": "About Darn Tough Vermont\nOur team’s client, Darn Tough Vermont, is a Vermont-based outdoor sock company that is serious about prioritizing sustainability and creating long-lasting products. Known for their lifetime warranty, Darn Tough offers customers a free new pair of socks for shipping back a worn-out pair, which gets recycled into blankets for the U.S. military. While this lifetime warranty is the company’s landmark sustainability initiative, they are increasingly concerned about their carbon footprint, as Darn Tough strives to align its operations with the 2030 target set by its largest retailer, REI (Recreational Equipment, Inc.). Specifically, this target is to achieve a 55% reduction in total GHG emissions by 2030 (compared to 2019 baseline level). Reaching the 2030 target will require Darn Tough to develop effective ways to model their emissions, so they can best understand the critical points in their supply chain that are responsible for the most emissions, in addition to points where emissions can be most easily reduced."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#understanding-ghg-emissions",
    "href": "blog/2024-6-20-post/index.html#understanding-ghg-emissions",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Understanding GHG Emissions",
    "text": "Understanding GHG Emissions\nCalculating an organization’s carbon footprint is a multifaceted endeavor, requiring a meticulous evaluation of every step in the product lifecycle, from raw material sourcing to manufacturing, distribution, and even post-consumer handling. Darn Tough follows the Greenhouse Gas (GHG) Protocol, which categorizes emissions into three distinct scopes:\n\nScope 1 (Burn): Direct emissions from sources owned or controlled by the company\nScope 2 (Buy): Indirect emissions from purchased energy\nScope 3 (Beyond): All other indirect emissions, including upstream and downstream activities\n\nFor Darn Tough, this involves accounting for emissions at every stage of their sock production, from manufacturing fibers, shipping products, to recycling worn-out socks through their warranty program.\n\nCaption: Presentation slide describing the framework for modeling Darn Tough’s emissions using the GHG Protocol."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#the-challenge",
    "href": "blog/2024-6-20-post/index.html#the-challenge",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "The Challenge",
    "text": "The Challenge\nDarn Tough, renowned for producing durable socks for the outdoor apparel industry, utilizes a blend of merino wool and synthetic materials. Their partnership with REI, which has set an ambitious target to reduce emissions by 55% from 2019 levels by 2030, underscores the urgency to refine their greenhouse gas emissions calculations. However, Darn Tough’s current method, which primarily uses Excel, is reliant on manual, non-reproducible calculations in Excel and poses challenges for the team, underscoring the need for a more streamlined and accurate solution."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#our-approach",
    "href": "blog/2024-6-20-post/index.html#our-approach",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Our Approach",
    "text": "Our Approach\nTo meet these challenges head-on, we defined three major objectives:\n\nUtilize Accurate Methodology: Address discrepancies and errors in the existing emissions calculations by updating and standardizing the methodology.\nStreamline Calculation Process: Develop a reproducible, efficient method to replace the complex and unwieldy Excel-based process.\nConduct Scenario Analysis: Create tools that allow for scenario planning and data-driven decision-making to reduce emissions effectively.\n\n\nCaption: New analytical workflow for Darn Tough’s carbon accounting and sustainability analysis."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#delivering-three-solutions",
    "href": "blog/2024-6-20-post/index.html#delivering-three-solutions",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Delivering Three Solutions",
    "text": "Delivering Three Solutions\n\n1) Updated Methodology\nOur first task was to refine Darn Tough’s calculation methodology. We conducted extensive research and improved calculations, ensuring alignment with industry standards and the Greenhouse Gas (GHG) Protocol, including using the latest emission factors. Along the way, we ensured all updates were thoroughly documented. This update not only improved accuracy but also provided a robust foundation for future calculations.\n\n\n2) Excel Input Template & Emissions Calculator Application\nNext, we tackled the complexity of Darn Tough’s existing workflow. Darn Tough’s original method used a complex Excel sheet to model their emissions. This approach was time consuming, resource intensive, and difficult to replicate yearly. By automating the emissions calculations using the R programming language, we created a reproducible approach.\nOur streamlined solution involved creating a simplified Excel Input Template with tidy, formula-free tables and automating calculations with R. This new model takes inventory data as inputs and produces precise emissions totals, eliminating the potential for manual errors and significantly simplifying the process.\nTo make these improvements user-friendly, we then developed the Emissions Calculator Application using Shiny. This app allows users to upload their data, select scopes, and calculate emissions effortlessly. It also provides downloadable outputs and interactive visualizations, making it easier to understand and manage carbon emissions.\nThe new Emissions Calculator Application makes it easier for Darn Tough to calculate their emissions. Users can upload data, select specific scopes, calculate emissions, and generate downloadable outputs and interactive visualizations all in one centralized interface.\n\nCaption: Presentation slide comparing how Excel was used in the old workflow to how it is being used in the newly developed workflow. In the new workflow, Excel is only used for data entry.\n\nCaption: Screenshot of the basic interface for computing emissions in the Emissions Calculatior Application. Specifically, this image shows Scope 2 emissions based on data in the uploaded Excel template (Note: Fake Data).\n\nCaption: The Interactive Sunburst Plot allows users to analyze emissions in terms of Scope, Category (for Scope 3), and input variables (e.g., Wool Fiber). Through clicking on a segment of the plot, users can more easily see the granular data. In addition, the user can hover over a segment to access exact values for emissions and percent of total emissions (Note: Fake Data).\n\n\n3) Scenario Analysis Tool (part of the Emissions Calculator Application)\nBeyond calculating emissions, our Scenario Analysis Tool helps Darn Tough identify practical ways to reduce their carbon footprint. Users can input hypothetical changes, such as switching to recycled wool, and compare the impact on emissions against a baseline year. This feature provides actionable insights for making informed, effective decisions on emission reductions.\n\nCaption: Screenshot showing the basic interface of the Scope 3 Category 1 Scenario Analysis Tool, which allows the user to evaluate emissions under scenarios with different fiber types and quantities (Note: Fake Data).\n\nCaption: As part of the tool, users can look at visualizations comparing baseline emissions to emissions under a new scenario. For example, this image looks at the impact of substituting some normal wool fiber for recycled wool (Note: Fake Data).\n\nCaption: Sustainability analysis, looking at the relative impact of replacing regular wool with recycled wool, based on the emissions factors associated with fiber procurement."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#moving-forward",
    "href": "blog/2024-6-20-post/index.html#moving-forward",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Moving Forward",
    "text": "Moving Forward\nWith the Emissions Calculator Application, Darn Tough is now equipped to calculate, visualize, and reduce their carbon emissions more efficiently. This streamlined process not only saves time but also allows the company to focus on broader sustainability initiatives. As Darn Tough continues to innovate and improve, we are confident that their best sock is yet to come, and that they have the tools necessary to meet their 2030 target."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#my-experience",
    "href": "blog/2024-6-20-post/index.html#my-experience",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "My Experience",
    "text": "My Experience\nThis project was a unique opportunity to work with a real-world client and apply data science skills to a sustainability challenge, and I look forward to seeing how Darn Tough makes use of our project to reduce their emissions. Ultimately, working alongside my team members, Carly, Annie, and Flora has been the most rewarding experience, and I am grateful for their hard work and dedication throughout this project. Whether it was troubleshooting code, brain-storming ideas, cheering each other on, or learning to provide useful feedback, building this application with them has highlighted the importance of hard work, being kind, and having fun along the way!\n\nCaption: Darn Tough Team celebrating after faculty review presentations.\n\nCaption: Darn Tough Team rocking their socks at Montesito Peak."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#acknowledgments",
    "href": "blog/2024-6-20-post/index.html#acknowledgments",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe extend our deepest gratitude to Kristen Graf from Darn Tough Vermont, Dr. Enze Jin, Dr. Carmen Galaz Garcia, the MEDS Capstone Committee, and our entire MEDS cohort for their invaluable support and feedback throughout this project.\n\nCaption: Darn Tough Team celebrating after faculty review presentations.\n\nCaption: Darn Tough Team rocking their socks at Montecito Peak."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#time-series-visuals-for-statistics-final-project",
    "href": "blog/2024-7-24-post/index.html#time-series-visuals-for-statistics-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "",
    "text": "Context for plots: As a final project for my Statistics class, I conducted a 2010-2019 time series analysis of nutrient concentrations in the Chesapeake Bay. I used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 different monitoring stations positioned throughout the Bay’s tidal regions. Specifically, I constructed two STL decomposition models (one for nitrogen and one for phosphorus), which is a statistical model that separates data into seasonal trend, non-seasonal trend, and random components. Since 2010 marked the beginning of federal water quality requirements under the Clean Water Act, the goal of my analysis was mainly to identify any overarching non-seasonal trends in nitrogen and phosphorus levels over the ten years, but I was also interested in understanding the extent to which seasonal trends and randomness contributed to the variation in monthly average concentrations.\n\nCaption: The non-seasonal trend component (red) indicates that there was a negligible downward trend (95% CI: -0.28% to -0.05%) over the span of the ten years. Interestingly, this component also tells us that the increase in concentrations seen over the course of 2018, as well as the subsequent decrease seen during 2019, were both indicative of non-seasonal trends. Furthermore, there is a distinct seasonal trend component (green) to nitrogen concentrations in the Chesapeake Bay tidal regions (explains 55% of total variation). Each year, concentrations increased sharply around December. They then peaked around February to March, before decreasing substantially and reaching their minimum around July. Lastly, looking at the random component (blue), this analysis tells us that the spike at the beginning of 2014 was likely due to an isolated event and not indicative of a trend in nitrogen concentrations, as our model classified this spike as attributable to randomness.\n\nCaption: The non-seasonal trend component (red) indicates a slightly more pronounced downward trend (95% CI: -0.52% to -0.38%) in comparison to nitrogen. Compared to nitrogen, there was also a more distinct seasonal trend component (green) for phosphorus concentrations (explains 76% of total variation). Unlike nitrogen, phosphorus concentrations shot up in the middle of the year around May, had a relatively flat peak lasting from June to August, and then shot back down at the end of the Summer."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#visuals-for-ensemble-learning-project",
    "href": "blog/2024-7-23-post/index.html#visuals-for-ensemble-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Visuals for Ensemble Learning Project",
    "text": "Visuals for Ensemble Learning Project\nContext for plots: In this ensemble learning project, I built three different models that predict Dissolved Inorganic Carbon (DIC) content in water samples off the coast of California. The features being used to make these predictions were other ocean chemistry measurements that were also measured during water sampling.\n\nCaption: Based on this correlation heatmap, the variables Temperature_degC and R_Nuts were removed from the feature matrix due to high correlation with the target variable.\n\nCaption: The most important feature for predicting DIC in the random forest model was Sulfur trioxide (SiO3) concentration, with a feature importance over 0.7. This was significantly higher than the second most important feature, Phosphate (PO3) concentration, which had a feature importance of close to 0.2. The third and fourth most important features, reported Oxygen concentration and Nitrate (NO3) concentration, only had a features importance of about 0.03."
  },
  {
    "objectID": "blog/2024-7-23-post/index.html#visuals-for-deep-learning-project",
    "href": "blog/2024-7-23-post/index.html#visuals-for-deep-learning-project",
    "title": "Visualization Portfolio (Python)",
    "section": "Visuals for Deep Learning Project",
    "text": "Visuals for Deep Learning Project\nContext for plots: In this personal project, I built and compared two deep learning models that predict phosphorus concentration in Chesapeake Bay tidal regions based on the time and location of sample. Specifically, my data includes 43,809 samples taken over 10 years (2010-2019) from 143 different monitoring stations across the tidal regions of the Chesapeake Bay.\n\nCaption: Data exploration plot tracing monthly phosphorus concentration over time. This time series visual indicates distinct seasonal trends, with average concentration significantly higher during months in the summer and early fall seasons.\n\nCaption: From adding an additional 16-neuron dense layer with a ReLU activation function to our LSTM network architecture, we saw a 3.8% decrease in MAE and a 1.6% decrease in RMSE, suggesting that there are potential advantages from using a hybrid RNN-MLP model in this context. Based on these scatterplots of actual values and predicted values, it appears the reductions in MAE and RMSE can be attributed to enhanced outlier detection among actual concentration values &gt;0.15 mg/L."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#visuals-for-data-visualization-communication-project",
    "href": "blog/2024-7-24-post/index.html#visuals-for-data-visualization-communication-project",
    "title": "Visualization Portfolio (R)",
    "section": "Visuals for Data Visualization & Communication Project",
    "text": "Visuals for Data Visualization & Communication Project\nContext for plots: For a final project in my Data Visualization & Communication class, I created this plot as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\n\nCaption: The energy sector made up 55% of global anthropogenic methane emissions in 2021, about half of which came from China, Russia, the U.S., Iran, and India. Meanwhile, agriculture and waste contributed 29% and 14% of global emissions, with the remaining 2% coming from other sources.\n\nCaption: In terms of methane emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the ~50 tons emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. Among the highlighted countries, Australia and Canada had the highest emissions per person, at ~300 tons.\n\nCaption: As a percent of country-level methane emissions, Russia’s energy sector (85%) and Brazil’s agricultural sector (65%) stand out as particularly high. Meanwhile, energy-related emissions in the EU (29%) and Brazil (16%) make up a relatively low share of their total emissions, compared to about 60-70% in China, the U.S., Canada, and Australia."
  },
  {
    "objectID": "blog/2024-7-24-post/index.html#visuals-for-data-visualization-communication-final-project",
    "href": "blog/2024-7-24-post/index.html#visuals-for-data-visualization-communication-final-project",
    "title": "Visualization Portfolio (R)",
    "section": "Visuals for Data Visualization & Communication Final Project",
    "text": "Visuals for Data Visualization & Communication Final Project\nContext for plots: For a final project in my Data Visualization & Communication class, I created these plots as part of an infographic on anthropogenic methane emissions in 2021. My data came from the International Energy Agency, which provided granularity regarding the country and sector (energy, agriculture, waste, and other) of emissions.\n\nCaption: The energy sector made up 55% of global anthropogenic methane emissions in 2021, about half of which came from China, Russia, the U.S., Iran, and India. Meanwhile, agriculture and waste contributed 29% and 14% of global emissions, with the remaining 2% coming from other sources.\n\nCaption: In terms of methane emissions per person, the EU and China were fairly similar to other countries, while the U.S., Canada, Australia, and Russia were significantly higher than most. The average American emitted ~150 tons, more than three-times the ~50 tons emitted by the average person living in the EU. The average Canadian emitted ~200 tons, roughly equivalent to the total carbon footprint of a 48-hour private jet ride. Among the highlighted countries, Australia and Canada had the highest emissions per person, at ~300 tons.\n\nCaption: As a percent of country-level methane emissions, Russia’s energy sector (85%) and Brazil’s agricultural sector (65%) stand out as particularly high. Meanwhile, energy-related emissions in the EU (29%) and Brazil (16%) make up a relatively low share of their total emissions, compared to about 60-70% in China, the U.S., Canada, and Australia."
  },
  {
    "objectID": "writing/evs/ev.html#view-6-page-memo-as-pdf",
    "href": "writing/evs/ev.html#view-6-page-memo-as-pdf",
    "title": "Economic Analysis of EV Tax Credits from the Inflation Reduction Act",
    "section": "View 6-page memo as PDF",
    "text": "View 6-page memo as PDF\n\n\n\ndronepicr, CC BY 2.0, via Wikimedia Commons"
  },
  {
    "objectID": "index.html#data-visualizations-from-masters-program-projects",
    "href": "index.html#data-visualizations-from-masters-program-projects",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Data Visualizations from Master’s Program Projects",
    "text": "Data Visualizations from Master’s Program Projects\nBest of my Data Visualizations made with R | Link to Blog\nBest of my Data Visualizations made with Python | Link to Blog"
  },
  {
    "objectID": "index.html#masters-statistics-course-final-project",
    "href": "index.html#masters-statistics-course-final-project",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Statistics Course Final Project",
    "text": "Master’s Statistics Course Final Project\nTime Series Analysis of Nutrient Concentration in the Chesapeake Bay | Link to GitHub Repository | Link to Blog\nConstructed two Seasonal-Trend using LOESS (STL) decomposition models to conduct 2010-2019 time series analysis of nutrient concentrations in the Chesapeake Bay. Used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 monitoring stations positioned throughout the Bay’s tidal regions. Selecting seasonality for the model based on autocorrelation, visualized monthly mean concentration, seasonally adjusted monthly mean, STL seasonality component, and STL trend component for each of the two pollutants. Ran regressions of model parameters to compare the proportion of variation that was attributable to seasonality for nitrogen and phosphorus, as well as to compare 95% confidence intervals for change in each pollutant’s trend component over the 10-year period."
  },
  {
    "objectID": "index.html#masters-capstone-project",
    "href": "index.html#masters-capstone-project",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Capstone Project",
    "text": "Master’s Capstone Project\nCreating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis | Link to GitHub Page | Link to Blog\nWorked with the Head of Sustainability at outdoor apparel company Darn Tough Vermont and three classmates to streamline the company’s spreadsheet method for calculating and analyzing data on yearly Scope 1, 2, and 3 GHG emissions. Built interactive web-application dashboard that allows the company to visualize emissions and conduct scenario analysis based on adjustable input variables (e.g., can evaluate emissions under scenarios with differing fiber procurements)."
  },
  {
    "objectID": "index.html#masters-geospatial-analysis-course-project",
    "href": "index.html#masters-geospatial-analysis-course-project",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Geospatial Analysis Course Project",
    "text": "Master’s Geospatial Analysis Course Project\nSpatial Analysis of 2021 Houston Power Crisis | Link to Repository | Link to Blog\nUsed data from NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Determined and visualized census tracts in the Houston metropolitan area where residential blackouts occurred and analyzed how this related to median income of census tracts."
  },
  {
    "objectID": "index.html#masters-policy-evaluation-course-project",
    "href": "index.html#masters-policy-evaluation-course-project",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Policy Evaluation Course Project",
    "text": "Master’s Policy Evaluation Course Project\nImpact Analysis of a 1998 Cash-Transfer Program in Rural Mexico | Link to Repository | Link to Blog\nCompared pre-treatment characteristics in the treatment and control groups of the 1998 Prospera cash-transfer program. Estimated the Average Treatment Effect (ATE) of the program on a household’s value of owned animals with the First-Difference, Fixed-Effects, and Difference-in-Difference estimators."
  },
  {
    "objectID": "index.html#masters-modeling-environmental-systems-course-project",
    "href": "index.html#masters-modeling-environmental-systems-course-project",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Modeling Environmental Systems Course Project",
    "text": "Master’s Modeling Environmental Systems Course Project\nDynamic Simulation of Forest Growth | Link to Repository | Link to Blog\nGenerated stochastic parameter sets for forest size model inputs (exponential growth rate before canopy closure, linear growth rate after canopy closure, carrying capacity, and canopy closure threshold) and used an ordinary differential equations solver to run 300-year continuous dynamic simulations of forests. Conducted global sensitivity analysis (ran 2,000 simulations and computed Sobol indices of input parameters) to look at impact of varying parameter values on maximum forest size."
  },
  {
    "objectID": "index.html#masters-machine-learning-course-projects",
    "href": "index.html#masters-machine-learning-course-projects",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Master’s Machine Learning Course Projects",
    "text": "Master’s Machine Learning Course Projects\nCluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay | Link to Repository | Link to Blog\nImplemented K-means clustering algorithm with data on metal contents (Cd, Cr, Cu, Mn, and Ni) in two species of co-occurring algae at 10 sample sites around Port Jackson Bay to plot the Total Within Sum of Square (TWSS) for different numbers of clusters and determine the optimal number of clusters that indicate distinct types of bio-contaminating algae. In addition, calculated Euclidean distance matrix to build hierarchical clustering model and inspect the resulting dendrogram for outlier points.\nRegression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems | Link to Repository | Link to Blog\nUsed data from a marine ecosystem research program to build three models (single decision tree, random forest, and stochastic gradient boosted trees) that predict dissolved inorganic carbon (DIC) based on other ocean chemistry features (e.g., sulfur trioxide concentration) that were also measured during water sampling. Developed visualizations comparing root mean squared error (RMSE) among the three models and analyzing feature importances in the best performing model."
  },
  {
    "objectID": "index.html#data-visualization",
    "href": "index.html#data-visualization",
    "title": "My name is Linus Ghanadan. Welcome to my website!",
    "section": "Data Visualization",
    "text": "Data Visualization\nBest of my Data Visualizations made with R | Link to Blog\nBest of my Data Visualizations made with Python | Link to Blog\n\nFinal Project for Statistics Class\nTime Series Analysis of Nutrient Concentration in the Chesapeake Bay | Link to GitHub Repository | Link to Blog\nConstructed two Seasonal-Trend using LOESS (STL) decomposition models to conduct 2010-2019 time series analysis of nutrient concentrations in the Chesapeake Bay. Used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 monitoring stations positioned throughout the Bay’s tidal regions. Selecting seasonality for the model based on autocorrelation, visualized monthly mean concentration, seasonally adjusted monthly mean, STL seasonality component, and STL trend component for each of the two pollutants. Ran regressions of model parameters to compare the proportion of variation that was attributable to seasonality for nitrogen and phosphorus, as well as to compare 95% confidence intervals for change in each pollutant’s trend component over the 10-year period.\n\n\nMaster’s Capstone Project\nCreating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis | Link to GitHub Page | Link to Blog\nWorked with the Head of Sustainability at outdoor apparel company Darn Tough Vermont and three classmates to streamline the company’s spreadsheet method for calculating and analyzing data on yearly Scope 1, 2, and 3 GHG emissions. Built interactive web-application dashboard that allows the company to visualize emissions and conduct scenario analysis based on adjustable input variables (e.g., can evaluate emissions under scenarios with differing fiber procurements).\n\n\nProject for Geospatial Analysis Class\nSpatial Analysis of 2021 Houston Power Crisis | Link to Repository | Link to Blog\nUsed data from NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Determined and visualized census tracts in the Houston metropolitan area where residential blackouts occurred and analyzed how this related to median income of census tracts.\n\n\nProject for Policy Evaluation Class\nImpact Analysis of a 1998 Cash-Transfer Program in Rural Mexico | Link to Repository | Link to Blog\nCompared pre-treatment characteristics in the treatment and control groups of the 1998 Prospera cash-transfer program. Estimated the Average Treatment Effect (ATE) of the program on a household’s value of owned animals with the First-Difference, Fixed-Effects, and Difference-in-Difference estimators.\n\n\nProject for Modeling Environmental Systems Class\nDynamic Simulation of Forest Growth | Link to Repository | Link to Blog\nGenerated stochastic parameter sets for forest size model inputs (exponential growth rate before canopy closure, linear growth rate after canopy closure, carrying capacity, and canopy closure threshold) and used an ordinary differential equations solver to run 300-year continuous dynamic simulations of forests. Conducted global sensitivity analysis (ran 2,000 simulations and computed Sobol indices of input parameters) to look at impact of varying parameter values on maximum forest size.\n\n\nProjects for Machine Learning Class\nCluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay | Link to Repository | Link to Blog\nImplemented K-means clustering algorithm with data on metal contents (Cd, Cr, Cu, Mn, and Ni) in two species of co-occurring algae at 10 sample sites around Port Jackson Bay to plot the Total Within Sum of Square (TWSS) for different numbers of clusters and determine the optimal number of clusters that indicate distinct types of bio-contaminating algae. In addition, calculated Euclidean distance matrix to build hierarchical clustering model and inspect the resulting dendrogram for outlier points.\nRegression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems | Link to Repository | Link to Blog\nUsed data from a marine ecosystem research program to build three models (single decision tree, random forest, and stochastic gradient boosted trees) that predict dissolved inorganic carbon (DIC) based on other ocean chemistry features (e.g., sulfur trioxide concentration) that were also measured during water sampling. Developed visualizations comparing root mean squared error (RMSE) among the three models and analyzing feature importances in the best performing model."
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#context",
    "href": "blog/2024-6-10-post/index.html#context",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Context",
    "text": "Context\nThis project was completed for my Modeling Environmental Systems class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate modeling techniques."
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#project-summary",
    "href": "blog/2024-6-10-post/index.html#project-summary",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Project Summary",
    "text": "Project Summary\nGenerated stochastic parameter sets for forest size model inputs (exponential growth rate before canopy closure, linear growth rate after canopy closure, carrying capacity, and canopy closure threshold) and used an ordinary differential equations solver to run 300-year continuous dynamic simulations of forests. Conducted global sensitivity analysis (ran 2,000 simulations and computed Sobol indices of input parameters) to look at impact of varying parameter values on maximum forest size."
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#link-to-github-repository",
    "href": "blog/2024-6-10-post/index.html#link-to-github-repository",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Link to GitHub Repository",
    "text": "Link to GitHub Repository"
  },
  {
    "objectID": "blog/2024-3-6-post/index.html",
    "href": "blog/2024-3-6-post/index.html",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "",
    "text": "This project was completed for my Policy Evaluation class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate causal inference modeling techniques."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#context",
    "href": "blog/2024-3-6-post/index.html#context",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Context",
    "text": "Context\nThis project was completed for my Policy Evaluation class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate causal inference modeling techniques."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#project-summary",
    "href": "blog/2024-3-6-post/index.html#project-summary",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Project Summary",
    "text": "Project Summary\nCompared pre-treatment characteristics in the treatment and control groups of the 1998 Prospera cash-transfer program. Estimated the Average Treatment Effect (ATE) of the program on a household’s value of owned animals with the First-Difference, Fixed-Effects, and Difference-in-Difference estimators."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#link-to-github-repository",
    "href": "blog/2024-3-6-post/index.html#link-to-github-repository",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Link to GitHub Repository",
    "text": "Link to GitHub Repository"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html",
    "href": "blog/2024-1-20-post/index.html",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "",
    "text": "This project was completed for my Geospatial Analysis & Remote Sensing class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate geospatial modeling techniques."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#context",
    "href": "blog/2024-1-20-post/index.html#context",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Context",
    "text": "Context\nThis project was completed for my Geospatial Analysis & Remote Sensing class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate geospatial modeling techniques."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#project-summary",
    "href": "blog/2024-1-20-post/index.html#project-summary",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Project Summary",
    "text": "Project Summary\nUsed data from NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Determined and visualized census tracts in the Houston metropolitan area where residential blackouts occurred and analyzed how this related to median income of census tracts."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#link-to-github-repository",
    "href": "blog/2024-1-20-post/index.html#link-to-github-repository",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Link to GitHub Repository",
    "text": "Link to GitHub Repository"
  },
  {
    "objectID": "blog/2024-4-1-post/index.html",
    "href": "blog/2024-4-1-post/index.html",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "",
    "text": "This project was completed for my Machine Learning class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate machine learning techniques."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#context",
    "href": "blog/2024-4-1-post/index.html#context",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Context",
    "text": "Context\nThis project was completed for my Machine Learning class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate machine learning techniques."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#project-summary",
    "href": "blog/2024-4-1-post/index.html#project-summary",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Project Summary",
    "text": "Project Summary\nImplemented K-means clustering algorithm with data on metal contents (Cd, Cr, Cu, Mn, and Ni) in two species of co-occurring algae at 10 sample sites around Port Jackson Bay to plot the Total Within Sum of Square (TWSS) for different numbers of clusters and determine the optimal number of clusters that indicate distinct types of bio-contaminating algae. In addition, calculated Euclidean distance matrix to build hierarchical clustering model and inspect the resulting dendrogram for outlier points."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#link-to-github-repository",
    "href": "blog/2024-4-1-post/index.html#link-to-github-repository",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Link to GitHub Repository",
    "text": "Link to GitHub Repository"
  },
  {
    "objectID": "blog/2023-12-12-post/index.html",
    "href": "blog/2023-12-12-post/index.html",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "",
    "text": "This project was completed as a final project for my Statistics class, taken as part of my Master’s program at UC Santa Barbara. I worked independently to find data, pose a statistical question, and carry out analysis using appropriate modeling techniques."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#project-summary",
    "href": "blog/2023-12-12-post/index.html#project-summary",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Project Summary",
    "text": "Project Summary\nConstructed two Seasonal-Trend using LOESS (STL) decomposition models to conduct 2010-2019 time series analysis of nutrient concentrations in the Chesapeake Bay. Used 43,809 nitrogen samples and 43,590 phosphorus samples from 143 monitoring stations positioned throughout the Bay’s tidal regions. Selecting seasonality for the model based on autocorrelation, visualized monthly mean concentration, seasonally adjusted monthly mean, STL seasonality component, and STL trend component for each of the two pollutants. Ran regressions of model parameters to compare the proportion of variation that was attributable to seasonality for nitrogen and phosphorus, as well as to compare 95% confidence intervals for change in each pollutant’s trend component over the 10-year period."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#context",
    "href": "blog/2024-6-20-post/index.html#context",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "",
    "text": "This Master’s capstone project was completed with the client Darn Tough Vermont and three classmates for my Master’s program at UC Santa Barbara."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#project-summary",
    "href": "blog/2024-6-20-post/index.html#project-summary",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Project Summary",
    "text": "Project Summary\nWorked with the Head of Sustainability at outdoor apparel company Darn Tough Vermont and three classmates to streamline the company’s spreadsheet method for calculating and analyzing data on yearly Scope 1, 2, and 3 GHG emissions. Built interactive web-application dashboard that allows the company to visualize emissions and conduct scenario analysis based on adjustable input variables (e.g., can evaluate emissions under scenarios with differing fiber procurements)."
  },
  {
    "objectID": "blog/2024-6-20-post/index.html#link-to-github-page",
    "href": "blog/2024-6-20-post/index.html#link-to-github-page",
    "title": "Creating and Implementing an Analytical Workflow for an Outdoor Apparel Company’s Carbon Accounting and Sustainability Analysis",
    "section": "Link to GitHub Page",
    "text": "Link to GitHub Page"
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#context",
    "href": "blog/2023-12-12-post/index.html#context",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "",
    "text": "This project was completed as a final project for my Statistics class, taken as part of my Master’s program at UC Santa Barbara. I worked independently to find data, pose a statistical question, and carry out analysis using appropriate modeling techniques."
  },
  {
    "objectID": "blog/2023-12-12-post/index.html#link-to-github-repository",
    "href": "blog/2023-12-12-post/index.html#link-to-github-repository",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay",
    "section": "Link to GitHub Repository",
    "text": "Link to GitHub Repository"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#question",
    "href": "blog/2024-1-20-post/index.html#question",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Question",
    "text": "Question\nDuring the 2021 Houston Power Crisis, in which census tracts did residential blackouts occur, and how did this relate to a census tract’s median income?"
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#analysis-summary",
    "href": "blog/2024-1-20-post/index.html#analysis-summary",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Analysis Summary",
    "text": "Analysis Summary\nUsed data from NASA’s VIIRS instrument to conduct a spatial analysis of the 2021 Houston Power Crisis. Determined and visualized census tracts in the Houston metropolitan area where residential blackouts occurred and analyzed how this related to median income of census tracts."
  },
  {
    "objectID": "blog/2024-1-20-post/index.html#introduction",
    "href": "blog/2024-1-20-post/index.html#introduction",
    "title": "Spatial Analysis of 2021 Houston Power Crisis",
    "section": "Introduction",
    "text": "Introduction\nIn February 2021, Texas faced an unprecedented power crisis that left millions without electricity as a result of three winter storms (Ramsey, 2021). Struggling to meet the extraordinary demand for heating amid freezing temperatures, the Electric Reliability Council of Texas (ERCOT) implemented widespread blackouts to prevent a total grid collapse. In addition to exposing the vulnerabilities of Texas’s energy infrastructure, the crisis also prompted a nationwide discussion on the resilience of power grids in the face of extreme weather events."
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#question",
    "href": "blog/2024-6-10-post/index.html#question",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Question",
    "text": "Question\nBased on essential parameters, how is forest size likely to change over 300 years, and which of the parameters are likely to be the most influential?"
  },
  {
    "objectID": "blog/2024-6-10-post/index.html#data",
    "href": "blog/2024-6-10-post/index.html#data",
    "title": "Dynamic Simulation of Forest Growth",
    "section": "Data",
    "text": "Data\nData was synthetically generated for this project based on typical ranges for the essential parameters."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#question",
    "href": "blog/2024-4-1-post/index.html#question",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Question",
    "text": "Question\nWhat are the major clusters of bio-contaminating algae in the Port Jackson Bay, in terms of the expected ranges of metal content across five types of metal (Cd, Cr, Cu, Mn, and Ni)?"
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#analysis-summary",
    "href": "blog/2024-4-1-post/index.html#analysis-summary",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Analysis Summary",
    "text": "Analysis Summary\nImplemented K-means clustering algorithm with data on metal contents (Cd, Cr, Cu, Mn, and Ni) in two species of co-occurring algae at 10 sample sites around Port Jackson Bay to plot the Total Within Sum of Square (TWSS) for different numbers of clusters and determine the optimal number of clusters that indicate distinct types of bio-contaminating algae. In addition, calculated Euclidean distance matrix to build hierarchical clustering model and inspect the resulting dendrogram for outlier points."
  },
  {
    "objectID": "blog/2024-4-1-post/index.html#data",
    "href": "blog/2024-4-1-post/index.html#data",
    "title": "Cluster Analysis of Bio-Contaminating Algae in the Port Jackson Bay",
    "section": "Data",
    "text": "Data\nWe’ll use data from Roberts et al. 2008 on biological contaminants in Port Jackson Bay (located in Sydney, Australia). The data are measurements of metal content in algae at 10 sample sites around the bay."
  },
  {
    "objectID": "blog/2024-3-6-post/index.html#analysis-summary",
    "href": "blog/2024-3-6-post/index.html#analysis-summary",
    "title": "Impact Analysis of a 1998 Cash-Transfer Program in Rural Mexico",
    "section": "Analysis Summary",
    "text": "Analysis Summary\nCompared pre-treatment characteristics in the treatment and control groups of the 1998 Prospera cash-transfer program. Estimated the Average Treatment Effect (ATE) of the program on a household’s value of owned animals with the First-Difference, Fixed-Effects, and Difference-in-Difference estimators."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#context",
    "href": "blog/2024-4-3-post/dic-ml-models.html#context",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Context",
    "text": "Context\nThis project was completed for my Machine Learning class, taken as part of my Master’s program at UC Santa Barbara. Provided with data and questions, I carried out this analysis using appropriate machine learning techniques."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#question",
    "href": "blog/2024-4-3-post/dic-ml-models.html#question",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Question",
    "text": "Question\nHow can machine learning models, specifically random forest and stochastic gradient boosted tree models, be used to predict Dissovled Inorganic Carbon (DIC) in California coastal ecosystems based on ocean chemistry features?"
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#analysis-summary",
    "href": "blog/2024-4-3-post/dic-ml-models.html#analysis-summary",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Analysis Summary",
    "text": "Analysis Summary\nUsed data from a marine ecosystem research program to build three models (single decision tree, random forest, and stochastic gradient boosted trees) that predict dissolved inorganic carbon (DIC) based on other ocean chemistry features (e.g., sulfur trioxide concentration) that were also measured during water sampling. Developed visualizations comparing root mean squared error (RMSE) among the three models and analyzing feature importances in the best performing model."
  },
  {
    "objectID": "blog/2024-4-3-post/dic-ml-models.html#data",
    "href": "blog/2024-4-3-post/dic-ml-models.html#data",
    "title": "Regression Models to predict Dissolved Inorganic Carbon in California Coastal Ecosystems",
    "section": "Data",
    "text": "Data\nOur data set comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI), an oceanographic and marine ecosystem research program located in California (link to data set source). All water samples were taken off the California coast."
  },
  {
    "objectID": "writing/bag-leakage/bag-leakage.html",
    "href": "writing/bag-leakage/bag-leakage.html",
    "title": "Understanding the Impact of California Disposable Carryout Bag Policies",
    "section": "",
    "text": "Tony Webster, CC BY 2.0, via Wikimedia Commons\nFor this Econometrics class 20-minute presentation, my group explained the research question, research design, data, regression equation, and econometric results from Rebecca L.C. Taylor’s 2019 paper in the Journal of Environmental Economics and Management on the impact of California disposable carryout bag (DCB) policies, which investigates this impact in terms of changes in the weight of consumed plastic bags accounting for bag leakage (i.e., some of the decrease in DCBs is offset by increase in purchases of small, medium, and tall kitchen trash bags). In addition, we also assessed the internal validity, construct validity, and external validity of the study."
  },
  {
    "objectID": "writing/bag-leakage/bag-leakage.html#view-slide-deck-as-pdf-used-for-20-minute-group-presentation",
    "href": "writing/bag-leakage/bag-leakage.html#view-slide-deck-as-pdf-used-for-20-minute-group-presentation",
    "title": "Understanding the Impact of California Disposable Carryout Bag Policies",
    "section": "View slide-deck as PDF (used for 20-minute group presentation)",
    "text": "View slide-deck as PDF (used for 20-minute group presentation)"
  },
  {
    "objectID": "writing/urban-dev/urban-dev.html",
    "href": "writing/urban-dev/urban-dev.html",
    "title": "Sustainable Urban Development in São Paulo",
    "section": "",
    "text": "Gini coefficient comparison of São Paulo and countries around the world\nFor this paper (completed for Culture & Natural Resource Management class at the University of Maryland), I analyzed the sustainability challenges and opportunities for São Paulo, Brazil, within the framework of environmental, economic, and social conditions. I evaluated the city’s Ecological Footprint, identified key local environmental issues like soil degradation and air pollution, and assessed the impact of income inequality on sustainable development. I also explored potential policy solutions from the São Paulo Climate Action Plan, which focuses on renewable energy, green infrastructure, and social equity."
  },
  {
    "objectID": "writing/urban-dev/urban-dev.html#view-8-page-essay-as-pdf",
    "href": "writing/urban-dev/urban-dev.html#view-8-page-essay-as-pdf",
    "title": "Sustainable Urban Development in São Paulo",
    "section": "View 8-page essay as PDF",
    "text": "View 8-page essay as PDF"
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#context",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#context",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Context",
    "text": "Context\nThis project was completed as a final project for my Statistics class, taken as part of my Master’s program at UC Santa Barbara. I worked independently to find data, pose a statistical question, and carry out analysis using appropriate modeling techniques. Originally, this analysis was conducted in R (Link to R Code Repository), but I decided to reproduce this analysis to practice my Python skills working with large datasets and building statistical models with the statsmodels library."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#question",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#question",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Question",
    "text": "Question\nSince the 2010 introduction of federal water quality requirements, what seasonal and non-seasonal trends are present for nitrogen and phosphorus concentrations in Chesapeake Bay tidal regions?"
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#analysis-summary",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#analysis-summary",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Analysis summary",
    "text": "Analysis summary\nProposed statistical question on how nutrient concentrations have changed since Clean Water Act protection measures (implemented in 2010) and found appropriate data for answering the question (used over 43,000 samples from the Bay’s tidal regions). Constructed two Seasonal-Trend using LOESS (STL) decomposition models to conduct time series analysis of nitrogen and phosphorus concentrations (selected length of seasons based on autocorrelation). For each pollutant, visualized model parameters comparatively. In addition, ran regressions to determine the proportion of variation attributable to seasonality and the 95% confidence interval for change in trend component over the 10-year period."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#introduction",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#introduction",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Introduction",
    "text": "Introduction\nThe Chesapeake Bay is the largest estuary in the United States, and the largest body of water that is regulated under the Clean Water Act (U.S. Environmental Protection Agency 2023). Federal regulation pertaining to the Bay took decades to implement, and this is in large part because of the Bay’s large size and the many stakeholders involved. In the 1970s, the Bay was identified as one of the first areas in the world to have a marine dead zone, a phenomenon that literally suffocates aquatic life due to lack of oxygen in the water. Despite dead zones being identified in the 1970s, it was not until 2000 that the Bay was designated as an “impaired water” under the Clean Water Act. Then, it took another ten years, until 2010, for the EPA to take the next step of issuing Total Maximum Daily Load (TMDL) requirements, the regulatory action mandating water quality improvement.\nSpecifically, a TMDL is the maximum amount of a particular pollutant that a body of water can receive and still meet applicable water quality standards (U.S. Environmental Protection Agency 2023). This maximum amount is calculated in pounds based on measurements taken at areas where pollution is likely to end up in the Bay. In their 2010 regulation, the EPA established TMDL requirements for nitrogen, phosphorus, and sediment. Nitrogen and phosphorus, referred to as nutrients because of their role in providing nutrition to many animals and plants, cause algal blooms, which cause marine dead zones through taking in dissolved oxygen and blocking sunlight. Sediment contributes to dead zones by blocking sunlight as well, leading it to also be included in the 2010 TMDL requirements.\nThis analysis will focus on nitrogen and phosphorus, the two pollutants responsible for algal blooms in the Chesapeake Bay. A 2022 study found that agricultural runoff was the largest source of nutrient pollution, accounting for 48% of nitrogen and 27% of phosphorus in the Chesapeake Bay (Chesapeake Progress, n.d.). Both of these pollutants also get to the Bay as a result of urban and suburban runoff, wastewater treatment plants releasing treated water, and natural sources (e.g., runoff from forests, wetlands, etc.). In addition, about 25% of nitrogen that ends up in the Bay comes from air pollution that is originally emitted to the atmosphere by sources such as cars and factories (Burns et al. 2021). Through a process called atmospheric deposition, these nitrogen compounds react with other chemicals to become nitrous oxides, which can be deposited back to Earth’s surface through precipitation or as dry deposition.\nThrough conducting a time series analysis of post-2010 nitrogen and phosphorus concentration measurements, my goal is to better understand how concentrations have changed since the introduction of TMDL requirements. I’m also interested in the nature of any seasonality and whether the three time series components (i.e., seasonal, trend, and random) are consistent across both nitrogen and phosphorus."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#data",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#data",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Data",
    "text": "Data\nYearly water quality data on the Chesapeake Bay’s tidal and non-tidal regions going back to 1984 is publicly available on the Chesapeake Bay Program (CBP) DataHub (Chesapeake Bay Program DataHub, n.d.). Data is organized into either Tier 1, 2, or 3 depending on how it was collected. While Tier 1 and 2 data can be collected by any interested group, Tier 3 data is collected by monitoring stations overseen by experienced professionals. Only Tier 3 data can be used for governmental regulatory assessments.\nFor my analysis, I will be using 2010 to 2019 Tier 3 data collected at 143 different monitoring stations positioned throughout the Chesapeake Bay tidal regions, which includes the mainstem Bay and tributary components. Across the 10 years that we are looking at, we’ll have a total of 43,809 nitrogen observations and 43,590 phosphorus observations.\nBelow, we import the Python libraries used in this analysis. Then, we read in the yearly water quality data using their CBP DataHub URL. We also process the data by creating separate data.frames for total nitrogen and phosphorus measurements.\n\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics libraries\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.regression.linear_model import OLS\n\n# Other libraries\nimport requests\nfrom io import BytesIO\n\n\n# Create a list of data URLs\nexcel_urls = [\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2019_CEDR_tidal_data_01jun21.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2018_CEDR_tidal_data_01jun21.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2017_CEDR_tidal_data_11oct18.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2016_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2015_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2014_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2013_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2012_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2011_CEDR_tidal_data_15jun17.xlsx',\n    'https://datahub-content.chesapeakebay.net/traditional_annual_tidal_02jun21/2010_CEDR_tidal_data_15jun17.xlsx'\n]\n\n# Create an empty list to store data frames\ndfs = []\n\n# Loop through each URL, read the Excel file directly into pandas, and append to list of data frames\nfor url in excel_urls:\n    # Get the content of the Excel file\n    response = requests.get(url)\n    \n    # Read the Excel file directly from the content\n    wq_data = pd.read_excel(BytesIO(response.content), sheet_name=0)\n    dfs.append(wq_data)\n\n# Combine all data frames into a single data frame\nwq_data_combined = pd.concat(dfs, ignore_index=True)\n\n\n# Wrangle data for relevant column variables, filter for TN (total nitrogen), and remove Parameter column\nnitr_data = wq_data_combined[[\"SampleDate\", \"Parameter\", \"MeasureValue\", \"Latitude\", \"Longitude\"]]\nnitr_data = nitr_data[nitr_data[\"Parameter\"] == \"TN\"]\nnitr_data.drop(columns=['Parameter'], inplace=True)\n\nnitr_data.info()\n\n# Wrangle data for relevant column variables, and filter for TP (total phosphorus)\nphos_data = wq_data_combined[[\"SampleDate\", \"Parameter\", \"MeasureValue\", \"Latitude\", \"Longitude\"]]\nphos_data = phos_data[phos_data[\"Parameter\"] == \"TP\"]\nphos_data.drop(columns=['Parameter'], inplace=True)\n\nphos_data.info()\n\n# Remove from environment\ndel wq_data_combined\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 43809 entries, 56 to 2484389\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   SampleDate    43809 non-null  datetime64[ns]\n 1   MeasureValue  43809 non-null  float64       \n 2   Latitude      43809 non-null  float64       \n 3   Longitude     43809 non-null  float64       \ndtypes: datetime64[ns](1), float64(3)\nmemory usage: 1.7 MB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 43590 entries, 60 to 2484391\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   SampleDate    43590 non-null  datetime64[ns]\n 1   MeasureValue  43590 non-null  float64       \n 2   Latitude      43590 non-null  float64       \n 3   Longitude     43590 non-null  float64       \ndtypes: datetime64[ns](1), float64(3)\nmemory usage: 1.7 MB"
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#exploratory-analysis",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#exploratory-analysis",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nWe’ll start by briefly looking at average concentration by location using a heatmap (courtesy of the seaborn library).\n\n# Nitrogen\n\n# Bin latitude and longitude into groups\nnitr_data_copy = pd.DataFrame(nitr_data)\nnitr_data_copy['Latitude Group'] = pd.cut(nitr_data['Latitude'], bins=10)\nnitr_data_copy['Longitude Group'] = pd.cut(nitr_data['Longitude'], bins=10)\n\n# Pivot the data to create a heatmap\nnitr_heatmap_data = nitr_data_copy.pivot_table(index='Latitude Group', columns='Longitude Group', values='MeasureValue', aggfunc='mean')\n\n# Remove from environment\ndel nitr_data_copy\n\n# Initialize the figure\nplt.figure(figsize=(10, 8))\n\n# Set style\nplt.style.use('seaborn-whitegrid')\n\n# Plot heatmap\nsns.heatmap(nitr_heatmap_data, cmap='Reds')\n\n# Customize the plot\nplt.title('Heatmap of Mean Nitrogen Concentration (mg/L) by Location')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\nplt.tight_layout()\nplt.show()\n\n# Remove from environment\ndel nitr_heatmap_data\n\n/tmp/ipykernel_18/2516891936.py:9: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  nitr_heatmap_data = nitr_data_copy.pivot_table(index='Latitude Group', columns='Longitude Group', values='MeasureValue', aggfunc='mean')\n/tmp/ipykernel_18/2516891936.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\n\n# Phosphorus\n\n# Bin latitude and longitude into groups\nphos_data_copy = pd.DataFrame(phos_data)\nphos_data_copy['Latitude Group'] = pd.cut(phos_data['Latitude'], bins=10)\nphos_data_copy['Longitude Group'] = pd.cut(phos_data['Longitude'], bins=10)\n\n# Pivot the data to create a heatmap\nphos_heatmap_data = phos_data_copy.pivot_table(index='Latitude Group', columns='Longitude Group', values='MeasureValue', aggfunc='mean')\n\n# Remove from environment\ndel phos_data_copy\n\n# Initialize the figure\nplt.figure(figsize=(10, 8))\n\n# Set style\nplt.style.use('seaborn-whitegrid')\n\n# Plot heatmap\nsns.heatmap(phos_heatmap_data, cmap='Reds')\n\n# Customize the plot\nplt.title('Heatmap of Mean Phosphorus Concentration (mg/L) by Location')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\nplt.tight_layout()\nplt.show()\n\n# Remove from environment\ndel phos_heatmap_data\n\n/tmp/ipykernel_18/190755117.py:9: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n  phos_heatmap_data = phos_data_copy.pivot_table(index='Latitude Group', columns='Longitude Group', values='MeasureValue', aggfunc='mean')\n/tmp/ipykernel_18/190755117.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\nInterestingly, nitrogen and phosphorus concentration are both generally higher in the southeast regions of the Chesapeake Bay.\nMoving forward, we’ll start with our time series analysis by calculating moving averages for each year-month. We’ll plot these moving averages just to get a general idea of what types of trends we might be looking at.\n\n# Nitrogen\n\n# Remove columns from the nitrogen data that are no longer needed\nnitr_data.drop(columns=['Latitude', 'Longitude'], inplace=True)\n\n# Convert 'SampleDate' to datetime\nnitr_data['SampleDate'] = pd.to_datetime(nitr_data['SampleDate'])\n\n# Set 'SampleDate' as the index\nnitr_data.set_index('SampleDate', inplace=True)\n\n# Resample to monthly frequency and calculate the mean for the 'MeasureValue' column\nnitr_monthly_avg = nitr_data['MeasureValue'].resample('M').mean()\n\n# Convert the result to a DataFrame\nnitr_monthly_avg = nitr_monthly_avg.reset_index()\n\n# Remove from environment\ndel nitr_data\n\n# Initialize figure\nplt.figure(figsize=(12, 6))\n\n# Plot monthly average nitrogen data\nplt.plot(nitr_monthly_avg.index, nitr_monthly_avg['MeasureValue'], color='black', alpha=0.7)\n\n# Customize the plot\nplt.title('Monthly Average Concentration of Nitrogen Samples (2010-2019)')\nplt.xlabel('Year')\nplt.ylabel('Monthly Average Concentration (mg/L)')\n\n# Rotate and align tick labels\nplt.gcf().autofmt_xdate()\n\n# Use tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\nplt.show()\n\n/tmp/ipykernel_18/368804770.py:13: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  nitr_monthly_avg = nitr_data['MeasureValue'].resample('M').mean()\n\n\n\n\n\nFrom this plot, there does appear to be a seasonal trend for nitrogen concentrations, but there is no clear non-seasonal trend. In addition, there is a notable spike in early 2014.\n\n# Phosphorus\n\n# Remove columns from the phosphorus data that are no longer needed\nphos_data.drop(columns=['Latitude', 'Longitude'], inplace=True)\n\n# Convert 'SampleDate' to datetime\nphos_data['SampleDate'] = pd.to_datetime(phos_data['SampleDate'])\n\n# Set 'SampleDate' as the index\nphos_data.set_index('SampleDate', inplace=True)\n\n# Resample to monthly frequency and calculate the mean for the 'MeasureValue' column\nphos_monthly_avg = phos_data['MeasureValue'].resample('M').mean()\n\n# Convert the result to a DataFrame\nphos_monthly_avg = phos_monthly_avg.reset_index()\n\n# Remove from environment\ndel phos_data\n\n# Initialize figure\nplt.figure(figsize=(12, 6))\n\n# Plot monthly average phosphorus data\nplt.plot(phos_monthly_avg.index, phos_monthly_avg['MeasureValue'], color='black', alpha=0.7)\n\n# Customize the plot\nplt.title('Monthly Average Concentration of Phosphorus Samples (2010-2019)')\nplt.xlabel('Year')\nplt.ylabel('Monthly Average Concentration (mg/L)')\n\n# Rotate and align tick labels\nplt.gcf().autofmt_xdate()\n\n# Use tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\nplt.show()\n\n/tmp/ipykernel_18/3702329752.py:13: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  phos_monthly_avg = phos_data['MeasureValue'].resample('M').mean()\n\n\n\n\n\nSimilar to the nitrogen plot, phosphorus also seems to exhibit a distinct seasonal trend. Again, it is unclear whether there is a non-seasonal trend."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#methods",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#methods",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Methods",
    "text": "Methods\n\nAutocorrelation function\nThe autocorrelation function calculates the correlation between the dependent variable at a given point in time and various time lags for this same variable. Thus, the autocorrelation function provides us with a tool that allows us to better understand any seasonal trends present in our data. This will be useful for us in subsequent steps of our time series analysis.\n\n\nSTL decomposition\nAs a core part of this time series analysis, I’ll be constructing a seasonal trend decomposition using locally estimated scatterplot smoothing (LOESS), which is often abbreviated as a STL decomposition model. STL allows us to separate our monthly average concentrations into three components: seasonal trend, non-seasonal trend, and remainder. Through plotting these components next to each other, we gain a more intuitive understanding of the underlying forces contributing to variation in our dependent variable, which in this case is the monthly average concentration.\nAs opposed to other decomposition methods, one thing that is particular to STL models is the ability to specify the length of a season. It can be helpful to adjust this input depending on our desired level of smoothing for the non-seasonal trend. We will use our results from the autocorrelation function to inform our chosen length of seasons. The autocorrelation function is useful in this context because it can tell us after how many lags we see a drop-off in correlation, indicating there is a drop in the significance of the seasonal trend."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#results",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#results",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Results",
    "text": "Results\n\nAutocorrelation function\n\n# Nitrogen\n\n# Plot autocorrelation function for nitrogen with lags going back three years\nsm.graphics.tsa.plot_acf(nitr_monthly_avg['MeasureValue'], lags=36, alpha=0.05)\n\nplt.xlabel('Lags (Months)')\nplt.title('Autocorrelation Function for Nitrogen')\nplt.show()\n\n\n\n\nLooking at this autocorrelation plot for nitrogen, I see that the t-16 lag is significant at the alpha equals 0.05 level, indicated by the black line extending beyond the blue dashed line. Meanwhile, the t-4 lag is not statistically significant. The rest of the lags remain the same or slightly decrease when comparing from the first and second year of lags. In the third year, there is a drop off in the t-28 lag compared to t-12, and there continues to be what seems like a marginal decrease in correlation across most lags. Considering all of this, I decided to set the seasonality of my STL model for nitrogen to 24 months.\n\n# Phosphorus\n\n# Plot autocorrelation function for phosphorus with lags going back three years\nsm.graphics.tsa.plot_acf(phos_monthly_avg['MeasureValue'], lags=36)\n\nplt.xlabel('Lags (Months)')\nplt.title('Autocorrelation Function for Phosphorus')\nplt.show()\n\n\n\n\nFor phosphorus, there is a very consistent marginal decline for each set of lags over the course of the three years. This is good news for our STL because it means that the seasonal trend will be easier to separate from the non-seasonal trend. Like I did with nitrogen, I’m also going to use two-year seasons for phosphorus. Similar to the case with nitrogen, it seems to me like the drop-off in lag correlations from year two to year three is a bit larger than from year one to year two. This suggests that a two-year seasonal cycle will give us an informative non-seasonal trend component that is neither too eager nor too hesitant to categorize differences as non-seasonal trends.\n\n\nSTL decomposition\n\n# Nitrogen\n\n# Perform STL decomposition\nnitr_stl = STL(nitr_monthly_avg['MeasureValue'], period=24)\nnitr_decomp = nitr_stl.fit()\n\nnitr_decomp.plot()\n\n# Remove from environment\ndel nitr_stl\n\n\n\n\nIn this plot of the three STL components for nitrogen, it is still difficult to see a long-term trend, despite the line being fairly smooth. There does seem to be a slight downward trend until 2018. From 2018 to 2019, there is a clear increase, but this change is then offset by an equivalent decrease over the course of 2019 to 2020.\n\n# Phosphorus\n\n# Perform STL decomposition\nphos_stl = STL(phos_monthly_avg['MeasureValue'], period=24)\nphos_decomp = phos_stl.fit()\n\nphos_decomp.plot()\n\n# Remove from environment\ndel phos_stl\n\n\n\n\nThe STL plot for phosphorus does make it seem like there is a long-term downward trend, but it is difficult to tell how significant it is because of the long grey bar, which indicates it is least influential of the three components in our STL model.\n\n# Nitrogen\n\n# Extract the components\nnitr_trend = nitr_decomp.trend\nnitr_seasonal = nitr_decomp.seasonal\nnitr_resid = nitr_decomp.resid\n\n# Remove from environment\ndel nitr_decomp\n\n# Calculate the seasonally adjusted series\nnitr_seasonally_adjusted = nitr_monthly_avg['MeasureValue'].values - nitr_seasonal\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Plot all components on the same axis\nplt.plot(nitr_monthly_avg.index, nitr_monthly_avg['MeasureValue'].values, label='Monthly Mean', color='black')\nplt.plot(nitr_monthly_avg.index, nitr_seasonally_adjusted, label='Seasonal-Adjusted Monthly Mean', color='blue', linewidth=3)\nplt.plot(nitr_monthly_avg.index, nitr_trend, label='Trend Component', color='red', linewidth=3)\nplt.plot(nitr_monthly_avg.index, nitr_seasonal, label='Seasonal Component', color='green', linewidth=3)\n\n# Customize x-axis for date breaks\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))  # Yearly major ticks\nplt.gca().xaxis.set_minor_locator(plt.MaxNLocator(20))  # Half-yearly minor ticks\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n\n# Set labels and title\nplt.xlabel('Year-Month')\nplt.ylabel('Concentration (mg/L)')\nplt.title('Nitrogen in Chesapeake Bay (2010-2019)', fontsize=14)\n\n# Customize legend\nplt.legend(loc='upper left')\n\n# Add a grid and theme\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n# Remove from environment\ndel nitr_seasonally_adjusted\n\n\n\n\nI decided to make this visualization to get a better idea of exactly how these components map on to each other. This plot seems to confirm the idea of negligible trend for nitrogen. Since the x-axis is labeled for each year here, it is also easier to see the seasonal trend. Each year, nitrogen concentrations increase sharply around December. They then peak around February to March, before decreasing substantially and reaching their minimum around July.\n\n# Phosphorus\n\n# Extract the components\nphos_trend = phos_decomp.trend\nphos_seasonal = phos_decomp.seasonal\nphos_resid = phos_decomp.resid\n\n# Remove from environment\ndel phos_decomp\n\n# Calculate the seasonally adjusted series\nphos_seasonally_adjusted = phos_monthly_avg['MeasureValue'].values - phos_seasonal\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Plot all components on the same axis\nplt.plot(phos_monthly_avg.index, phos_monthly_avg['MeasureValue'].values, label='Monthly Mean', color='black')\nplt.plot(nitr_monthly_avg.index, phos_seasonally_adjusted, label='Seasonal-Adjusted Monthly Mean', color='blue', linewidth=3)\nplt.plot(phos_monthly_avg.index, phos_trend, label='Trend Component', color='red', linewidth=3)\nplt.plot(phos_monthly_avg.index, phos_seasonal, label='Seasonal Component', color='green', linewidth=3)\n\n# Customize x-axis for date breaks\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(10))  # Yearly major ticks\nplt.gca().xaxis.set_minor_locator(plt.MaxNLocator(20))  # Half-yearly minor ticks\nplt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n\n# Set labels and title\nplt.xlabel('Year-Month')\nplt.ylabel('Concentration (mg/L)')\nplt.title('Phosphorus in Chesapeake Bay (2010-2019)', fontsize=14)\n\n# Customize legend\nplt.legend(loc='upper left')\n\n# Add a grid and theme\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n# Remove from environment\ndel phos_seasonally_adjusted\n\n\n\n\nOur plot for phosphorus further supports the idea that there is a slight downward trend over the decade. If you independently trace the maximums or minimums, the line does seem to be moving downward at an oscillating but fairly consistent rate. Unlike nitrogen, phosphorus concentrations shoot up in the middle of the year around May, have a relatively flat peak lasting from June to August, and then shoot down at the end of Summer.\n\n\nLinear regressions based on STL model parameters\n\n# Nitrogen\n\n# Run regression of season component on monthly average\nX = sm.add_constant(nitr_seasonal)\ny = nitr_monthly_avg['MeasureValue'].values\nnitr_season_reg = OLS(y, X).fit()\n\n# Print R-squared\nr_squared = nitr_season_reg.rsquared\nprint(f\"R-squared: {r_squared}\")\n\nR-squared: 0.7216759708762595\n\n\nThe adjusted R-squared of 0.55 indicates that seasonal trends can explain a bit over half (55%) of the variation in nitrogen monthly mean.\n\n# Phosphorus\n\n# Run regression of season component on monthly average\nX = sm.add_constant(phos_seasonal)\ny = phos_monthly_avg['MeasureValue'].values\nphos_season_reg = OLS(y, X).fit()\n\n# Print R-squared\nr_squared = phos_season_reg.rsquared\nprint(f\"R-squared: {r_squared}\")\n\nR-squared: 0.8716948739102784\n\n\nFor phosphorus, adjusted R-squared of this regression is 0.76, confirming our idea that seasonality is more pronounced with phosphorus.\n\n# Nitrogen\n\n# Run regression of trend component on monthly average\nX = nitr_monthly_avg.index\ny = nitr_trend\nnitr_season_reg = OLS(y, X).fit()\n\n# Get 95% confidence interval for 10 year trend\nconf_int = nitr_season_reg.conf_int(alpha=0.05)\nnitr_trend_conf_int = 120 * conf_int[1]\n\nprint(f\"95% Confidence Interval for nitr_trend coefficient (mg/L): {nitr_trend_conf_int}\")\n\n# Get the 95% confidence interval for 10 year trend (in percent terms)\nnitr_trend_conf_int = nitr_trend_conf_int / nitr_monthly_avg['MeasureValue'].values[0]\n\nprint(f\"95% Confidence Interval for nitr_trend coefficient (%): {nitr_trend_conf_int}\")\n\n# Remove from environment\ndel nitr_monthly_avg\ndel nitr_trend\n\n95% Confidence Interval for nitr_trend coefficient (mg/L): x1    1.499793\nName: 1, dtype: float64\n95% Confidence Interval for nitr_trend coefficient (%): x1    1.279909\nName: 1, dtype: float64\n\n\nIn this linear regression, we look at the influence of year-month on our non-seasonal trend component for nitrogen. The regression output tells us that we can say at an alpha equals 0.01 significance level that the 10-year change in non-seasonal trend component was negative. However, the low adjusted R-squared also tells us that variation in year-month explains very little of the variation in trend component. Then, the first interval tells us that there is a 95% chance that the interval from -0.0028 mg/L to -0.0005 mg/L contains the true 10-year change in non-seasonal trend component. The second interval shows that this represents a -0.28% to -0.05% change as compared to the non-seasonal trend component in January 2010.\n\n# Phosphorus\n\n# Run regression of trend component on monthly average\nX = phos_monthly_avg.index\ny = phos_trend\nphos_season_reg = OLS(y, X).fit()\n\n# Get 95% confidence interval for 10 year trend\nconf_int = phos_season_reg.conf_int(alpha=0.05)\nphos_trend_conf_int = 120 * conf_int[1]\n\nprint(f\"95% Confidence Interval for phos_trend coefficient (mg/L): {phos_trend_conf_int}\")\n\n# Get the 95% confidence interval for 10 year trend (in percent terms)\nphos_trend_conf_int = phos_trend_conf_int / phos_monthly_avg['MeasureValue'].values[0]\n\nprint(f\"95% Confidence Interval for phos_trend coefficient (%): {phos_trend_conf_int}\")\n\n95% Confidence Interval for phos_trend coefficient (mg/L): x1    0.093551\nName: 1, dtype: float64\n95% Confidence Interval for phos_trend coefficient (%): x1    1.639039\nName: 1, dtype: float64\n\n\nFor phosphorus, the linear regression output tells us that we are confident at the alpha equals 0.01 level that the 10-year change in non-seasonal trend component was negative. It is also worth noting that the adjusted R-squared tells us that over half (56%) of the the variation in trend component can be explained by variation in year-month. Lastly, we find that there is a 95% chance that the interval between -0.00030 mg/L and -0.00022 mg/L contains the true 10-year change in non-seasonal trend component. This represents a -0.52% to -0.38% change as compared to the non-seasonal trend component in January 2010."
  },
  {
    "objectID": "blog/2024-8-20-post/chesapeake-bay-python.html#conclusion",
    "href": "blog/2024-8-20-post/chesapeake-bay-python.html#conclusion",
    "title": "Time Series Analysis of Nutrient Concentration in Chesapeake Bay (Using Python)",
    "section": "Conclusion",
    "text": "Conclusion\nMy time series analysis suggests that seasonality plays a substantial role in contributing to variation in the monthly mean concentration of nitrogen and phosphorus in tidal regions of the Chesapeake Bay. For nitrogen, seasonal trends explained 55% of the variation in monthly means, and the relationship was even stronger for phosphorus, with seasonal trends explaining 76% of the variation. While the seasonal component for nitrogen was highest during Winter, the seasonal component for phosphorus was highest during Summer.\nThis analysis was also interested in any non-seasonal trend that has occurred since the introduction of TMDL requirements in 2010. For both nitrogen and phosphorus, we find evidence at an alpha level of 0.01 that the 10-year change in non-seasonal trend is negative. However, our confidence intervals suggest that, for both nutrient pollutants, these changes in trend represent a less than 1% decrease in concentration over the decade.\nThe main limitation of this analysis was that no form of spatial interpolation was employed to estimate concentrations across the tidal region based on the location of measurements. It would be interesting to compare such an analysis to what we did here, as any significant differences would imply that sampled areas are not spread throughout the region in a representative manner. Further analysis might also investigate what happened at the beginning of 2014 that could have led to the high spike in nitrogen levels at that time, in addition to factors that might have fueled the increase seen over the course of 2018."
  }
]